2018 - 07 - 17

FIRST PAGES
Jurimetry

Machine Translated by Google
Machine Translated by Google
Machine Translated by Google
Machine Translated by Google
© of this issue [2016]

Machine Translated by Google
Jurimetry

© of this issue [2016]

ABOUT THE AUTHOR

2018 - 07 - 17

MARCELO GUEDES NUNES

Researcher interested in the areas of Business Law, evaluation of public policies
and regulatory impact studies. Lawyer.
Professor of Law at PUC/SP. President of the Brazilian Association of Jurimetry - ABJ.

Machine Translated by Google
Jurimetry

© of this issue [2016]

2018 - 07 - 17

ACKNOWLEDGMENTS

THANKS

Gone are the days when research in Law was synonymous with the seclusion of a
jurist in his library, inspecting alphabets in search of arguments of authority and the
Roman origin of some norm. Society is clamoring for results and modern legal
research, responding to this clamor with some delay, is becoming increasingly
interdisciplinary and therefore based on teamwork. Only group work can launch
research capable of understanding the factors that animate the complex, numerous
and decentralized reality of the current legal world. The thanks I express here are,
therefore, more than a formal “thank you very much for your support”: they are, in
reality, the recognition of teamwork, without which this book would not even be
conceivable. I would like to express my reverent thanks to my advisor and friend
Fábio Ulhoa Coelho, to my trench companions Luiz Ernesto Oliveira and Pedro
Roquim, partners and inseparable brothers, to the mathematicians Adilson Simonis,
Carlos Pereira, Flávio Ulhoa Coelho, Julio Stern, Rafael Stern and Julio Trecenti, to
professors and colleagues Ivo Waisberg, Manoel Queiroz Pereira Calças and Jairo
Saddi, whose comments on the doctoral committee were of inestimable value, and to
all the advisors, directors, associates and partners of the Brazilian Jurimetrics
Association. Finally, I would like to thank my wife, Maria Isabel, for her careful review
of the originals, which corrected numerous excesses and obscurities in the text. Bel's
intelligence and fine elegance made her my ideal reader model and, at the end of the
day, everything I write aims to please her. My biggest wish for this book is that, to some extent, I have succeeded.

Machine Translated by Google
Jurimetry
PREFACE

2018 - 07 - 17

PREFACE

The trajectory that marks the birth of the philosophy of Law is expressed through a
normative epistemology. In its beginning and in the end, there is Kelsen: the methodological
cuts of the pure theory of Law inaugurate it and the Kelsenian hermeneutics inadvertently
exhausts it. The epistemology characteristic of this founding trajectory of the philosophy
of law is normative because it delimits the conditions from which legal knowledge could
be classified as scientific.

When Scandinavian realism flourished in the 1960s, statistics had not yet experienced
the extraordinary boost of recent decades. At least he hadn't developed to the point of
attracting Alf Ross's attention. I believe it is indisputable that it would have a lot to dialogue
with statistics, if the conditions that favored the emergence of Jurimetria had been
anticipated half a century.

The first reflection to which the philosophy of law was dedicated created what we could
identify as a trajectory, quite clear, destined to reflect on the status of legal knowledge.
Lately inserted in positivism, the philosophy of law was built on the discussion of the
premises for the recognition of its scientific nature. Legal knowledge thus expressed the
same "inferiority complex" nurtured by the social sciences, envious of the extraordinary
achievements achieved by mathematics and the natural sciences.

Among the legal philosophers included in the scientific trajectory, reference should be
made to the Danish Alf Ross. An exponent of Scandinavian realism, he formulated the
interesting notion of degrees of validity of legal norms. By bringing validity closer to the
norm's effectiveness, it moved away from the validity/invalidity binomial. The norm will be
more valid the more effective the judges' decisions give it. Therefore, a norm that is
incompatible with a later or higher hierarchy is not invalid; is ineffective, as it is not
generally applied by judges.

In terms of legal epistemology, the graduation of the validity of the norm in terms of
effectiveness, measured by the judicial decisions that apply it, leads to a normative agenda
of scientificity that is very different from that dictated by the pure theory of Law. Once
Ross' assumptions were accepted, a scientist would be a jurist who managed to anticipate
the probability of a certain judicial decision. The method would consist of knowing how
the judges are interpreting the norm under study, identifying the different interpretative
aspects that surround it, in order to be able to anticipate not the decision to be taken in a
specific case, but its probability. The jurist would, therefore, be a scientist to the extent
that he managed to state, based on the examination of jurisprudence, whether there is
greater or lesser, and to what degree, the chance of being given, in a specific case, a certain interpretation to

At the same time as the emergence of Scandinavian realism, the philosophy of law opened up

The Philosophy of Law was born in the 20th century. Of course, philosophers have
always reflected on justice and law (Aristotle, Kant, Bentham, Hegel). But while philosophers
philosophize, there is not yet properly philosophy of law. This specialized knowledge
arises when jurists start to do philosophy.

Machine Translated by Google
As I understand the issue, depending on the purpose of the knowing subject who
focuses on the norm, his knowledge will have scientific status or not. If you aim to
understand the reasons why a certain society was governed by a certain norm, the
conclusions you reach can be scientific, if the correct method is adopted. But if the
objective is to understand how a legal norm should be interpreted, the conclusions
reached will not be scientific. Science, here, is knowledge whose statements can be
valued as true or false. The truth value (faithful description of the real) is attributable
to that statement submitted to methodical verification subject to confirmation and
reconfirmation. No statement about how a legal norm is interpreted meets this
condition.

It is a rhetorical knowledge. We, jurists, know how to convince the interlocutor
that norm x should be interpreted in way y and not in way z. We know how to operate
with a given repertoire (text of the law, implicit principles, hermeneutic rules,
jurisprudence precedents, doctrinal lessons, etc.) to try to make the interlocutor,
also familiar with the same repertoire, share the same conviction about the
interpretation of the norms that should guide the overcoming of a certain conflict.

to a perspective quite different from that explored by normative epistemology. An
anti-positivist (anti-scientificist) rupture redirects the philosophy of law. Legal
epistemology abandons the normative bias of its founding trajectory and returns to
the description of its object, legal knowledge. The main philosophers of law who
inspired the rupture were Theodor Viehweg (in his study on the topical nature of
legal concepts), Chaïm Perelman (with the rescue of rhetoric in the Aristotelian view,
as knowledge worthy of attention) and Tércio Sampaio Ferraz Jr. ( when classifying
legal dogmatics as technology).

Technology, in turn, is the knowledge of appropriate means to achieve ends
external to knowledge. Anyone who knows the interpretation (better, the
interpretations) of a given legal rule, knows what means (more or less fair, more or
less efficient, etc.) exist for society to deal with conflicts of interest.

In doctrine, scientific and technological statements come together, but not in
equal measures, with those from the latter category significantly predominating.

Jurimetry is the approximation of two types of knowledge, legal and statistical. It
can be defined as knowledge about the measurement of legal facts, understood as
judicial and administrative decisions, signing of contracts, carrying out corporate
operations, decreeing bankruptcy, filing company recoveries, growth in the number
of lawsuits in progress , relationship between the number of judges and the
population, etc. This new legal discipline helps the two levels of doctrinal knowledge.
To science, it helps to identify the norms that should be adopted to guide the
overcoming of conflicts of interest. It is an instrument of legal public policy. With
legal technology, it collaborates in the definition of argumentative strategies and in
the rationalization of a new type of argument.

It collaborates with the science of Law, providing the methodology for gathering
the empirical data necessary for the formulation of legal public policy. I register that,
in Brazil, legal professionals are formed by acquiring only skills related to the
resolution of conflicts of interest, in the judicial or arbitral scope, guided mainly by
the law, but also by some other references (case law, doctrine, uses etc.). This is
exclusively the knowledge transmitted to law students, since the implementation of
the first legal courses among us, in 1827.

Machine Translated by Google
In this context, it is clear that, for some time now, attention has been paid to what we
might call "quantitative" arguments, that is, based on statistical measurements. Its
use is still incipient in judicial reasoning, jurisprudence and doctrine, but a
considerable increase in this "new" form of argumentation is foreseen.

This is because they are different knowledge. On the one hand, those that need to
be mastered by the competent legal professional to act as a lawyer, judge, prosecutor,
etc.; and on the other, those necessary for those involved in broader changes in the
legal system. It is inappropriate and, in general, leads to disastrous results, to try to
act in legal public policy, making use only of the knowledge employed in the
application of the Law. In a petition, it is usual and appropriate for the lawyer to rely
on authoritative arguments, represented by transcripts of lessons from recognized
and consecrated doctrinaires. It is also usual and appropriate for the judge to
mention, in the Judgment, that he is judging based on precedents in the same sense.
They are rhetorical resources, more or less efficient, but undeniably appropriate for
the institutionalized solution of conflicts of interest, in the judicial or arbitration
environment. Well then, arguments from authority and precedents are irrelevant
when discussing, for example, the pertinence and content of the amendment of a
certain law. More than irrelevant, they are harmful to the discussion around this and
other public policy hypotheses - when different methods must be used, including Jurimetry.

Training professionals to act in the institutionalized solution of conflicts of
interest, guided by the law and other references, does not mean, in short, preparing
them to participate in debates around public policies in the legal area. The jurist,
however respected and competent he may be in his work, does not necessarily meet
the conditions to contribute, in a satisfactory way, in the elaboration of bills, drafts
of infralegal norms, in the improvement of the administration of Justice, in the
organization of movements academics, in the improvement of legal education or in
any other public policy of a legal nature.

superior juridical, with the training of professionals equally able to formulate, develop
and implement public policies.

The arguments are historical, that is, those that are convincing in a given time or
situation may not be convincing in other contexts. Arguing, today, around the
autonomy of the will and binding to the contract, adopting the same foundations as
those that circulated a hundred years ago, is clearly unconvincing, inefficient.

In terms of legal technology, Jurimetrics also has a unique contribution to make.
Technological knowledge of Law is, as seen, composed of argumentative statements,
designed to convince the interlocutor. Every scientific discourse has some amount
of argumentation, since it is always addressed to other scientists in the same area of
knowledge. But knowledge of physics, chemistry, biology, etc. don't exhaust yourself
in argument-building technology. Lawyers, judges and scholars, if they know
anything, know how to formulate convincing arguments, starting from certain
premises (the rule of law, mainly).

The most competent lawyer or magistrate simply may not be in a position to
participate efficiently in public policy discussions, even those related specifically to
his area of professional expertise. A renowned professor of procedural law is not
necessarily prepared to contribute to improving the management of the Judiciary;
the best-prepared civil lawyers eventually fails to collaborate, in any way, in the
reform of the Civil Code - these are examples that could perfectly relate to any other
field of legal knowledge: commercial, tax, criminal, etc.

Machine Translated by Google
The quantitative legal argument differs from the non-quantitative (qualitative?)
ones by one particularity. It can be mathematically (statistically) tested. Anyone who
intends to oppose the non-quantitative argument can question any passage in the
interlocutor's speech, but will never be able to subject it to any type of test. The
quantitative argument, in addition to being questioned in any passage of the
reasoning constructed (statistical methodology adopted, for example), can be
discredited in terms of the measurements made.

Brazilian Jurimetry was born at PUC-SP, in the research that preceded the
preparation of Marcelo Guedes Nunes' doctoral thesis, under my guidance. The
project, initially, was not focused on empirical research, mentioned only as a possible
derivation. In my role as advisor, I suggested that you pay greater attention to the
statistical study of the corporate question. I was concerned about a certain
improvisation and superficiality in some legal works that had ventured into empirical
studies and, therefore, I tried to encourage Marcelo to seek the adequate rooting of
his reflections in statistics. It was, of course, a huge challenge. Fortunately, it was
sent to one of those rare doctoral students, whose motivation is directly proportional
to the difficulties raised by the progress of the research; and has seriousness,
professional experience, academic sensitivity and intellectual capacity more than enough to face them.

In other words, whoever argues that the significant majority of judicial decisions
gave interpretation y to norm x, and quantifies this predominance of jurisprudential
understanding, can face counter-arguments of two orders. On the one hand, someone
may call into question the usefulness of the quantification itself (if a minority gave a
different interpretation to the same norm, another judge can also validly give it), or
the statistical methodology employed (not all the courts in the country were
considered; whether a long or short time cut etc.). In these cases, we will still be on
the level of incommensurability, and the quantitative argument has nothing specific
in relation to the others. On the other hand, someone can redo the calculations,
starting from the same raw data and following, or not, the same methodology, and
arrive at different quantitative results. Here, the test of the argument took place, in a
counter-argumentative resource that does not find a similar one in the context of
traditional legal argumentation.

If the perception that quantitative arguments and counter-arguments tend to grow
is relevant, in the context of legal discourse, legal professionals should become
familiar with Jurimetry. Both to build arguments and to efficiently test those of the
opposing party.

At that time, my brother, Flávio Ulhoa Coelho, happened to be the Director of the
Institute of Mathematics and Statistics of the University of São Paulo (IME-USP). he promptly

Of course, if the lawyer of one of the parties, based on accurate jurimetric
research, articulates a quantitative argument, demonstrating that 100% of the judges'
decisions on similar causes, gave prestige to the interpretation for which he strives,
the other lawyer will always be open to the alternative of questioning the relevance
of this. After all, it is the essence of Law to evolve also thanks to innovative interpretations accepted by

Sustaining the pertinence, or impertinence, of a certain understanding of the law in
statistical data corresponds to the way of organizing reasoning that is entirely
compatible with contemporary discourse; it is inevitable that it reflects on the legal argumentation.

Of course, this last lawyer will have the task of convincingly arguing for the judge to
be convinced that all his fellow magistrates, up to that point, were wrong. The task
will be made difficult by the need to counter the quantitative argument, but it will by
no means be made impossible.

Machine Translated by Google
Marcelo not only accepted the challenge and handled it, he went beyond, much beyond.

Brazilian Jurimetry, under Marcelo's leadership, impresses specialized audiences
around the world. The other preface to this book, Christoph Engel, director of the Max
Plank Institute, testifies to how much Brazilian legal empirical research brings together
unique conditions to participate in a privileged way in foreign and international forums.

While developing research for his doctorate, he adopted fundamental initiatives for
the dissemination and consolidation of empirical studies in Brazilian Law. It held
annual meetings, with the aim of bringing together legal scholars interested in
statistics, and statisticians interested in legal issues. These are events in which all
those involved in legal empirical research in Brazil, each with their specificities and
motivations, find a favorable environment to exchange experiences, disseminate
results and mutually enrich themselves.

In 2012, Marcelo led the founding of ABJ - Associação Brasileira de Jurimetria. This
entity has already developed important studies for the National Council of Justice, the
Court of Justice of São Paulo, the Public Ministry, the National Confederation of
Industry, the Sou da Paz Institute, among others.

Marcelo defended his thesis at PUC-SP on May 31, 2012. It was the first and,
probably to date, the only multidisciplinary doctoral committee with jurists and
statisticians at a Brazilian University. The thesis was approved with full marks, after a
very rich epistemological debate and fertile discussion about the perspectives opened
by the innovative approach to empirical research and its limits, without neglecting to
consider the elucidations reached on the societal issue. Many of its conclusions
guided the norms of the new Code of Civil Procedure, governing actions for the partial
dissolution of a company (art. 599 et seq.), and provided subsidies to the corporate
law book of the Commercial Code Project pending in the Senate (PLS 487/2013).

This book is based on the first part of Marcelo's thesis. I insisted a lot that he find
time, in his always busy schedule, to proceed with the preparation of this work for
publication purposes. I insisted, in fact, because I considered this to be my last task
in guiding the seminal work of Brazilian Jurimetry. I am very happy with the publication
of this book, which makes the fundamentals of Jurimetry accessible to Brazilian law
students and professionals.

From the beginning, he was concerned with inserting Brazilian Jurimetrics into the
main circles dedicated to empirical research into Law abroad. He participated in the
events of the Society of Empirical Legal Studies - SELS, gave a lecture at the Max
Plank Institute and always invites professors who are international references in the
field, such as Michael Heisel, Kuo-ChangHuang, Cristoph Engel and Theodore Eisenberg, to the annual Brazilian

understood the scope of the project to seek a closer academic dialogue between law
and statistics. Flávio created the conditions for Marcelo's fruitful approach to some of
the most distinguished Brazilian mathematicians and statisticians: Carlos Pereira,
Adilson Simonis, Julio Stern, Sergio Oliva. This approach also involved a young
statistician, Julio Trecenti, then in his final year of graduation.

In June 2014, Marcelo and I attended the founding meeting of the Global Society of
Empirical Legal Studies -GSELS. Representatives of prestigious academic institutions
from the USA, Germany, Italy, India, Israel, Singapore and England participated in this
important event. ABJ is the only entity in Latin America invited to join the GSELS.

Machine Translated by Google
Fabio Ulhoa Coelho

Full Professor of Commercial Law at PUC-SP.

Jurimetry is a new discipline in the inexhaustibly fascinating legal knowledge. I had
the privilege of accompanying his birth in Brazil. I dare say that my happiness in
preface to this book is perhaps equal to that which takes the spirit of the astronomer
when he discovers a new star.

© of this issue [2016]

Machine Translated by Google
Jurimetry

PRESENTATION

2018 - 07 - 17

PRESENTATION

The empirical movement in law is advancing rapidly. All the big US schools want
to hire empirical lawyers and scholars. The Conference on Legal Empirical Studies
has had a growing audience. The Journal of Empirical Legal Studies is thriving, as
are its competitors. Increasingly, the best of Academia Jurídica is published in
specialized journals and much of this work is empirical. And like every change of
reference, the emergence of legal (read: quantitative) empiricism has more than one
cause. Technology is certainly not the most important thing. Some statistical
techniques that arguably suit legal research questions require powerful computation:
Bayesian analysis, structural equation models, and bootstrapping are examples.
More importantly, the almost ubiquitous availability of computers makes it possible
to compile huge databases that originate directly from the court system. Some
countries have shifted the exchange of documents at trial to computerized interaction.
Then the raw material of the empirical work will already be available in an electronic
form. Even better if that raw material isn't just images of text, but already encoded
and ready for analysis. Brazil is at the forefront of this development. It has a database
of unique, unprecedented breadth and depth. And Marcelo Guedes Nunes is at the
forefront of analysis, generating these datasets, making them available to the legal
community and exploiting them to increase our knowledge of what the law is.

After all, Law is not (just) another Social Science. For Law, sophistication is not a
value per se, nor is elegance, or the surprising ability to solve a seemingly intractable
statistical problem. Law governs people's lives. A good scholarly production does
not lose touch with this final purpose of any legal reasoning. How does the
fundamentally different definition of research purpose affect the use of quantitative
techniques in legal research?

When legal theorists began to organize an argument around the "scientific" concept
of causality, natural science had already become skeptical. He reports on the many
reasons why one might question Mother Nature's determinism and explains why a
probabilistic model can seem appealing. Now, if physics and biology already move
away from deterministic models, shouldn't those interested in social reality adopt
the same posture a fortiori? Undoubtedly, the interaction of human actors adds so
many other levels of complexity that in this field it seems even less plausible to
assume the existence of certainties or "natural laws".

Everyone would understand if Marcelo Guedes Nunes had written a book that
extolled the technical achievements of his country and the role that the Brazilian
Association of Jurimetry - of which he is president - plays. Such a book could also
be an opportunity to boast your technical skills or surprise the legal community in
Brazil and abroad with the incredible power of Brazilian data. He wisely chose not to.

This question both motivates and organizes the book. It begins with a troubling
question from legal theory: Is the statistical approach to legal questions inappropriate
in the first place? Digging deep into legal philosophy and contrasting it with (nonlegal)
epistemology, Marcelo Guedes Nunes gives us a surprising answer.

Machine Translated by Google
Against this background, Marcelo Guedes Nunes defines the appropriate scope of
empirical knowledge in Law. He makes his readers understand the seemingly
paradoxical nature of the enterprise. Didn't we want to "become scientific" in order to
give more confidence to our judgments? And now you come to tell me that all
quantitative statements are, at best, probably true?! Or, more precisely, that the
probability of a statement being false is small enough that we can assume it is true,
"small enough" here being a matter of convention. Why would a lawyer be willing to
adopt such a weak methodology? Marcelo Guedes Nunes gives two answers: (1) first,
because the (social) world of interest of Law is not deterministic, (2) and even if some
phenomenon of interest really were deterministic, Law would not have direct access
to it . He needs to make inferences from the observations he is capable of making.

Christoph Engel

Executive Director of the Max Planck Institute, Humanities Sector.

The book thus provides a highly valuable service to the legal community. He not
only introduces jurists to the nature, power, and limitations of quantitative empirical
analyses. The book also makes lawyers understand what quantitative analysis can
really give them: no magical extinction of legal uncertainty, but a much more controlled
way of dealing with that uncertainty, of reducing it to the limit of the possible and of
quantifying the remaining risk. of fallibility. For this reason, the book should not be
read only by legal scholars interested in better understanding this empirical movement.
Nor should the book be read only by traditional lawyers interested in better
understanding what they stand to gain from quantitative analysis. The book should be
read by all legal practitioners interested in a lucid analysis of the nature of legal
decision-making.

© of this issue [2016]

Machine Translated by Google
1

2018 - 07 - 17

Jurimetria
CHAPTER 1. THE END OF BIBLIOGRAPHIC WEIGHTLING

Chapter 1. The end of bibliographic weightlifting

Interestingly, the social sciences were still trying, at the end of the 19th century, to realize a deterministic
ideal, when the great classical sciences had already become disillusioned with the explanatory potential of
theories based on the idea of exact natural laws. Human behavior researchers - in essence, highly complex,
uncertain and variable - were slow to become aware of the value that statistical techniques could add to
social research and insisted on developing mechanistic theories about man.

The 20th century saw a drastic change in the relationship with knowledge. The classical model of
science germinated in Greek antiquity, which repudiates uncertainty and admits only knowledge associated
with absolute truth, was gradually replaced by a stochastic model, which accepts variability and uncertainty.
In several branches of science, including the more traditional ones (such as physics and chemistry),
knowing no longer means having control over all causes of production of a fact and over its future behavior.
The researcher's object is no longer the discovery of invariable natural laws and universal cogency, capable
of predetermining the results of experiments with any degree of precision.

This scenario was modified throughout the 20th century and today geography, medicine, sociology,
administration and economics, to name a few examples, are branches of

"The true journey of discovery consists not in looking for new landscapes, but in

Currently, researchers are aware that the complexity of certain processes makes it impossible to reduce
their causes to a deterministic model. Science has abandoned its claim to be exhaustive in the investigation
of causes and accurate in predicting the future, to admit an incomplete knowledge, which seeks only to
make less mistakes. Within this new and more modest approach, traditional components of scientific
thought take on new features. Instead of natural laws, we have probabilistic models. Instead of deterministic
causal relationships, we have correlation and regression indices. And instead of certain results, we have
distribution frequencies of possible

results.

Such changes led historians to coin the expression "Statistical Revolution" to describe this shift in
trajectory described by human knowledge. From the second half of the 19th century, statistical methods
gradually began to be used in all fields of knowledge, including the bastions of classical scientific thought
such as astronomy and physics. The focus of scientific research ceases to be the isolated individual,
governed by mechanical laws, and becomes the study of the different characteristics of a population. The
new model of knowledge begins to be based on the pragmatism of statistics, with its techniques for
controlling uncertainties and measuring variability, and aimed not at the exact study of a single individual,
but at the approximate description of entire populations.

see with new eyes"

Machine Translated by Google
humanities that largely use statistical techniques and probability models.

Law is a latecomer science in this movement towards statistics. The jurist studies
the laws without worrying about their practical results. Law graduates (future lawyers,
judges, legislative consultants, prosecutors and legal directors of companies) are
trained to discuss ad nauseum all the hypothetical meanings attributable to a law,
but, due to the lack of basic knowledge in statistics and empirical research, they do
not have any preparation to verify the practical consequences that these senses
produce. Our theses are still carried out exclusively within libraries and are limited
to compiling mountains of citations, in the academic modality that I usually call
bibliographic weightlifting. As a result, we know almost everything other jurists have
said about the law, but we know almost nothing about what goes on in the outside
world. This bibliophile fetish of ours is reminiscent of the joke of the fanatical
gourmet who defined life as a boring interval between meals. For jurists, life is that
boring space between one visit to the library and another.

Part of this alienation can be explained by the idea of mechanical jurisprudence.
Fans of mechanical jurisprudence believe that the law predetermines judicial
decisions and that, therefore, its meaning can be understood independently of the
courts' practice. Within this mechanistic logic, the judge has no will of his own and
serves as an inert means through which the meaning of the law is manifested. Hence
the reason for the study of court practice to be overlooked as a predictable particular
manifestation of the general meaning of one of the possible meanings of the law,
within the limits in which it was defined by doctrinal theory.

Not by chance, the mechanistic view of law has always found a focus of resistance
in commercialist lawyers. The entrepreneur's creativity and market dynamics meant
that the role of the courts in the construction of commercial law was different. If in
public law (as in tax or criminal law) the idea of legality

No laboratory launches a medicine or treatment without its effectiveness and side
effects having been subjected to rigorous statistical control tests. Administration
and sociology also carry out research using empirical methodologies, largely
dependent on the elaboration of statistical inferences. Economics was, without a
doubt, the human science that best knew how to explore, through econometrics, the
potential that statistical techniques offer to explain people's behavior. This is the
reason why economics went from being a subject in law curricula to becoming the
most influential social science in history in just over 50 years.

This premise is, however, false. The meaning of the law as defined in the manuals
is just one of the factors that interfere in the conformation of concrete law, defined
here as the set of individual orders addressed to specific citizens (such as judgments
and contracts). If we take, for example, a court ruling, it is reasonable to assume that
its ultimate meaning is the product not only of what the law says, but also of an
intricate and complex set of social, economic and cultural factors involved in an
elaborate psychosocial process of conviction. , influenced by factors such as the
political and personal values of the magistrate, empathy with the parties, the line of
argument chosen by them, the life experience of the judge, the institutional pressure
exerted by control bodies of the Judiciary, the meaning of precedents given in similar
cases, among countless others. To evaluate concrete law, investigating the
hypothetical meanings of the law only solves part of the problem. It is also necessary
to build models capable of describing individual cases and understanding how they
arose and why they are being resolved in this or that way.

Machine Translated by Google
Through Constitutional

Amendment n. 45 of December 30, 2004, the National Council of Justice - CNJ was
then created, which aims to control the administrative and financial performance of
the courts and supervise the fulfillment of functional duties by the judges.

strict unilaterally imposed by state authority makes sense, in private law, especially
in commercial law, it is common for disputes to arise involving situations without
legal provision, in which judges need to create law where it does not exist. It was like
this with corporations, with franchising, with the internet and it will continue to be
like this with bioengineering, robotics and other cutting-edge areas.

With the aim of collecting data about the system, the CNJ began a series of
research to understand how many and what processes were taking place in Brazil,
with special interest in investigating the causes of slowness. These initiatives sought
to provide elements for the adequate fulfillment of Resolution no. 70 of March 18,
2009, which instituted for the first time the elaboration of strategic planning for the
Judiciary. The planning aimed to build an overview of the situation of the courts,
including the number of judges, employees and processes. Therefore, one of the
first surveys conceived was called "100 biggest litigants" and its purpose was to list
the one hundred largest entities and legal entities involved in processes in the
Brazilian Judiciary, excluding the Public Ministry and the processes that were being
processed in the Criminal, Electoral, Military and Children and Youth.

The first data, released in 2011 with files not filed until December 31, 2010, revealed
surprising numbers. The report estimated that these 100 entities (including Banco
do Brasil and União) accounted for approximately 20% of the total number of cases
pending in the country.

Hence the reason why American commercialists of the early 20th century, such
as Karl Llewellyn, Theodore S. Hope, Jerome Frank and William Underhill Moore, led
efforts to recognize the creative role of courts in the formation of law, in the
movement known as legal realism. In Brazil, the arrival of this realistic vision ended
up taking a while, but today it has finally arrived and has brought together high-level
researchers from most regions of the country, including professors from the law
schools of the Pontifical Catholic University of São Paulo, the University of São Paulo
and the Getúlio Vargas Foundation in Rio and São Paulo, to name a few examples.
The movement in Brazil is also characterized by the active participation of the
Judiciary which, pressured by a slow system and incapable of managing an
enormous mass of tens of millions of cases, ended up being forced to react. The
example of the survey of the 100 largest litigators illustrates how Brazilian judges
have been participating in this transformation. Until 2004, the Brazilian Judiciary did
not have a central body dedicated to managing its operation. At that time, the
president of the Federal Supreme Court assumed the administration of the judicial
system without even knowing how many cases were being processed in the 91 courts of the federation.

In addition, it was

the Public Power itself, at the Federal, State and Municipal level, which was
responsible for most of the cases involving the biggest litigants, such as the National
Institute of Social Security - INSS, with 22.33%, alongside heavyweights of the
economy, such as Caixa Econômica Federal - CEF, with 8.50%. This concentration
of litigiousness in a few people and in specific branches of economic activity shed
light on the discussion on the improvement of jurisdictional provision. In addition to
the traditional solutions, involving reforms in the structure of justice and procedural
legislation, it was perceived that the solution to the clogging of the courts involved
the creation of a channel of understanding with these entities. A significant part of
the problems could be resolved if only fifteen of those hundred people sat down at a
table for a frank conversation with the Judiciary. Thus, for the first time, a consensus
was reached that not only judges and legislators had a role to play in solving the problem of hyperlitigation,

two

4

3

Machine Translated by Google
And that's exactly what happened. The discomfort arising from the inclusion of their
names on the list and the fear of reprisal encouraged a healthy race among the major
litigants to reduce this mass of processes. Banco Itaú, for example, carried out a review
of its internal management practices to distinguish cases in which the bank had a real
right to be defended, from others in which an error against the customer had actually
been committed. For these last cases, a policy called defense of the agreement was
instituted , in which the bank's objective was not to win the lawsuit, but to obtain a
friendly settlement with the client. Caixa Econômica Federal also started a policy of
withdrawing proposed appeals against contrary theses already pacified in higher courts
and which had been brought only to fulfill the institutional duty to appeal to the end. In
2014, these efforts resulted in an agreement between banks, governments and telephone
companies around a National Non-Judicialization Strategy (Enajud), with the aim of
unburdening justice.

Of course, the problem of litigation in Brazil is far from being resolved and since the
creation of the CNJ the numbers have been getting worse. But the lesson of this first
decade reinforces the need for public administrators to put aside improvisation and
intuition in order to become professional and objectively investigate how the legal order
works. And that's what empirical research is about: knowing reality to solve problems.
This statement may sound like a platitude, but for the law it still expresses a truth that is
not always remembered: a serious scientific effort must first investigate reality and then
propose solutions. It is the impartial observation of things that allows us to understand
how the system operates and detect the causes behind the problems in our daily lives.
Jurists need to get off their books, leave the libraries for a moment and start investigating
the real world. Literary erudition was already impressive in the past, before access to
information was popularized on the internet.
Today, what is impressive is the creation of an original solution to an effective problem.
It has never been so easy to quote others' ideas. It's hard to come up with an original idea.

Associação Brasileira de Jurimetria - ABJ, to name a few. What is this change
due to? I believe that several factors collaborated, from the emergence of new
technologies to the increase in the complexity of legal activity in general. There are
several explanations, of which I list here the six that I consider the most important.

but the big litigants also needed to review their internal practices to combat the underlying
causes of this colossal volume of proceedings.

First, the insufficiency of theoretical efforts to understand the law and promote
reforms. Second, the increase in complexity and number of legal institutes in Brazil,
creating a mass of cases impossible to manage without the help of empirical
methodologies. Third, but not least, the computerization of courts, local authorities, law
firms and entities linked to law in general, which facilitates access to data on the
functioning of law. Fourth, the development of statistical techniques and the increase in
the calculation capacity of computers. Fifth, the increasing influence of empirical research
methodologies in the social sciences. And, finally, sixthly, the development of society,
which is increasingly demanding quality services.

It is true that, despite some isolated pockets of resistance, this is a message that
many legal practitioners have already understood. In parallel to the CNJ's research
programs, several empirical investigation efforts began to emerge in Brazil, including
initiatives by the Getúlio Vargas Foundation - FGV, the Brazilian Society of Public Law -
SBDP, the Pontifical Catholic University of São Paulo - PUCSP, the of Applied Economic
Research - IPEA, the University of São Paulo - USP and the
6

5

Machine Translated by Google
FOOTNOTES

Namely: 24 courts of Labor Justice, 27 courts of State Justice and 5 federal courts, 5 special courts

(STF, STJ, TST, STM and TSE), 3 courts of State Military Justice and 27 courts of State Electoral

Justice.'

[www.cnj.jus.br/images/pesquisas

Available [espaco-vital.jusbrasil.com.br/noticias/2629087/saiu-o-listao-dos-maiores-litigantes-najustica-brasileirao-estado-do-rs-figura-num-indesejavel-1-lugar-como
-author-and-or-reuin:Availablejudiciarias/pesquisa_100_maiores_litigantes.pdf].in:In
the original: Le véritable voyage de découverte ne consiste pas à chercher de nouveaux
paysages, mais à avoir de nouveaux yeux. Marcel Proust in A la recherche du temps perdus.

two

3

4
1
Every time statistical techniques are applied to understand some dimension of
law - for example, a type of judicial conflict, or the behavior of witnesses in court, or
bills that deal with criminal matters in the National Congress, or fiscal executions - a
plethora of new results appear, many of which are counterintuitive. It's like traveling
for the first time on a newly discovered continent, where everything is new and
interesting. Empirical research opens our horizons of knowledge and allows us to
investigate the concrete plane of law and the institutional spaces where norms are
created. This is privileged access to what actually happens in the legal world: which
real conflicts are knocking on the doors of the courts, which aspects of the law do
not meet the demands of the population, which concrete effects a new legal provision
has on society.

Due to the richness of the results, the pressure to use statistics in the study of
law is giving rise to a new area of knowledge: Jurimetrics . Jurimetrics starts from
the premise that law is not limited to the theoretical study of laws. We must also
study the decision-making processes through which all norms, general and
individual, are formulated. Jurimetrics also assumes that this study needs to be
concrete, that is, it must locate its object in time and space and investigate the main
factors capable of interfering in its results. And Jurimetria believes that the study of
decision-making processes must abandon deterministic pretensions and admit in
the academic environment what has always been admitted in the professional
environment: that the complexity of the legal order does not allow absolute
statements and that the law, like everything that involves the will human, is variable
and uncertain. Therefore, understanding the law is, first of all, describing its variabilities and controlling

Machine Translated by Google
5

6

© of this issue [2016]

in-the-demands-placed-in-the-state-justice-departments-all-over].

Available at: [www.gazetadopovo.com.br/Economia/conteudo.phtml?id=1480836].

For an overview of legal consequentialism in Brazil: Salama, Bruno Meyerhof and Pargendler,
Mariana. Law and Consequence in Brazil: in search of a discourse on the method, In:
Revista de Direito Administrativo (RDA) 262 (2013): 95-144.

Machine Translated by Google
Greek contributions to mathematics emerged as a result of a civilization focused on
theoretical speculation about the world, a cosmopolitan culture that valued exact and
universal knowledge. Mathematics, philosophy and religion made up, for the Greeks, a
common area aimed, at the same time, at understanding the genesis of the world, solving
practical day-to-day problems (such as, for example, the cure of diseases and the
development of civil construction projects) and the moral rules of good living (what is
good, how to act correctly). Understanding the relationship of the Greeks with knowledge
presupposes understanding the role of mathematics as a reference for perfect and
rigorous knowledge, associated with absolute, infallible knowledge, disconnected from the needs of everyday

The story of Pythagoras (570 BC-495 BC) illustrates this relationship. Little is known
about the life of this philosopher. The main records date from the 3rd and 4th centuries
after Christ and are authored by Diogenes Laertius and the Neoplatonists, Porphyry and
Iamblichus. Pythagoras is known to have founded a religious movement known as
Pythagoreanism, which flourished in what is now the Crotona region of southern Italy.
His followers, the Pythagoreans, are responsible for the first effort to create a general
philosophy based on mathematical abstraction. The Pythagorean "ecolé" (from the Greek
ÿÿÿÿÿ, or idleness) was dedicated to the observation of the world and theoretical bios
(from the Greek ÿÿÿÿ ÿÿÿÿÿÿÿÿÿÿ, or life of contemplation), disconnected from work and
practical problems. The Pythagoreans venerated numbers and geometric figures and
recognized in them an abstract and independent existence that, despite being immaterial,
is capable of offering resistance to the intellect. For the Pythagoreans, numbers
constituted the immutable essence of the universe, and there was a coincidence between the acquisition of

As early as 600 BC, Thales of Miletus recorded that the diameter of a
circle divided it into two identical semicircles, and that in an isosceles triangle, the angles
facing equal sides were also equal. The Pythagoreans, in their veneration for numbers
and geometric shapes, transformed geometry into an object of worship and, consequently,
already in the fifth century before Christ, had developed much of the geometry that
centuries later would compose books I, II, IV and VI of Euclid's Elements.

The Greeks were known for their mathematical genius, with seminal contributions to
geometry, arithmetic, trigonometry and algebra. The list of great Hellenic mathematicians
including Diophantus of Alexandria (240 BC-170 BC), Archimedes of Syracuse (287
BC-212 BC), Euclid of Alexandria (360 BC-295 BC), Pythagoras of Samos (570 BC-495
BC ) and Thales of Miletus (624 BC-546 BC), coincides with a significant portion of the
parentage of this subject.

"The most important conceptual event in physics in the 20th century was the
discovery that the world is not deterministic. Causality, long the bastion of
metaphysics, has been overthrown, or at least shaken: the past does not exactly
determine what happens in the future. future. This event was preceded by a more
gradual change. During the 19th century, it could be realized that the world can be regular and still
universals of nature. A space had been opened up for chance."

I. Greek Mathematics

1

two

3

Chapter 2. Determinism and the Statistical Revolution

Jurimetry
CHAPTER 2. DETERMINISM AND STATISTICAL REVOLUTION

2018 - 07 - 17

Machine Translated by Google
It is said that the Pythagorean school would have separated into two currents. One
more focused on religious and ritualistic aspects, called "acousmaticoi" (from the Greek
ÿÿÿÿÿÿÿÿÿÿÿÿ), meaning those who speak and hear, and another dedicated to the study of
numbers and their relationships, known as "mathematicoi" (from the Greek ÿÿÿÿÿÿÿÿÿÿÿ), or those who investigate.

Another example is Plato's relationship with geometry. Plato (428 BC-348 BC), alongside
Socrates, of whom he was a student, and Aristotle, of whom he was a teacher, forms part
of the great triad of classical Greek philosophy. Plato lived in Athens and is the author of
the most important philosophical record of Antiquity, the thirty-six Socratic dialogues. The
intimate connection between Plato's philosophy and geometry appears in the dialogue
Timaeus in which Socrates debates with Critias, Hemocrates and Timaeus of Locri, the
latter a Pythagorean philosopher, his cosmogony (ÿÿÿÿÿÿÿÿÿÿ, or origin of the world).

The "acousmata" were orally transmitted maxims relating to habits, such as, for example,
the prohibition against eating broad beans, picking up crumbs on the floor, eating meat
(the Pythagoreans were vegetarians) and wearing white in religious ceremonies. The
"mathematicoi" dedicated themselves to the study of mathematical demonstrations and
relationships whose truth was independent of oral traditions and could be revealed directly by the intellect.

In the dialogue, Timaeus distinguishes the physical world (changeable, unstable and
precarious) from the eternal world (immutable, fixed and intelligible) and, starting from the
premise that everything has a cause, suggests that the physical world would have been
created by a first cause, or causeless cause, called Demiurge, based on an ideal model of
perfect geometric shapes. Timaeus claimed that the universe is spherical, as the sphere is
the most perfect, proportional and omnimorphic figure known, and reduces the apparent
reality of the world to five basic polyhedral figures, the first four associated with the four
natural elements: the tetrahedron with fire, the octahedron in air, the icosahedron in water
and the cube on earth. According to this geometric cosmogony, with the exception of the
cube, the other three polyhedra result from combinations of equilateral triangles (four,
eight and twenty) and the faces of each of these polyhedra can be divided into straight,
scalene or isosceles triangles, with the properties Physics of the elements (such as the
immobility of earth or the ability of water to put out fire) were attributed to the relationships between the sides

One of the main "acousmata" attributed a mystical character to the first four numbers
(1, 2, 3 and 4), the "tetraktýs", due to the relationships and mathematical demonstrations.
For example, the four numbers of "tetraktýs" combine in pairs, forming the musical
intervals of the octave (2/1, 3/2 and 4/3). Furthermore, the sum of the "tetraktýs" is
equivalent to the perfect number 10, which is the second triangular number, after six.
Triangular numbers have the same structure as quadratic numbers, but with one difference.
While quadratic numbers are those that, organized in points, draw a square, for example,
the number 16 makes up a square with 4 points on each side (hence the name square root),
triangular numbers draw an equilateral triangle, for example, the number 10 makes up a
triangle with 4 points on each side.

5

4

Machine Translated by Google
The Greeks identified the knowledge of truth with exact and invariable formulations.
His model of science was arithmetic and geometry. Initially developed for practical
applications, such as measuring the height of buildings by shadow length and plotting
rural areas, geometry (from the Greek ÿÿÿÿÿÿÿÿÿ, measure of the earth) and arithmetic
(from the Greek ÿÿÿÿÿÿÿ, number) were absorbed from Egypt and Asia by the Greek
culture and there they developed until they were united in an abstract theory. Under the
influence of Greek thought, mathematics ceased to be a measurement tool to become an
autonomous language, with elements and rules independent of the concrete world. What
was an applied science gradually became an abstract form of contemplation of ideal
objects turned to the immaterial, perfect and immutable reality of numbers, geometric
figures and their relationships.

Therefore, for classical Greek thought, knowing meant the same as apprehending an
immutable, invariable and absolute truth. Knowledge was situated in the field

Most of the ideas in the work were not his own. However,

Euclid's genius appears not in the conception of the theorems, but in the structuring of
their presentation. The Elements is the first work that systematizes a field of knowledge,
fixing self-evident axioms from which all theorems are deduced. There are twenty-three
definitions, five geometric postulates and five additional postulates, from which four
hundred and sixty-five theorems are deduced. The book forms a logically coherent
whole, in which each statement is proved, only accepted logical rules are applied, and all
reasoning is explicit.

But the best approximation to what mathematics (especially geometry) meant in the
Greek world is in the work of Euclid.

Despite being one of the cradles of mathematics, it is noteworthy that the Greek
civilization did not record any contribution linked to what is now called Statistics. It is
interesting to think about the reasons for the distance between this great mathematical
culture of antiquity and the set of measurement methods most used today.

Euclid lived and taught in Alexandria (around 300
BC) and, once again, little is known about his life and work, much of which has disappeared.

The concern with the description of the geometric structure of the world also appears
in book VII of the Republic, in which Plato prescribes what would be the ideal education
to be given to young Athenian citizens, who would be responsible for the government of
his ideal state. Education should be based exclusively on the four branches of
mathematics (arithmetic, geometry, stereometry and astronomy), considered the only
true knowledge and therefore useful and worthy of being transmitted. What is interesting
here is that the value of mathematics is not in the practical possibility of calculating
supplies or movements of war battalions, but rather in its ability to train intelligence and
bring the intellect closer to the ideals of goodness 8 and beauty .

Euclid is recognized as the first to use the expression "as we wanted to
demonstrate" (from the Greek ÿÿÿÿÿÿÿÿÿÿÿÿÿÿ - hòper èdei dèixai or, in Latin, quod erat
demonstrandum). There are no intuitions, digressions, fallacies, hidden reasonings or
unnecessary concepts, so that the argumentation can be accompanied, step by step,
from the premises (the postulates or axioms) to its conclusions (the theorems), through
a route as rigorous and economical. The work organized all knowledge of classical
geometry, which, since then, came to be known as Euclidean geometry, and set the
premises on which the subsequent study of the so-called Euclidean space was built.

Tradition says that he was educated by Neoplatonists, in Athens, and that he was
Ptolemy's teacher. Euclid wrote the most influential scientific work of all time, The
Elements, a compendium organized into thirteen books that contained all the geometric
knowledge of his time.
6

10

11

7

9

II. Aversion to Statistics

Machine Translated by Google
In its broadest version, determinism is the conception that the current states in the
universe are the unique and necessary result of the states that preceded them. Nature
is investigated as a system composed of chains of facts, all linked together by causal
relationships. Each fact is at the same time an effect of previous causes and a cause of
subsequent effects, in a chain in which there is no room for variation. Deterministic
thinking assumes that each event is entirely contained in its causes and, therefore,
exhaustive knowledge of the causes is sufficient to access integral knowledge of its
future effects.

As a result, despite being at the same time expert gamblers, inspired philosophers
and brilliant mathematicians, the Greeks were not able to unite these three qualities and
develop statistical methods and a theory of probability to deal with problems that
involved some degree of uncertainty and unpredictability. Israeli historian Samuel
Sambursky credits this difficulty to the rigid distinction drawn by the Greeks between
truth and probability. Plato, for example, separated truth from mere
possibility, which would be incompatible with the theoretical edifice of geometry, then
already in an advanced state of construction.

Peter Bernstein, American economist and historian, also sees in this aversion to
approximation knowledge the origin of the Greeks' relative lack of interest in probability.
Despite "probability being a discipline tailor-made for the Greeks, given their fondness
for games, their mathematical skills, their mastery of logic and their obsession with
proof", unfortunately this civilization never seriously "ventured into this fascinating
world", because for them the truth "was only what could be demonstrated by logic and
its axioms", disqualifying as precarious the apparent truths that could be demonstrated
only through empirical experimentation.

As a consequence, deterministic sciences aim to

Scientific determinism consists of the belief that the universe is a system governed
by absolute laws, which allow the rational prediction of any future event with any degree
of accuracy.

This separation between knowledge and uncertainty (to whatever degree) is
responsible for defining, over the next eighteen hundred years, the Western concept of
science. Due to Greek influence, the universe came to be seen as a system that could
be fully explained rationally, and knowledge was associated with theories capable of
apprehending, in its entirety, the explanation of its exact functioning, without any room
for doubt. Thus, the essential philosophical assumption of modern knowledge and the
antagonistic pole of statistical thinking was fixed: scientific determinism.

dogmatic of certainty, so that the contingent aspects of everyday life, such as the result
of a harvest, the chances of a game or a defeat in a battle, despite being relevant from a
practical point of view, ended up being relegated to the mystical speculation of the
oracles. Despite being skilful players, the Greeks understood that luck and fortune,
constantly present in all everyday problems, were opposed to mathematical knowledge,
so that the accidents and fortuities of earthly life did not constitute suitable objects for
the speculation of the philosophers and mathematicians. They were whims of
unfathomable divine designs.

Within a deterministic perspective, the position of each element of
the system is a necessary result of the immediately previous position of all other
elements. Knowing means accessing the relationship between initial and final conditions
of an event, including not only an omniscience of the causes involved, but also mastery
over the laws that govern the transformation between states. Thus, with knowledge of
an initial state and the laws that govern its transformation, it would be possible to
predetermine all future states of a system. 15
12

14

16

13

III. Scientific determinism

Machine Translated by Google
Democritus (460 BC-370 BC) is, alongside his teacher, Leucippus, the father of the
doctrine called atomism. The records of atomism are mainly due to Aristotle, who saw in
its mechanism and repudiation of theology a dangerous opposition to his own doctrine.
For the atomists, matter was not infinitely divisible and was composed of indivisible
particles, the atoms (from the Greek ÿÿÿÿÿÿ, without division), of different shapes and
weights, eternally in motion and mechanically interacting in the midst of chaos (from the
Greek ÿÿÿÿ, void ). For the atomists, the movement of the stars, earthquakes, the gallop of
a horse or human behavior were all effects of the movement of these particles.

The strength of determinism lies in the predictability of its statements, since, once the
initial conditions and transformation laws are known, there are ways to accurately
predetermine the future states to be assumed by the system. In determinism, time is
considered illusory. Once the laws of nature are known, understanding a given present
state would allow immediate visualization of the past state that originated it, and the future
state that will result from it. Furthermore, for the determinist, time is reversible, since past,
present and future are interchangeable, and uncertainties are the consequence of a
cognitive limitation, which prevents us from understanding the functioning of what we propose to analyze.

The search for a timeless and infallible knowledge of determinism, capable of
overcoming the contingencies and limitations of the uncertainties of earthly life, is also
associated with a scientific form of divinization of man.

The rigorous mechanical principles that govern this interaction of atoms and to which,
therefore, the functioning of the universe would obey is the first major historical reference
to the principle of natural legality.

This claim to achieve knowledge with divinizing characteristics permeated the history
of science. From Antiquity until 1905 (the year of the special theory of relativity), science
was mostly deterministic, as illustrated by three historical examples: Democritus, Laplace
and Hobbes.

Pierre-Simon Laplace, astronomer, physicist and mathematician, was born and lived in
18th century France. Despite being the author of an exhaustive work on probability
( Analytical Theory of Probabilities, from 1812), Laplace believed that the universe
comprised an invariable whole, in which past, present and future are linked in a chain of
infinite causality. Trapped in this succession of events, the future would already be
defined today and the uncertainty regarding tomorrow would only arise from our ignorance regarding the current

Nobel Prize-winning biologist Ilya Prigogine, taking
advantage of Leibniz's ideas, summarizes the automated view of nature and its relationship
with religion by stating that for the determinist, nature is an automaton mechanism in
which there is no room for choice. Free will is just an illusion of those who do not perceive
the forces that move everything in the universe, and the closer we get to knowing these
forces, the closer we are to a divine perspective of the world.

Thomas Hobbes of Malmesbury was a social theorist born in England in 1588 and

build exhaustive theories, capable of identifying general laws and accurately predetermining
the evolution of the systems they analyze.

Thus, for an intellect that knew the laws of transformation and the positions of a given
state of nature, nothing would be uncertain, and the entire future, as well as the past,
would be present before its eyes. This omniscient intellect became known as Laplace's
Demon, and its consciousness can be seen as the deterministic ideal: the mastery of the
natural laws of transformation added to an exhaustive knowledge of present states, which
culminates in an absolute ability to predict the future.

19

18

20

21
17

22

IV. Exponents of determinism

Machine Translated by Google
Hobbes declares himself a materialist and lists,

along the lines of Os Elementos, by Euclides, the axiomatic definitions of his theory,
based on the notion of movement and on the ideas of memory, passion and imagination.
From this structure of movement, attraction and repulsion, nineteen natural laws are
presented that predetermine human behavior. For Hobbes, man is a mechanical entity
composed of matter and movement, governed, therefore, by the same laws as physical
facts. As in Democritus, human life is reduced to movement, and the passions are
explained as forces of attraction and aversion, transforming man into a mechanical
figure, moved by impulses of attraction for pleasure and repulsion for pain. Free will is
denied and the state of nature is described as the pre-contractual situation in which
men fight driven by passions for the dominion of one over the other (from the Latin bellum omnium contra

Finally, Karl Popper created a famous metaphor using the images of clouds and
clocks to explain what would ultimately be determinism. Intuitively, we
associate clocks with mechanisms with regular and predictable functioning, while
clouds are seen as objects with erratic and unpredictable behavior. For the determinist,
the distinction between clouds and clocks would not be real, but just a consequence of
the degree of understanding we have of one and the other. All the clouds would work
with the same regularity as the clocks, but because they operate a simpler mechanism,
we have detailed knowledge about how the clocks work, while the mass of molecules in
the clouds would not allow this same level of detail.

In his main work, The Leviathan,

However, the peak of a movement is the harbinger of its exhaustion, and, from the
19th century onwards, determinism began to expose its limitations. The reaction of
scientists to the new difficulties posed by deterministic explanations marks one of the
most important moments in the history of the evolution of knowledge, called the Statistical Revolution.

Since Euclid, no other theory has strengthened deterministic ideals more than
Newtonian mechanics, one of the most important scientific discoveries in history.
Gravitation expressed the deterministic ideal: a mechanical, simplifying theory, of
universal validity and with a high capacity for prediction. If Euclid's work referred to
postulates and relationships between figures from the ideal world of geometry, Newton's
studies united, under a single set of laws, the movement of planets in space to the
movement of falling objects in our daily lives, using what he himself called the
"synthesis analysis" method, today known as the inductive-deductive method.

regarded as the father of modern political philosophy. Assistant to Francis Bacon (for
whom he translated from English into Latin), Hobbes maintained contacts with René
Descartes, in France, and Galileo Galilei, in Italy, figures who inspired him to produce a
social theory declaredly based on the systematization of the geometry program , with
whom he had late contact, at the age of 40. Despite his admiration for deductive logic
and Euclidean geometry, Hobbes is regarded as a philosopher without mathematical
talent, especially after an unsuccessful dispute with an Oxford mathematics professor
over the problem of squaring the circle, for which he believed he had found a solution.

The discovery of universal gravitation and the three laws of motion nurtured the hope
that planets, moons and bodies in outer and terrestrial space would behave like a simple
and regular mechanical clock. The impression, indelibly marked on the scholars of the
time, was that if we tried hard enough, all the clouds could one day be reduced to clocks
and that the revelation of the deterministic laws of the universe depended only on our
intellectual capacity. The universe would be predictable if we were able to identify the
laws that govern it. Due to universal gravitation, the 18th and 19th centuries were
scientistic, in the strict sense of scientific determinism and the search for elegant
unified theories capable of explaining an apparently chaotic variety of events, including
human behavior and the functioning of Law, morals and of society.

25

23

27

24

26

Machine Translated by Google
V. Statistical Revolution

Although games of chance have been part of the history of civilization since its dawn
(the astralagus and talus, mammalian heel bones from which data found in Egypt were
prepared, are a record of this fact), there is no news of any efforts of measuring
uncertainty until the Renaissance.

The problem that the researchers faced was that the practical results of the
experiments did not respect the theories. At least not with the expected accuracy.
Experimental observations presented an embarrassing variation, which prevented the
exact demonstration of the validity of natural laws. Even astronomy, equipped with the
powerful law of universal gravitation, did not achieve the expected success in comparing
the results of calculations and observations. The actual movement of the stars varied
slightly in relation to expected calculations, disturbing astronomers' efforts to empirically
prove the laws of astronomy. Reconciling the real movement of the moon and stars with
the results of mathematical predictions became an obsession. At the same time,
experimental physics measured phenomena related to, among others, heat, electricity,
light and magnetism, encountering the same obstacles: inaccuracy and variability. The
results of the experiments presented fluctuations that prevented the inductive proof of
general laws capable of explaining the phenomena.

And even between the middle of the

17th century and the beginning of the 19th, specific studies of statistics and probability
were relegated to a few state demographic surveys and the improvement of gambling techniques.
There were, then, two areas of knowledge: low science, such as medicine and alchemy,
restricted to judgments of opinion; and high science, such as astronomy and mechanics,
capable of boasting demonstrable knowledge.

Faced with the insistent variability, the scientists' first reaction was to disregard
results considered anomalous and, based on subjective criteria, choose the one
considered most correct. Guided by a deterministic epistemological perspective,
scientists sought a golden number, which expressed the most precise experimental
result, and refused to consider all results from a series of divergent measurements
because they understood that variation weakened accuracy and, therefore, credibility.
of your job. The perception was that, once aggregated, variations would accumulate
instead of compensating, increasing the discrepancy between what was measured and
what would have happened in reality.

Statistician and historian Stephen Stigler, son of economist and Nobel Prize winner
George Stigler, reports that "until the middle of the eighteenth century there is little
indication in the extensive literature of astronomers combining observations and that in
reality, as in the case of Euler, In 1748, there was actually a vehement rejection of this
practice." At that time, "astronomers even took simple averages of perfectly replicated
quantities, but the idea that the aceracea could be increased by combining measurements
taken under different conditions was distant. They feared that the errors in one
observation would be contaminated by others, that the errors would multiply, not compensate".

Needless to say, only the knowledge produced
by high science was worthy of respect.

The origins of statistics date back to 1700, a period in which physicists and
astronomers were faced with serious measurement problems. Isaac Newton's discoveries
about universal gravitation led astronomers to try to predetermine the position of
celestial bodies through mathematical calculations. Armed with a rigorous group of
exact laws, scientific calculations sought to mathematically reproduce the movements
that stars, planets and moons were predestined to undergo. The same was true of
physics and chemistry. The behavior of masses and chemical reagents in the laboratory
should obey rigorous relationships and proportions, which, once identified, would make
researchers capable of accurately anticipating the results of experiments.

28

30

31

Machine Translated by Google
As the divergent measurements could not be combined, scientists tried to identify
the best among all those observed. The concept of golden result (the golden measure,
the most correct among all) would indicate the one that was least distant from the
expected result, embedding a value judgment incompatible with the objectivity of the
experiment. The choices of golden numbers were intuitive, and often turned out to be
false. Then began to appear the fear that the exact value of a real phenomenon could be
elusive, since no measurement would be sufficiently accurate. The concern was that
everyone was after a chimera, direct access to a physical reality that would ultimately
be inaccessible.

Thus, it was discovered over the years that there was no single golden number to be
chosen among all the experiments. The complete set of results obtained was necessary
for an estimate of the real value, and the more observations, even under different
conditions, the closer the average would be to the true result. From that point on,
especially in the astronomy of the second half of the 18th century, scientists progressed
from calculating arithmetic means and weighted averages to creating linear models and,
by the beginning of the 19th century, had reached what is now considered a theory.
complete statistics.

Another important factor in the development of statistics was the avalanche of
numbers that invaded Europe during the Napoleonic era. State senses were reborn to
help a new bureaucracy to exercise its power of government, classifying, locating and
measuring the characteristics of citizens. More importantly, this information began to
be published, known and discussed outside the limits of the state bureaucracy. The
citizen began to see himself through statistics, identifying his position in the social
stratum, his purchasing power and his location in accordance with the senses.

Also the categories with which the information was classified, the forms of tabulation,
the techniques of correct interpretation of these results and their probabilistic laws
started to be debated in the academy. With this avalanche of data, statistical concepts
about normality, average and deviations began to become familiar.

Finally, a final factor concerns the cultural environment where these ideas flourished.
If the great liberal revolutions, American (1763) and French (1789), were triggered in the
second half of the 18th century, it is in the 19th century that they consolidate and
expand their influence. The concept of democracy and the valuation of the individual
and freedom of choice are the basis for abandoning mechanistic proposals of man, in
favor of an indeterministic position, in which chance and will play a prominent role.

That is why, in parallel with the search for golden results, some researchers began
to realize that the complete set of measurements could provide relevant information for
understanding the phenomena. If there was no golden number, the researcher should
compute all the results of successive measurements, since the accumulated divergences
would be compensated in a combination that would tend to approach the real value, in
an application of the central limit theorem. In general lines, this theorem states that the
distribution of the results of a sufficiently large number of experiments will tend to a
normal distribution, in which the mean of the observations will be very close to the true
result.

The changes brought about by the Industrial Revolution were also relevant.
Urbanization motivated the realization of demographic senses in cities, the search for
quality and regularity in industrial line production gave practical contours to the
concepts of standard and margin of error. Thus, whether in the administration of cities
or factories, statistics gained new fields of ready application. Felipe Fernández-Armesto,
an Oxford historian, explains the relationship between individualism and the crisis of determinism in the West

34

32

33

Machine Translated by Google
The rise of statistical thinking as a revolution is controversial. Stephen Stigler, for
example, agrees with this qualification and speaks of a revolutionary conceptual
framework when dealing with the rise of statistical methods in the 19th century.

Today, statistics is a set of methods used in virtually every aspect of our lives, from
the public (conducting opinion polls) to the private (DNA tests); from government
(implementation of public vaccination policies) to private (development of business
administration techniques); from professional (stock investment models) to recreational
(sports statistics). Setting aside the deterministic pretensions of the 19th century, we all
started to live in a society that thinks and sees itself through statistics.

This epistemological turn had a great impact on researchers’ working methodology
and their vision of what it means to do science. An impact whose seismic effects have
not been exhausted, and are still being felt in the current scientific debate. It can be said
that between 1700 and 1900 statistics underwent a vertical evolution (in the sophistication
of its technique) and horizontal evolution (in the breadth of its application).

From a horizontal point of view, statistics departed from astronomy and began to be
applied in several other fields of knowledge, including areas as disparate as chemistry,
engineering, medicine, geography or biology. From the vertical point of view, statistics
deepened its techniques for measuring uncertainty, starting from arithmetic means and
the study of frequencies to reach statistical inference and causality studies.

Lorenz Kruger,
Lorraine J. Daston, Michael Heidelberger, Gerd Gigerenzer and Mary Morgan, professors
at the Max Planck Institute and the Massachusetts Institute of Technology, edited a work
in which the ideological and historical aspects of the statistics insurgency are dissected
in the context of a revolution .

Differing on the occurrence of a revolution is Bernard Cohen, professor of history at
Harvard, who refutes the ideas of a knowledge revolution and speaks of the emergence

Theodore Eisenberg, one of the exponents of the empirical study
of law, also speaks of the resurgence of this form of research in the context of a
statistical revolution.

Richard Von Mises, brother of the economist Ludwig von Mises and one of the
fathers of modern statistics, believed that the attempt to extend the application of the
exact sciences without limit was a characteristic of eighteenth-century rationalism.

from Descartes: "The Cartesian maxim - 'I think, therefore I am' - called into question
the key to the only possible certainty. Struggling to escape the suspicion that all
appearances are illusory, Descartes reasoned that the reality of his mind was proven
through self-doubt. Descartes defined created laissez-faire economics and the doctrine
of human rights, transforming freedom into the highest value in a strictly limited
spectrum of self-evident truths.

And Ilya Prigogine,
already quoted here, was adamant about the reasons for this delay, among which he
included "the desire [of man] to reach an almost divine point of view about nature", the
need to create a "new arsenal mathematical, in which generalized functions, 'fractals',
as Mandelbrot called them, play an important role" and a resistance to the relative
disorder arising from a relativistic view of the world, subject to "an even stronger form
of dynamic instability, such that [deterministic] trajectories are destroyed whatever the
precision of the description."

38

39

36

37

35

40

SAW. revolution and evolution

Machine Translated by Google
But, beyond considerations about the meaning of the expression scientific revolution,
the main aspect that demonstrates the transformative nature of this new methodology
are its practical consequences.

Unlike a revolution in science, the rise of statistics corresponds to the birth of a new
methodology of analysis, capable of affecting not only science but everyday life and art.

Unlike previous revolutions, the statistical revolution did not depend on the emergence
of new facts. There was no emergence of a new inductive theory, capable of explaining
all the facts covered by the previous theory and also adding an additional set of facts
under a new theoretical model. What happened was a change in the relationship scientists
have with knowledge and a change in purpose. Science ceases to be the search for
absolute truths and becomes an effort to approximate the truth. It is about creating a new
approach to knowledge, based on a new set of techniques and aimed at a new purpose:
instead of seeking certainty, we start trying to control uncertainty. A less divinizing and
more pragmatic approach, based on probability laws rather than deterministic laws.

The move away from the deterministic point of view and the intellectual approach to
the world of "eikos", probability and uncertainty, did not imply the replacement of
progress with return. Quite the contrary, the improvement of probability theory is at the
root of the technological and scientific development that began in the 19th century and
accelerated from the first half of the 20th century, and, mainly, the surprising increase in
the living conditions of the population in general.

Ian Hacking, despite disliking the use of the expression statistical revolution in the
vulgar sense (that is, divergent from the sense of scientific revolution used by Thomas
Kuhn), describes the process of growth of the methodological influence of probability
and statistics in scientific work as a change even deeper than a revolution. For Hacking,
unlike a traditional revolution in science, in which the discovery of new facts leads to a
reassessment of theories, the entry of statistics corresponds to a methodological turn
that, regardless of the discovery of new facts, alters the scientist's relationship with the
knowledge and leads to a complete reassessment of our perspective on the world.

Felipe Fernández-Armesto explains the statistical revolution by comparing it to a
movement of retraction of the truth: "The retraction of the truth is one of the most
dramatic and least told 'stories' in history, we need to make an attempt to trace its course,
as this can help us explain one of the great enigmas of the modern world. For academic
professionals in the affected disciplines, becoming indifferent to the truth is an
extraordinary reversal of traditional obligations, as if doctors renounced the obligation
to maintain life, or theologians lost the interest in god - formerly unthinkable processes
that now loom as truth retracts".

The attachment to deterministic theories, instead of promoting the progress of humanity, had

of a new area. According to Cohen, the strengthening of statistics would not be, as
occurred in other revolutions in science, attributable to a group of individuals, would not
affect a specific area of research and would not characterize a break with natural laws.

Despite disagreements regarding the use of the expression, there is a consensus that
the use of statistics as a methodology for scientific investigation has had a profound
impact on the history of thought, which I consider sufficient to speak of revolution, if not
in its "Kuhnian" sense. , at least in its ordinary sense. It is, however, a revolution distinct
from those that occurred previously, such as those resulting from the discovery of the
law of universal gravitation or the theory of relativity.

44

41

43

42

Machine Translated by Google
narrowed our understanding of the world and reduced our ability to take advantage of
available human, financial and natural resources. Statistics not only revolutionized the
various fields of scientific knowledge, opening new conceptual horizons, but was also
responsible for enabling financial products that provide economic support to human
activities, including scientific and technological research centers.

Finally, an explanation of the epistemological consequences of the statistical
revolution. The notion that there is approximation knowledge is present in all intellects
that dedicate themselves to studying an object in more depth. We are not always (not to
say never) able to build a correct, exact and exhaustive theory about what we observe,
but as we study our object and gather successive information about it, we are clearly
aware that we are closer to our goals than we were at the beginning, and that, although
we do not have complete control, we understand more. We cultivate an ideal of exact
and absolute knowledge, however, in practice, we deal with approximation,
circumstantial, limited and essentially precarious knowledge.

Learning to live with these limitations and recognizing the value of incomplete
knowledge, which involves risk and margin for error, are attitudes that define modern
thinking. The hope of our ancestors that the development of increasingly precise
measuring equipment, added to the accumulation of empirical observations, would
create conditions for exact knowledge, causing determinism to extend its domains
beyond geometry and physics, reaching the human and social sciences, did not
materialize.

The statistical revolution marks the abandonment of belief in a deterministic world.
However, the end of determinism has not hurled us into an irrational or unintelligible
world. There are no laws for the exact control of the world, but that doesn't mean that
we can't influence and, to some extent, direct reality according to our purposes. On the
contrary, at the same time that it weakened the cogent force of deterministic laws, the
statistical revolution strengthened human freedom, detaching it from the gears of a
mechanical world and making room for our will to exercise greater control over the
consequences of our actions.

The 19th century brought notable progress in statistics, however, this progress was
not used, at least not with adequate intensity, by the social sciences. It was only in the
twentieth century that the social sciences, especially economics, woke up to the benefits of

As Peter Bernstein abundantly exemplifies, "without a theory of probability and other
risk control instruments, engineers would never have built bridges that cross our widest
rivers, houses would continue to be heated by fireplaces, household appliances would
not exist, polio would still kill children, airliners would not fly and travel to space would
be just a dream. Without insurance, the death of the breadwinner would reduce entire
families to poverty and extinction, most people would not have access to medical care
and only the richest could buy your own house. If farmers couldn't sell their produce at
a fixed price before harvest, they would produce much less food than they actually do."

What we saw was the opposite. Even physics (with statistical mechanics), the
historical stronghold of determinism, has drifted towards probabilistic theories. As Ian
Hacking explains in the epigraph of this chapter, the fundamental mark of scientific
thought in the 20th century was the fall of determinism, the break with the principle of
causality and its strict link between past, present and future and the discovery that
there are laws of regularity , which are not deterministic natural laws.

46

45

VII. Delay of the social sciences

Machine Translated by Google
This intermediate goal pursued by social science is also, not coincidentally, the main
scope of political debates.

Because of this overlap with political objectives, discussions around the

Graunt's work, at the time called political arithmetic, had a strong impact because
the population estimated by his study proved to be much higher than that intuited by
the authorities at the time. The work even earned him a seat at the Royal Society, an
unusual fact for a merchant. The use of statistics in the study of the human population,
however, did not continue over the next two hundred years, until William Farr, one of
the first epidemiologists, took up Graunt's project at the end of the 19th century and
began to systematize demographic records. from England.

Social scientists, despite suffering from the effects of variability and uncertainty
inherent in human behavior, offered greater resistance to the use of statistical methods
in their research. An example of this resistance is the criticism directed at the English
economist and logician William Stanley Jevons, one of the fathers of Econometrics,
who, still in the 19th century, used in a study on the variation in the price of gold a
reference index composed of a basket of several other commodities.

Critics of Jevons' work complained of a lack of rigor in the approach. The scientific
work had as its basic premise the choice of a single object of study, in this case gold,
whose price behavior would have nothing to do with that of other products on the
market subject to different factors and influences. At the time, the idea that combining
the prices of several commodities into a single metric (for example, the variation in Latin
American commodities , or mining, or agricultural, or a given period) could be illuminating
regarding behavior of the prices of each of the products.

The carrying out of demographic
censuses gradually spread across several countries, just as the techniques for statistical
investigation of these data ended up being occasionally appropriated by some social
sciences, especially Demography and Economics.

Social scientists, before and after John Graunt, always found variations in their
observations: in people's age, weight and height, in the number of children, in the price
of goods and in family income, among many others. Why, then, the resistance to using
statistics? Why not seek to reduce uncertainty by understanding how these variations
occurred? This antipathy to the quantitative approach has a remarkable historical
persistence, which led me to think about the origins of the problem beyond the ironic
blindness suggested by Stephen Stigler.

The influence of statistics on the social sciences began timidly with the creation of
censuses of mortality and epidemic diseases. The first demographic record was
prepared by John Graunt, a wealthy London merchant of trimmings, based on death
records of the population of London, started in 1603. Based on the organization of these
data, Graunt elaborated the first table of life expectancy by range age ( Natural and
political observations upon the bills of mortality, 1662).

quantification of social facts and for the usefulness of probabilistic models and
statistical inferences in the description, study and understanding of social behavior, in
a delay that Stephen Stigler ironically attributes to the influence of the blind or illiterate.

A likely explanation is related to the strong influence of the political dimension in
discussions about the dynamics of life in society. Social science's immediate objective
is to understand man's gregarious behavior, but its immediate objective is to contribute
to improving the functioning of society and improving people's living conditions.

And based

on the city's death rate (number of deaths per number of inhabitants), Graunt was able
to promote the first statistical inference regarding the size of a human population, in
this case, the inhabitants of the city of London.

48

50

51

47

49

Machine Translated by Google
Thus, both extreme conservatism, of those who try to prevent any change in the status
quo, on the one hand, and radical revolutionaryism, of those who seek to impose
changes only with the intention of shifting the center of power, on the other, reduce the
space for a empirical, cautious and consequentialist approach in the study of public
policies.

There is, however, a noticeable evolution in the quality of the debate, partly made
possible by the dissemination of statistical information on the situation in society.
Statistical indicators are capable of summarizing the general performance of a
government, allowing the comparison of projects presented in campaigns and the
effective results of management. If candidates are still evaluated emotionally today,
whether based on their social origin, the way they dress or based on personal
experiences (the candidate paved my street when he was mayor or gave the voter a set
of teeth), increasingly the technical and impersonal debate gains space, with an agenda
essentially based on the discussion of socioeconomic indicators, such as inflation rates, crime and unemployment.

Furthermore, every political discussion revolves around proposals about ideals of
society, whose counterfactual character is not subject to empirical tests. Do we want an
egalitarian or meritocratic society? Do we want a freer or safer society?
No statistical methodology is able to compare which of these aspirations is better. No
test will succeed in proving that equality is worth more than prosperity, or that security
is worth more than freedom. Statistics is a neutral tool for analyzing society as it is,
always realistic and sometimes uncomfortable, which can contribute to its improvement
through an analysis of the adequacy of means to given ends, but which does not have
much to contribute to the choice of these purposes.

Before the
creation of these indicators, voters would not even have access to the information
necessary for an objective analysis of a government's performance.

The desire for technical management of society and a more objective public debate
explains the rise of economics, a subject of law courses until the second half of the 19th
century, to the most powerful social science today. Driven by econometrics and the
pioneering use of statistics and empirical research in the study of human behavior,
economics gained its curricular independence from the beginning of the 20th century
(the first school of economics, the London School of Economics, was founded in 1895)
and in less than a hundred years it became the main tool for formulating public policies,
displacing law from its historical position. Even in Brazil, where the USP Faculty of
Economics was only founded in 1946, in just two decades its first former students were
already assuming prominent roles in the federal government, pointing to a trend that
would become preponderant in the future.

In comparison, politics is the field of ideal proposals, the trafficking of dreams and
aspirations. It is an area of people essentially concerned with final objectives, who
conveniently forget to deal with the means necessary for their implementation.

Social science purposes have the potential to affect the power spaces occupied by
interest groups. This interference transmutes a discussion that could be academic,
objective and impartial into a debate of a partisan political nature, in which the
protagonists often start to defend private interests to the detriment of common objectives.

Debate over the viability of any proposal usually entails a discussion of its costs. And
costs are always a pain to remember, especially during election campaigns. Hence
politics' aversion, at least in its lowest and most irresponsible manifestations, to
consequentialist discourse. No one wins an election by promising a government
spending cut or a tight fiscal adjustment. You win by promising to pave streets, build
thousands of schools, health centers and houses, even if there are no resources for
that. This emphasis on ideal objectives combined with the disregard for the real means
of implementing them is still a factor that distances politics from statistics.

52

Machine Translated by Google
FOOTNOTES

In the original: "The most decisive conceptual event of the twentieth century physics has been
the discovery that the world is not deterministic. Causality, long the bastion of metaphysics, was
toppled, or at least tilted: the past does not determine exactly what happens next . This event
was preceded by a more gradual transformation. During the nineteenth century it became
possible to see that the world might be regular and yet not subject to universal laws of nature. A
space was cleared for chance". HACKING, Ian. The taming of chance. New York: Cambridge University Press, 1990. p.

"The pre-eminence of the Greeks appears more clearly in mathematics and astronomy than in
anything else. What they did in art, literature and philosophy can be judged better or worse
according to taste, but what they accomplished in geometry is entirely beyond question. They
learned something from Egypt and a little less from Babylon; but what they obtained from these
sources was, in mathematics, mainly rudimentary rules and, in astronomy, records of observations
extending over very long periods. The art of mathematical proof was almost entirely Greek in
origin." RUSSEL, Bertrand. History of Western Philosophy. 3rd ed. São Paulo: Companhia Editora
Nacional, 1969, vol. 1, p. 242. Author's translation.

For a complete overview of the history of Greek mathematics: HEATH, Thomas Little. A history
of Greek mathematics. New York: Dover, 1981.

HUFFMAN, Carl A. The Pythagorean Tradition. In: LONG, AA Early Greek Philosophy (org.). São
Paulo: Ideas and Letters, 2008. p. 130; and HEATH, Thomas Little. A history of Greek mathematics.
New York: Dover, 1981. p. 11-12.

Today, two of the main challenges facing democratic governments revolve around
statistics. The first is to increase and improve social indicators, making updated data
available to the population, certified by independent entities that guarantee their
integrity, accompanied by their serious histories. In mass societies and countries of
continental dimensions, the people can only see their government through statistics.
The second is to educate the population and make them understand the function of
these indicators. High school statistics classes are just as important as math classes.
Explain to a low-income student what the inflation rate is and how it is capable of
eroding your father's salary in a few years, or how the crime rate in your neighborhood
is calculated and why it is higher than in other neighborhoods wealthy, is a means of
social awareness. If mathematics is the path to inclusion in the job market, statistics is
the path to citizenship and inclusion in modern political life.

3
1

4
two

Machine Translated by Google
10

8
6

13

9

11

7
5

12

See TAYLOR, Thomas. The commentaries of Proclus on the Timaeus of Plato, in five books.
London: AJ Valpy, 1820. See also: Plato. Timaeus. Fairfield: World Library Books, 2008.

SAMBURSKY, Samuel. The physical world of the Greeks. London: Routledge & Keagan Paul,
1987. p. 179.

HEATH, Thomas Little. Euclid in Greek. Cambridge: Cambridge University Press, 1920.

ADAM, James. The republic of Plato edited with critical notes and appendices. Cambridge:
Cambridge University Press, 1902.

CORNFORD, Francis MacDonald. Plato's cosmology, the Timeaus of Plato translated with a
running commentary. London: Compton Printing Ltd., 1937; HEATH, Thomas Little. A history of
Greek mathematics. New York: Dover, 1981. p. 284.

The point is summarized in the following passage: "The Greeks believed that order is to be found
only in the skies where the planets and stars appear regularly in their appointed places with an
unmatched regularity. To this harmonious performance, the Greeks paid deep respect, and their mathematicians

DOGSON, Charles L. Euclid and his modern rivals. Cambridge: Cambridge University Press, 1879.

RUSSEL, Bertrand. History of Western Philosophy. 3rd ed. vol. 1. São Paulo: Companhia Editora
Nacional, 1969, p. 245-246; HEATH, Thomas Little. Euclid in Greek. vol. 1. Cambridge: Cambridge
University Press, 1920. p. 3-40. Only with the work of Carl Friedrich Gauss in the 19th century
and the discovery of non-Euclidean geometries was Euclid's legacy relativized.

Part of geometry that studies the measurement of solids.

Machine Translated by Google
studied it intensely. But the perfection of the heavens served only to highlight the disarray of
life on earth. Moreover, the predictability of the firmament contrasted sharply with the behavior
of the flickle, foolish gods who dwelt on high." BERNSTEIN, Peter L. Against the gods: The
remarkable story of the risk. New York: John Wiley & Sons. 1998. p.70-71.

According to Karl Popper, determinism can be classified, according to its scope, into
philosophical, physical and psychological, or, according to its origin, into religious, scientific
and metaphysical. Here we deal with physical and scientific determinism. POPPER, Karl. The
open universe: an argument for indeterminism. London: Cambridge University Press, 1982. p. 7.

The intuitive idea of determinism may be summed up by saying that the world is like a motion
picture film: the picture or still that is just being projected is the present. Those parts of the
film which have already been shown constitute the past. And those which have not yet been
shown constitute the future.

In the film, the future co-exists with the past; and the future is fixed, in the exact same sense
as the past. Though the spectator may not know the future, every future event, without
exception, might in principle been known with certainty, exactly like the past, since it exists in
the same sense in which the past exists. In fact the future will be known to the producer of the
film, to the Creator of the world." POPPER, Karl. The open universe: an argument for
indeterminism. London: Cambridge University Press, 1982, p. 1, 2,5 .

"To give a causal explanation of an event means to deduce a statement which describes it,
using the premises of the deduction one or more universal laws, together with certain singular
statements, the initial conditions". POPPER, Karl. The logic of scientific discovery. New York: Basic Books, 1959. p.

In the words of Niehls Bohr: "We would go too far if we recalled in detail how, with the
elimination of mythical cosmological ideas and arguments concerning the purpose of our
actions, a coherent scheme of mechanics was constructed, based on the pioneering work of
Galileo, which reached great perfection through Newton's mastery. Above all, the principles of
Newtonian mechanics meant a broad clarification of the problem of cause and effect, allowing,
from the state of a physical system defined at a given instant by measurable quantities, the
prediction of his state on any subsequent occasion. It is well known how much this kind of

In the words of Karl Popper: "My central problem is to examine the validity of the arguments in
favor of what I call 'scientific determinism'; that is to say, the doctrine that the structure of the
world is such that any event can be rationally predicted, with any desired degree of precision,
if we are given a sufficiently precise description of past events, together with all the laws of nature. ( omissis)

17

15

16

14

Machine Translated by Google
"[In determinism] The laws of nature enunciated by physics are of the sphere, therefore, of an
ideal knowledge that reaches certainty. Once the initial conditions are given, everything is
determined. Nature is an automaton that we can control, by the least in principle.Novelty, choice,
spontaneous activity are only appearances, relative only to the human point of view.

In determinism, probability is a state of mind, a measure of our ignorance, not an objective quality
of reality. GIRENZER, Gerd. The Empire of Chance: How Probability Changed Science and Every
Day Life. Cambridge: Cambridge University Press, 1989. p. 11 sec.

TAYLOR, CCW The atomists, Leucippus and Democritus: fragments, a text, a translation and
commentaries. Toronto: University of Toronto Press, 1999. p. 157-200.

Many historians emphasize the essential role played by the figure of the Christian God, understood
in the 17th century as an all-powerful legislator, in this formulation of laws of nature. Theology
and science converged at the time. Leibnz wrote: 'in the smallest of substances, eyes as keen as
God's could at once read the whole sequence of things in the universe.
Quae sint, quasi fuerint, quae mox futuro trahantur (What are, what were, what will happen in the future)'.

This is the "theological" meaning of the phrase attributed to Albert Einstein: "when the answer is
simple, God is speaking". Despite being widely referenced in other works, we were unable to
locate the origin of the citation in Einstein's works.

deterministic or causal explanation led to the mechanistic conception of nature and came to
figure as an ideal of scientific explanation in all fields of knowledge, regardless of the way in
which knowledge was obtained.". BOHR, Niehls. Atomic Physics and Human Knowledge: Essays
1932- 1957. São Paulo: Contratempo, 1996. p. 87.

The submission of nature to deterministic laws thus brought human knowledge closer to the
timeless divine point of view."PRIGOGINE, Ilya. The end of certainties. São Paulo: Unesp, 1996. p. 19-20.

HAHN, Roger. Pierre-Simon Laplace: a determined scientist. Cambridge: Harvard Press, 2005. p.
168-179; GILLISPIE, Charles C. Pierre-Simon Laplace: a life in exact science. Princeton: Princeton
University Press, 1997. p. 271-279.

23

21

18

20

19

22

Machine Translated by Google
30

26

HACKING, Ian. The emergence of probability. New York: Cambridge University Press, 2006. p. 39-48.

STIGLER, Stephen. The history of statistics. The measurement of uncertainty before 1900. Cambridge: Harvard University Press,

1986. p. 11 ff.

POPPER, Karl. The open universe: and argument for indeterminism. London: Cambridge University Press, 1982. See also: POPPER,

Karl. Objective knowledge, corrected edition. Oxford: Oxford University Press. 1972. p. 227.

27

STIGLER, Stephen. The history of statistics. The measurement of uncertainty before 1900. Cambridge: Harvard University Press,

1986. p. 4-5.

Meteorology is often remembered as an example of the limitations of a science that seeks to predict, only through observation and

without resorting to experiments, the behavior of complex systems.

31

HOBBES, Thomas. Leviathan. Parts I and II. Rev. AP Matirnich and Brian Battiste. Ontario: Broadview, 2011.

28

The Central Limit Theorem states that the distribution of successive sample means approximates

HALD, Anders. A history of statistics and probability and their application before 1750. New Jersey: John Wiley and Sons, 2003. p.

172-176.

32

RAPHAEL, David D. Hobbes: morals and politics. London: Routledge, 2004. p. 9-15.

29

25

24

HACKING, Ian. The emergence of probability. New York: Cambridge University Press, 2006. p. 1-10.

Machine Translated by Google
39

35

STIGLER, Stephen. The history of statistics. The measurement of uncertainty before 1900. Cambridge: Harvard University Press,

1986, p. 8.

LORENZ KRUGER, Lorraine J. Daston, Michael Heidelberger, Gerd Gigerenzer and Mary Morgan. The probabilistic revolution. (Vol 1.

Ideas in History. Vol. 2. Ideas in Science). Massachusetts: MIT Press,

HACKING, Ian. The emergence of probability. New York: Cambridge University Press, 2006. p.xviii.

36

40

FERNÁNDEZ-ARMESTO, Felipe. Truth: a story. Rio de Janeiro: Record, 2000. p. 195.

nineteen ninety.

more and more of a normal distribution as the number of samples increases.

37

VON MISES, Richard. Probability, statistics and truth. New York: Dover Publications, 1981. p. 12.

"ELS [empircal legal studies] is not, in my view, a competitor with other 'law and' social science disciplines. It is complementary to

them and helps the study of Law and the legal system to join part of a larger probabilistic revolution. That revolution has been said

to encompass the web of changes that made probability a part of philosophy, scientific theories and practice, social policy, and daily

life between circa 1800 and 1950, and has obviously been accelerated by the growth of personal computer." EISENBERG, Theodore.

The origins, nature, and promise of empirical legal studies and a response to concerns. University of Illinois Law Review. v. 5, 2011.

p. 1719.

"During the course of the 18th century astronomers progressed from the simple means to linear models and, with the aid of

Newtonian theories, were able to reach a mature statistical theory in the first two decades of the 19th century." STIGLER, Stephen.

The history of statistics. The measurement of uncertainty before 1900. Cambridge: Harvard University Press, 1986. p. 29.

38

34

33

PRIGOGINE, Ilya. The end of certainties. São Paulo: Unesp, 1996,. P. 39-40.

Machine Translated by Google
FERNÁNDEZ-ARMESTO, Felipe. Truth: a story. Rio de Janeiro: Record, 2000. p. 193.

"Was there such a thing [like a probabilistic revolution]? Yes, if we mean revolution in an ordinary

present-day generous sense. As the second collective product, The Empire of Chance, put it in its
subtitle, How Probability Changed Science and Every Day Life. Those changes were truly revolutionary
(in the ordinary manner of speaking). I have also tried to give a more narrow definition to those words,
'probabilistic revolution', taking off an idea of Kuhn's about a 'second scientific revolution' early in
the nineteenth century. The emergence of probability, however, was a change more fundamental than
any revolution. A new thinking cap." HACKING, Ian. The emergence of probability. New York:
Cambridge University Press, 2006. p. xix.

HACKING, Ian. The taming of chance. New York: Cambridge University Press, 1990.

BERNSTEIN, Peter. Against the gods. New York: John Wiley & Sons, 1996. p. two.

The beauty and challenges of this historical moment are expressed by Ilya Prigogine in the last

paragraphs of his work The End of Certainties: "Pure chance is as much a denial of reality and our
demand to understand the world as determinism is. What what we seek to build is a narrow path
between these two conceptions that equally lead to alienation, that of a world governed by laws that
leave no room for novelty, and that of an absurd world, the causal one, where nothing can be predicted
or described in any way. general terms. ( omissis) What emerges today is, therefore, an average
description, situated between two alienating representations, that of a deterministic world and that of
an arbitrary world subjected only to chance. Laws do not govern the world, but neither does this is
governed by chance. Physical laws correspond to a new form of intelligibility that irreducible
probabilistic representations express. They are associated with instability and, whether at the
macroscopic or microscopic level, describe events as possible, without reducing them to deducible
consequences or predictable deterministic laws. Perhaps this distinction between what can be
predicted and controlled and what cannot be would have satisfied the quest for intelligibility of nature
at the heart of Einstein's work? In this process of building a narrow path between blind laws and
arbitrary events, we discovered that a large part of the world around us had until then 'slipped between
the meshes of the scientific network', to resume an expression of

COHEN, I Bernard. Revolution in science. Cambridge: The Belknap Press, 1985. p. 23-44.

42

43

44

45

46

41

Machine Translated by Google
MERRIL, Ray M. & Timmrech, Thomas C. Introduction to epidemiology. 4th ed. Ontario: Johns and Bartlet, 2002. p. 33-34.

47

There is a historical dispute regarding the authorship of this work, attributing it to John Graunt or the English economist and philosopher Sir

William Petty. William Petty himself, however, recognizes the authorship of Graunt, who had his name attributed to the work in all publications

published while he was alive. For details: PETTY, Sir William. The economic writings of Sir William Petty. London: Routledge, 1997. p. xliii.

"By the 1830's statistical methods were widely used in astronomy, and we can find reasonably accessible texts from that period that bear at least

a cousinly resemblance to modern elementary texts. Yet it is only in the twentieth century that we find these same methods making substantial

inroads into the social sciences. Were nineteenth-century social scientists unable to read? And even if they were neither illiterate nor too dense

to see the need for the quantification of uncertainty in their data, why did they ignore what was so obvious a century later ?" STIGLER, Stephen.

The history of statistics. The measurement of uncertainty before 1900. Cambridge: Harvard University Press, 1986. p. two.

48

50

PETTY, Sir William. The economic writings of Sir William Petty. London: Routledge, 1997. p. xxxiv xxxviii. The complete report is in HALD,

Anders. A history of statistics and probability and their application before 1750. New Jersey: John Wiley and Sons, 2003. p. 81-105.

52

STIGLER, Stephen. The history of statistics. The measurement of uncertainty before 1900. Cambridge: Harvard University Press, 1986. p. 5.

The two main socioeconomic indicators are the Gross Domestic Product (GDP) and the Human Development Index (HDI). The creators of both

were awarded the Nobel Prize in Economics. Simon Smith Kuznets, Russian economist and creator of the GDP concept, was awarded in 1971.

Amartya Sem, Indian economist and one of the creators of the HDI, was awarded in 1998.

Whitehead. We discern new horizons, new questions, new risks. We live in a privileged moment in the history of science."PRIGOGINE, Ilya. The

end of certainties. São Paulo: Unesp, 1996. p. 197-199.

51

49

Machine Translated by Google
© of this issue [2016]

Machine Translated by Google
two

3
1

5

6

I. Definition of statistics

2018 - 07 - 17

Jurimetry
CHAPTER 3. STATISTICAL METHODS

Chapter 3. Statistical Methods

Gottfried Achenwall, Germanic philosopher, to assign the study of economic and population data to the State
Others argue that the first modern statistical description is by the Englishman John Graunt in his study on
mortality in London in 1662. Graunt, however, did not use the statistical expression, choosing to call it "political
arithmetic". For the English, the expression statistics was only used for the first time by J. Sinclair in the work
Statistical account of Scotland drawn up from the communications of the ministers of the different parishes,
prepared 4 between 1791 and 1799.

One way to control for the effect of other variables on patients' healing is to divide them into three groups:
one not medicated, another receiving placebo, and a last one medicated. The verification of the results in each
of the groups is capable of isolating the effects of the medicine and controlling the effects of time and of the
placebo on the patients, strengthening the conclusion regarding the effectiveness of the first one. The main objective of planning is, therefore,

.

For example, a researcher intends to test whether a drug is capable of curing headaches. One way to answer
this question is to gather a group of people with a headache and administer the medicine, then measuring
(through questionnaires and tests) how many patients were cured. This test, however, does not allow rejecting
the possibility that the improvement would be a consequence of other factors, such as a placebo effect or a
spontaneous improvement resulting from the passage of time.

"In God we believe; everyone else needs to bring data."

Statistics analyze data from different sources, such as prices, weight of animals, temperatures, position of
celestial bodies, height of people, results of medical treatments or chemical reactions. In common, the
information that can be analyzed through this method exhibit some degree of variability, allowing the distribution
of its results in categories or ranges. For example, income variation allows the distribution of the Brazilian
population between high, middle and low social classes; the variation of rites enables the distribution (in a
statistical and non-procedural sense) of lawsuits between ordinary, execution or special procedures. For each
of these populations there is a degree of uncertainty. I don't know exactly how many enforcement actions are
being processed in Brazil, because every hour some actions are distributed and others are extinguished.
However, a periodic sampling can tell me approximately how the actions are in Brazil in relation to the type of
procedure and how many enforcement processes there are.

Statistics deals with the collection, organization and analysis of data sets. Your objective is to describe these
sets and obtain, from them, the greatest amount of knowledge possible. The object of statistics is not ideal or
abstract. Its purpose is to offer solutions to combine measurements and analyze sets or series of information
collected in the most diverse fields of knowledge.

A comparison of dates is enough to realize that the reason lies with the Germans.

The word statistics has its etymological origin in the Latin expression statisticum collegium and since the
16th century it has been used in Italy with the purpose of designating a collection of facts (verbal reports,
topographical and geographical descriptions) for the information of State officials. The modern meaning of
statistics, indicating a collection of data regarding the population and finances of a State, is the subject of controversy.

Planning a
research involves several steps, including formulating hypotheses, choosing the type of test, choosing a
significance level, collecting data, determining the critical value, and presenting the results.

Unlike the sciences that are defined by their objects (biology studies life, chemistry studies the
transformations of matter, etc.), statistics is a discipline defined by its methodology and can be applied to any
object capable of experimentation and observation . In Stephen Stigler's definition: "Modern statistics offers
quantitative technology for empirical science; it is a logic and methodology for measuring uncertainty and for
examining the consequences of that uncertainty in the design and interpretation of experimentation and observation."

Statistics is a research methodology, which aims to plan and carry out hypothesis tests.

Some historians claim that the expression was appropriated from the German statistik, a term introduced in 1749 by

The planning of a research involves the choice of variables and a careful delineation of its objectives.

The connection with problems of practical application makes statistics to be considered a set of methods
different from mathematics. An important part of statistics involves non-mathematical problems such as, for
example, identifying the source of data, choosing tests capable of strengthening conclusions or how to visualize
results in graphs and tables.

Machine Translated by Google
In an experiment, I roll a die 20 times and observe the following results (R): R1) 3; R2) 4; R3) 3; R4) 3; R5) 4; R6) 3. This
is the absolute frequency distribution of the variable "data result". The relative frequency distribution is the percentage
results: R1) 15%; R2) 20%; R3) 15%; R4) 15%; R5) 20%; and R6) 15%; whose sum gives 100%. In other words, the
frequency distribution indicates the number of realizations of each value of a variable (when absolute) or the percentage
of realization of these values (when relative).
Another fundamental concept is the summary measure. Summary measures are indicators capable of summarizing
information about a large amount of data. The best-known summary measure is the mean. For example, if I say that on
average a lawsuit takes 4 years to be judged by a court, I am summarizing in a single measure the thousands of results
of the variable "time of lawsuit" observed in a survey. There are several other summary measures that are not as
famous as the mean, so much so that they are classified into measures of position and dispersion.

It is important to explain, albeit occasionally, how data is described. I start with a classic example, which helps us
understand the concept of frequency distribution of a variable: the game of dice. When rolling a dice, you have a
variable with six possible outcomes, the numbers one through six, corresponding to each face.

Statistical research can only provide a description of what has been observed. However, it can also move on to
considerations about what has not been directly observed and make inferences about, for example, the future behavior
of variables, the association between two or more variables, or the characteristics of an unobserved portion of the
population. These approaches divide statistics into two distinct subareas: descriptive and inferential.

Or you might be interested in understanding whether the distribution of actions between ordinary, enforcement or
special procedures is or is not associated with the twelve months of the year. This is a bivariate analysis focusing on
the variables "procedure" and "month of the year".

The mean expresses a central tendency because it indicates an intermediate

position between the observed results. It, however, does not indicate the distance between these results. When I say
that the average between two numbers equals 50, these two numbers can be 49 and 51, very close results, or they can
be 1 and 99, farther results.

Summary measures of position (also known as measures of central tendency) indicate the values around which the
data set focuses. Examples of position summary measures are: the arithmetic mean, the mode and the median. The
arithmetic mean is the sum of the observed results, divided by the number of results. Mode is the most frequent result.
And the median is the result that occupies the central position, when the observations 13 are ordered in ascending
order.

foresee possible weaknesses and design the tests in order to strengthen, as much as possible, the conclusions of a
research.

The standard deviation is the best-known measure of dispersion because it

expresses how far each result is approximately from the mean. In the example above, the first two results (49 + 51) are
very close to the mean and have variance equal to 1, because {[(12 + (-12)] ÷ 2 = 1}. Since the square root of 1 is equal a
1, the standard deviation is also 1. The variance of the other two results (1 + 99) is equal to 2041, because {[(492 + (-492)]
÷ 2 = 2041}. Extracting the square root of 2041 we arrive at a standard deviation of 49, which means that each result is
49 points away from the mean.

That's what summary measures of dispersion are for: to indicate how much the results deviate from the measures
of position. Examples of measures of dispersion are variance and standard deviation. The variance corresponds to the
mean of the squares of the differences between, on the one hand, each observed result and, on the other hand, the
arithmetic mean. The standard deviation is the square root of the variance.

The hypotheses may contain doubts about the behavior of a variable, in the so-called univariate analysis, or about
the association between two or more variables, in the so-called bivariate and multivariate analyses.

When you have access to a certain amount of data, the first task is to understand what kind of information this data
brings. This area of statistics is limited to what can be observed directly and it only describes, in an exhaustive or
summarized way, the set of collected data. Hence its name. This description can be presented in several ways, through
a frequency distribution of each variable, by calculating summary measures, by tabulating results and by visualizing
them through tables and graphs.

To obtain this number, the researchers counted the duration, between the distribution and

judgment of the appeal in the second instance, of a sample with 718 dissolution actions in the 27 states of the federation.
Adding all the times and dividing by the number of actions in the sample, an average of 1,782 days was obtained, that
is, 4 years, 10 months and 22 days.

You may be

interested in knowing, returning to the example of lawsuits, how lawsuits are distributed in Brazil between ordinary,
execution or special procedures. This is a univariate analysis focusing on the procedure variable.

Descriptive statistics is the area that studies the process of exploring, visualizing and summarizing data.

Therefore, it is very important to reinforce that each summary measure should, whenever possible, be analyzed
together with as many other measures as possible. See this example: a survey conducted with the support of the
Associação Brasileira de Jurimetria - ABJ sought to estimate the average duration of an action for the dissolution of a
business company in Brazil.

II. Descriptive statistics

14

7

8

15

9

16

11

10

12

Machine Translated by Google
Statistical inference usually works with samples. The sample corresponds to a subset of 19 It is the correct use of individuals from
a population separated for analysis using a certain methodology. methodology in constructing the sample that validates the
extrapolation of observations to the population, establishing a known and accepted margin of error. The definition of the methodology
for constructing a sample takes place through the elaboration of a sampling plan.

17

Inferential statistics complement descriptive

statistics. While the latter summarizes, explores and describes the data, the former makes statements that go beyond the mere
description of the data, such as, for example, (i) inferences about a population (if the data constitute a sample), (ii) predictions about
behavior future of the variables and (iii) recognition of trends, associations and correlations in the variables.

18

They just mean different things. The average value expresses the sum of all values, divided by the number of cases. It is higher
because a few executions of hundreds of millions of reais are capable of raising the average value. The median indicates the central
value when the results of all processes are listed in ascending order. For that reason, it is less affected by extreme outcomes. To
better understand the distinction, consider, for example, the following hypothetical list of results from a survey on the value in reais of
moral damage convictions granted by a civil court, based on a sample of 13 lawsuits:

The underlined value is the central or median result. Adding up all the values, it comes to R$ 43,000.00, which, divided by 13,
gives the average value per process of R$ 3,307.69. The median is R$ 1,000.00. The fact that the average indicates a value three
times greater than the median is explained by the effect that the last two results in the list (10 and 20 thousand) have on the total
value of the sum of the results. The mean is by definition sensitive to the inclusion of cases with extreme values, while the median is
not.

21

200, 300, 500, 500, 1000, 1000, 1000, 1000, 1000, 1500, 5000, 10000 and 20000

20

However, the average is not a good reference for how long a dissolution action in Brazil will take, because the standard deviation
observed in the results was too large. As in a group of dwarfs and basketball players, in which the average is that of an average
person, but no one in the population is of normal height, in the population of processes there were many actions that were absurdly
time consuming and others much faster. Thus, despite having an average close to five years, the standard deviation was 1,063 days,
indicating that the observed processes were 2 years, 11 months and 3 days above or below the average, with many actions lasting
more than 7 years and others being closed after only 3 years. Only through the joint analysis of these two summary measures was it
possible to visualize the enormous disparity in behavior within the Brazilian Judiciary and to understand in practice what the real
possibilities of duration of a process of dissolution of society are.

As it is less susceptible to extreme cases, the median value is closer to the reality of most tax foreclosures in the daily life of the
Judiciary. Therefore, the fact that the average value of tax executions carried out by the PGFN is R$ 26,303.25 does not lead to the
conclusion that the process is being used efficiently. The value to be observed is the median, of only R$ 3,154.39, well below the
average unit cost of these actions. In fact, according to IPEA, considering that the probability of full recovery of the credit is 25.8%,
the minimum value for filing a tax enforcement action by the PGFN is R$21,731.45. For execution values below this threshold, which
the median indicates are many, the public administration will be spending more than it will probably receive.

A

Imagine, making the example more sophisticated, that a fourteenth case was included in the list and its value for moral damages
was an extraordinary R$200,000.00, paid for the loss of an arm in a car accident. Due to this single additional case, the average
would increase six times, rising to R$18,692.30, while the median would move one house to the side and remain at the same
R$1,000.00. This is the reason why the average and median value of fiscal execution in the IPEA survey differ substantially. It is likely
that some tax foreclosures at the end of the list of cases analyzed presented extreme values, capable of pulling the sum of all results
and increasing the average. As in the example of moral damage, these same cases were not able to affect the median value.

The population of interest corresponds to the universe of all individuals from which the sample was obtained.

The results showed that the average amount charged in actions filed by the Attorney General of the National Treasury is R$ 26,303.25.
However, the median of these amounts was only BRL 3,154.39. The first amount is well above the average unit cost of a tax
foreclosure carried out by the Attorney General of the National Treasury, calculated at R$ 5,606.67. The second is slightly below.

Inferential statistics is the area that studies how certain conclusions can be logically induced from the analysis of a set of data
subject to random variation.

What do these values mean and which one is more correct? The answer is that neither is more correct.

Even measures of position must be combined and analyzed together. The combination between mean and median, for example,
is often illuminating. In another survey, the Institute of Applied Economic Research - IPEA, at the request of the National Council of
Justice - CNJ, conducted between 2009 and 2011 a study on the unit cost and time of the tax enforcement process in Brazil.
Concerned about the number of tax foreclosures, the CNJ decided to investigate how much these foreclosures cost the Public Power
and what value they effectively recovered.

III. Inferential statistics

Machine Translated by Google
23

22

24

IV. definition of probability

Every time we make statements
about a population from a sample or analyze whether the change in behavior of a variable is statistically
significant, we are speaking the language of inferential statistics.

The graph below, produced based on data provided by SERASA between 1997 and 2014, helps to understand
the situation of bankruptcy filings in Brazil before and after the law. If before 2005 the graph resembles that of
a cardiac patient having a heart attack, since the new law there has been an impressive reduction in the number
of bankruptcies required, followed by a stabilization at a level close to what can be interpreted as a possible natural rate.

The word probability originates from the Latin word probitas, which means "honesty". Roman probitas
expressed a quality attributable to people who performed public functions, such as politicians, government
officials, and witnesses . The association of the word probitas with witnesses gave rise to the term "probable"
in modern Portuguese, indicating a coherent and honest testimony, liable to be admitted in court as a
demonstration of controversial occurrences. Probability, proof, probable and probity are words of common
origin, all related to the attempt to obtain some degree of reliability in the midst of uncertainty.

An example of an important statistical inference for Jurimetrics is regulatory impact analysis. We often want
to understand how changing a law affected the behavior of the parties and judges in handling conflicts.
Empirical research that aims to evaluate changes resulting from changes in regulation is Through this, the
effects of changes can be monitored, called regulatory impact assessment. implemented in legal
regimes, or even to assess, preventively, whether a proposed law, once enacted, will or will not produce the
desired effects.

The fact that we can visualize the variation in the graph (as drastic as it appears to be) does not allow us to
say that it means anything from a statistical perspective. Variables, pardon the redundancy, vary by nature and
one of the goals of inferential statistics is to distinguish significant variations from others that may be the result
of random fluctuations. This theme will be discussed in greater detail in the topics below, but for now, it is
enough to state that, after carrying out the inferential tests of significance, the researchers concluded that the
reduction in the number of bankruptcy filings observed from 2005 onwards had a 1% probability of being result
of chance and that, therefore, it was statistically significant. With this, they concluded that, with a high
probability of success, the new law had produced an important part of the results desired by the legislator.

Some studies demonstrate how this analysis works. Law 11,101, which deals with Bankruptcy and Company
Recovery, enacted on February 9, 2005, had as one of its objectives to reduce the exposure of companies in
crisis to the risk of bankruptcy and to reduce the number of opportunistic bankruptcy filings ( used as a means
of pressuring the debtor to prioritize payment from the person making the request). At the request of the
Ministry of Justice (in the series Thinking about Law), the Getúlio Vargas Foundation of Rio de Janeiro sought
to answer this question (whether the law met its objective) through an impact analysis, in which the effect of
the new law on the amount of bankruptcies.

Sample is by definition a subset of the population of interest. By identifying certain characteristics of a sample,
it is possible to estimate the population parameters with some precision.

The results of the study were the following: with regard to the number of bankruptcies declared, the new law
reduced the average of 266 bankruptcies declared in the period of 12 months prior to the entry into force of the
new Law to an average of 185 in the following 12 months (reduction of 30.45%). The same was observed in the
requested bankruptcies. The number of bankruptcy filings before the enactment of the new law went from an
average of 1,030 filings per month in the 12 months prior to the new law coming into force, to an average of 452
in the following 12 months (a reduction of 56. 11%).

Machine Translated by Google
26

27

29

28

30

25

We saw that determinism has its origins in the speculation of Greek philosophers, with their characteristic
love for absolute truth and their contempt for money and practical everyday problems. Probability, however,
bears a less noble and more argent birth certificate. The concern with probability has its origin with the
appearance of games of chance and with the expectation of result. The Cairo Museum has specimens of
astralagus, cow heel bones and bristles polished and engraved with symbols, the shape of which allowed them
to be used in games of chance. The shape that these pieces maintain to this day and their ability, artificially
created by artisans, to be thrown like dice with each symbol having the same chance of coming out
(equiprobable results), prove that the Egyptians dominated and had fun with probability and the concept. relative frequency.
It is from this relationship with games of chance that the study of probability is born. The initial steps
towards a theory of probability were taken by pragmatic men, concerned with improving their betting
performance and making a fortune.

A variable has a probability distribution if the values it assumes are random, but the probability of it assuming
any value within the sample space can be calculated. The essence of probability lies in its way of facing the
limits of our knowledge, as it assumes a state of ignorance and proposes to make predictions regarding the
possible values that a given variable can assume.

The probability of something occurring is a
function of how often that something has happened in the past. This is an objective interpretation, since
probability is seen as a latent characteristic of the observed object. For example, the probability of a 25-yearold,
single adult living in São Paulo being involved in a vehicle collision is equivalent to the relative frequency
of the number of times that other people with the same characteristic were involved in accidents in the past.
Likewise, if throwing an unlucky die a sufficiently large number of times yields the result three 40% of the time
(when the expected would be almost 16.67%), for the frequentist, the die is objectively considered biased . and
the probability of outcome three is close to 40%.

Girolamo, son of Fazio Cardano, a lawyer friend of
Leonardo da Vinci, wrote in 1526 the Liber de ludo aleae (published in 1663), a treatise that, under the pretext
of teaching techniques for gambling, brings the first systematic study of related problems the likelihood.
Cardano studied probability with the aim of raising money to pay for his medical studies (his father wanted him
to be a lawyer) and his own support.

From a Bayesian point of view , probability is not a property of an object (such as biased data, whose
disproportion of chances in favor of an outcome is intrinsic to it), but it is a subject's belief regarding the
behavior of that object. . Probability is, for the Bayesian, the measure of an observer's uncertainty regarding
the chances of an event occurring, which, like any opinion, can change as the subject acquires more information
about the object.

The first Western scholar of probability was Girolamo Cardano, an Italian physician
who lived in Pavia, in present-day Italy, between 1501 and 1576.

It is necessary to distinguish between the definition and interpretation of the concept of probability.

Probability depends on the degree of uncertainty about an event, that is, it depends on randomness. There
is randomness whenever the outcome of a given variable does not describe a deterministic pattern, but presents

For example, Marcos works for an insurance company and needs to assess the chances of a dam bursting
in order to calculate the cost of insurance. There is no frequency of times this dam has burst in the past to be
considered. It is possible to calculate a worldwide frequency of dam failures, it is true, but replicating this
frequency disregards the differences that exist between each dam. Marcos, a civil engineer, decides to do the
following: first he calculates the generic rate of dam failure, arriving at an initial probability of, say, 1 in 462
thousand, called " a priori probability". Marcos then evaluates the quality of the project and the materials used
in the construction of that specific dam, the volume of water in the dam, the rainfall in the region over the last
20 years and the chances of a seismic shock in that area, adjusting its a priori probability . and starting to
believe that there are chances of 1 in 837 thousand of the breakup happening, the so-called " posterior
probability".

The definition of probability cannot be confused with its possible interpretations. The interpretation of
probability has two main strands, which correspond to two schools of statistical thought: Bayesian and
frequentist.

a sample space (set of possible outcomes) to which a probability distribution can be assigned.

Cardano is credited with creating the concept of sample space (ÿ or U notation for universe), which
corresponds to the set of all possible values of a random variable. The concept of sample space can be
illustrated by the data. In a die, the sample space is ÿ = {1, 2, 3, 4, 5, 6}, with the probability of occurrence of
each of the numbers equal to 1/6 if the die is fair or equiprobable. The importance of the sample space is to provide a view

From a frequentist point of view , the probability corresponds to the relative frequency of an experiment
repeated infinitely or, at least, for a very large number of times.

Probability is defined in mathematics by satisfying the axioms of Russian mathematician Andrei Nikolaevich
Kolmogorov (1903-1987). Kolmogorov's axioms start from the notion that every event has a sample space and
that: 1) each event in that sample space has a probability equal to a non-negative number, 2) the probability of
all events together is equal to one and 3) for any two disjoint events, the probability of their union is equal to
the sum of their probabilities. Probability is therefore defined as a mathematical function that satisfies
Kolmogorov's three axioms.

Machine Translated by Google
32

31

34

35

33

36

V. Probabilistic causality

But how do statistics and probability deal with knowing the truth? "Probability, statistics and truth" is the title
of the main work of Richard von Mises, considered one of the great names of modern statistics and brother of
economist Ludwig von Mises, first published in 1928.

For those who propose to investigate reality, statistics are more than an instrument for making a good guess,
what the Anglo-Saxons call an "educated guess". It is a technique that allows controlling uncertainty and making
inferences to understand and control uncertainty. Thus, despite not producing absolute truths,
statistics does not fall into a sterile skepticism, incapable of measuring the degree of significance of its
statements. Statistics is a tool capable of reducing our ignorance, a state in which it is not possible to make
meaningful statements, to modelable uncertainty, in which the chances of each event are quantified. But the
question that arises is: how to produce a statement in the midst of uncertainty? In order to clarify this point, the
topics below deal with some techniques for producing meaningful statements in the midst of uncertainty.

The argument goes like this: God may or may not exist, and there is no rational way to defend in absolute terms
either alternative. The solution is to bet. Faced with the two options (to believe or not to believe), believing in the
existence of God is the one with the greatest expected gain, since the multiplication of any number greater than
zero (relative to the chance of God's existence) by infinity (relative to the expected benefit with the grace of God)
is equal to infinity. Thus, no matter how small the probability that God exists, betting on his existence brings infinite benefit.

We learned that the law investigates the world of the must-be, where there would be no causal relationships,
only imputation. When a legal norm imputes a sanction to a conduct - for example, a prison sentence for those who

The main historical milestone in probability is the work of the Frenchmen Blaise Pascal (1623 to 1662) and
Pierre de Fermat (1601 to 1665), around what became known as the problem of points. Pascal was one of the few
geniuses of the first magnitude of humanity. Defining his occupation is somewhat complicated, as his
contributions include mathematics, philosophy, physics and literature. What interests me here is his role as the father of Probability Theory.
In 1654, Pascal was approached by his friend, writer and gambler Antoine Gombaud, the Chevalier de Méré, with
the following problem: two players intend to end a game of chance early, dividing the prize fairly based on the
chances of each one win the game. Taking into account the points scored by each, as well as the remaining
rounds, in what proportion should the prize be divided?

Pascal de Fermat's efforts were the first step towards the creation of a mathematical theory aimed at the study
of uncertainty, where fundamental concepts such as probabilistic event and probability distribution were already
articulated. Probability is important to statistics because of its ability to produce models capable of estimating
the occurrence of future events. It is through probability that statistics in general, and all empirical disciplines
based on it, are able to make prospective judgments about the future behavior of their object of study.

The association of terms raised
by the title leads to an important question, which is at the root of the late development of this branch of study:
amidst only uncertainty, how to distinguish significant from non-significant inference? Or more broadly: in the
absence of absolute criteria of truth, typical of stochastic reasoning, what does it mean to know the "true"
probability of an event? There are those who see a paradox in the attempt to organize knowledge based on
uncertainty. In the phrase of Leslie Ellis, polymath (from the Greek ÿÿÿÿÿÿÿÿÿ, varied knowledge) and English
mathematician: "Mere ignorance is no ground for any inference whatever. Ex nihilo nihil".
Although witty, Ellis' phrase does not correspond to the fallible knowledge that we are used to dealing with
when we try to explain physical reality, always changing and expanding. Rarely are we faced with absolutely true
knowledge, which does not involve some kind of uncertainty. Science differs from other dogmatic approaches
precisely because it always admits a probability of being wrong in its conclusions.

Pascal introduced the problem in his correspondence with Pierre de Fermat, a French lawyer and amateur
mathematician. The answer offered by both is considered the cornerstone of probability theory. The solution
takes into account two phases of the game: the one already completed (determined points) and the one to be
completed (undetermined points). First, the sample space of the possible results of the step to be performed is
constructed. Then, the points of the stage already completed are added to the points of each of the possible
stages to be carried out, bringing together all the alternative endings of the match. According to the PascalFermat
solution, each player must receive the share of the prize proportional to his wins in relation to the totality
of the alternative endings. The solution to the problem of points is the first explanation of the current concept of expected value, which

general over all possible values that a random variable can take, allowing a probability to be assigned to each
possible event. Events, it should be clarified, are combinations of elementary results from a sample space. For
example, the face of the die takes on an even value, where the event is {2, 4, 6} or the face of the die takes on a
value of three, where the event is {3}.

Our reality horizons expand every day and science must always be open to new facts that force the revision of
established theories. For those who, like Ellis, only admit absolute certainties, the recommendation is to follow
the example of the Pythagoreans: abandon reality and turn your interest to ideal objects. In the ideal world, 2 + 2
will always equal 4.

At the end of his life, already in his notorious phase of religious fervor, Pascal used this concept again in his
Thoughts, in a probabilistic argument to justify belief in God known as Pascal's wager.

Machine Translated by Google
39

37

41

38

40

In practice, however, the consequences of enforcing the rules matter a great deal. Law is a mechanism of
social control that depends on adherence to reality. If I sit down this afternoon and write in my living room the
constitution of a country that has my neighborhood as its territory, it will not be law for the simple reason that
society does not recognize these rules as binding. To be considered part of a legal order, norms need to be
obeyed to a minimum degree and, therefore, law is not just a set of abstract texts, which imposes sanctions on
conduct: it is an established authority capable of controlling the behavior of people. people.

It is the same with law. Adopting an anti-legal conduct does not cause a sanction in the deterministic sense,
because the application of the sanction needs to be mediated by an authority, which can be omitted. But in the
probabilistic sense, in which there is room for these possible omissions, it may be possible to speak of an
indirect causal relationship between a conduct and its sanction, provided that there is a strong non-spurious
correlation. Tax evasion can be a probabilistic cause of imprisonment, since the act of evasion significantly
increases the probability of the tax evader being arrested. Similarly, non-compliance with a contract can be the
probabilistic cause of execution, provided that non-compliance increases the probability of execution. In fact,
the less causal the norms are in the probabilistic sense, the closer to my fictitious constitution they will be.

Of course, an infallible legal order is a fiction. The rules are not applied in all cases. The police fail, judges
make mistakes and there are many cases in which the law should be applied and is not. Fallibility, however,
does not make Law immune to causality, at least not to the modern concept of causality. After the statistical
revolution, the idea of deterministic causality, in which the effect is necessarily associated with the cause, was
replaced by the notion of probabilistic causality, in which the effect is probably associated with the cause.
There is probabilistic causality when the realization of a given variable is capable of significantly increasing the
probability of realizing another variable. Thus, even if a certain cause is not always followed by the expected effect, the relationship
exists as long as there are high chances that this effect will occur.

Speaking of spurious correlation, it is important to underline here the difference between correlation and
causality. The fact that two variables are associated does not mean that one is the effect of the other. For
example, there is a correlation between the size of children's feet and their ability to solve math problems. While
this relationship is true, it is wrong to assume that growing feet causes increased intelligence. As we know, the
growth of the feet is associated with the general development of the child, including their neuropsychic
structure and their school training, which imply a greater ability to solve mathematical problems.
There is an American site dedicated to gathering absurd correlations, full of funny real cases, which show
how this confusion, if taken seriously, can be dangerous. Some examples: Between 1999 and 2009, the number
of people drowning in swimming pools in the US is correlated with the number of films in which actor Nicolas
Cage appears. Over the same period, per capita consumption of mozzarella cheese is correlated with the number
of civil engineering doctorates and the number of people electrocuted on power lines is correlated with the
number of marriages in Alabama. In other words, if we want to save lives, are we going to retire Nicolas Cage
and abolish marriage in Alabama? And as there is a lack of engineers in Brazil, are we going to encourage the
consumption of mozzarella cheese? Obviously not. But how to distinguish spurious correlation from causation?
Identifying whether a given variable is the probabilistic cause of another is a complicated problem. One of the
most common ways to try to solve this problem is through control group experimentation.
Control groups are subdivisions of a sample or population that allow experimental study of one variable at a
time.

This type of relationship is especially appropriate for the study of human society, where the search for
deterministic certainty is replaced by the identification of trends and propensities. Let me explain: Rain, for
example, is a probabilistic cause of car accidents, but that does not mean that if it rains, all the cars on the street will have an accident.

committing homicide -, the conduct would not cause the application of the penalty in the same sense that the
combination of acid and base causes a chemical reaction that results in water and salt. The norm would only
attribute a sanction to a conduct, idealizing, but not necessarily implying, its application by the authorities.
Causality, a relationship proper to all sciences, and the study of the real consequences produced by the norm
in society would, therefore, be alien to Law, a science of normative character that would not study a portion of
the real world, but only the logical relations established through of legal commands.

The following thought experiment may help with understanding. As a consequence of the
principle of the natural judge and freedom of conviction, judges are free to sentence their cases according to
their conviction. As a result of this discretion, we can believe that the sentences vary according to the
background, values and personality of each judge. In order to test this hypothesis, a researcher decides to
investigate the relationship between the judge's training and the level of severity of the sentence. In a first step,
this study is carried out through a survey in which the characteristics of actual sentences handed down and
their relationship with the socioeconomic background of the magistrate are verified. The observed results
indicate that the judges formed by the five colleges best ranked by the OAB Examination deliver sentences, on average, 25% more severe

It just means that the number of accidents tends to increase on a prolonged rainy day. Tobacco is a probabilistic
cause of lung cancer, not because it is enough to predetermine the disease, but because its consumption
increases the probability of incidence of cancer in that organ. And it is clear that the fact that some smokers
have not developed lung cancer is not evidence capable of invalidating the relationship between this disease
and smoking.

Machine Translated by Google
SAW. Statistical error, p-value and significance

47 48

45

44

43

46

statistic and the p-value. The p-value is defined as the probability of a hypothesis being rejected when it is true.

while the non-rejection of a false null hypothesis characterizes type II error.

In the example of bankruptcy law, the probability that the drop in the number of bankruptcy filings occurred due
to chance was only 1%. Hypotheses are of two types: null and alternative.

The answer is no, at least not based on this result alone. It is possible that the increase in sentences is due, as
anticipated above, to a difference in the profile of cases judged by judges, assuming that crime in large cities is
more organized and violent. Furthermore, cities with greater population density have higher income, greater social
disparity and, therefore, may constitute an environment prone to generating more offensive criminal conduct. If
this distinction is confirmed, the place of jurisdiction would be a spurious variable in relation to the severity of the
penalties.

Instead of dealing with the concept of truth, statistics works with the idea of statistical significance.
A result is considered statistically significant when the probability of being the product of a random fluctuation is
sufficiently small.

Statistical error, in turn, consists of the mistaken acceptance or rejection of a null hypothesis. In the example
above, the null hypothesis is that music therapy has no effect on a criminal's recidivism. On the other hand, the
alternative hypothesis is that the therapy has an effect.

True to its anti-determinist roots, statistical testing never declares a hypothesis to be true or false. It just
recognizes that the probability that the null hypothesis was wrongly rejected is small enough to declare the result
statistically significant. As it is a measure of probability, the p-value can vary between 0 and 1. The p-value level
to be established for rejecting or not a null hypothesis is called the level of probability.

Probabilistic reasoning and the logic on which it is based, inductive logic, is distinguished from other logics
(aletic and apophantic) by the role played by risk. Probabilistic reasoning always incorporates a risk component .

The null hypothesis is the one
that denies the existence of a relationship between the observed sample and the parameter to be estimated. The
alternative hypothesis is that it admits the existence of this relationship. Still in the example, the null hypothesis
was that the drop in the number of bankruptcy filings was not due to the new law, while the alternative hypothesis
stated that these two factors were associated.

graduates from other faculties.

One

The rejection of a true null hypothesis characterizes the type

I error, Still in the example, the p-value of the research is the relative probability that, at the moment of the division
of the two groups, 89 or more of the 101 men who would not relapse had been drawn for the therapy-exposed
group. That is, the probability of a Type I error.

The conclusion is that simple observational research would not be enough to answer researchers' questions.
One solution would be to carry out an experiment, in which two groups of judges would be asked to deliver
sentences on the same set of hypothetical cases. The groups would be divided according to the magistrate's
training institution. The creation of two groups helps to isolate the differences between the cases and to control
only the effects of the magistrate's training. With this, the sentences given by each group could be compared in
relation to cases of equal complexity and the results controlled according to the variable of interest in the study.

The issue of statistical significance is related to two other equally important concepts: error

Another function of control groups is to identify spurious correlations. Spurious are statistical correlations that
do not express a causal relationship. One more example may help: imagine that a researcher hypothesizes that
judges in cities with more than a million inhabitants hand down sentences with more severe penalties than judges
in smaller cities. A survey carried out revealed that sentences handed down by judges in big cities impose
sentences on average 37% higher than those handed down by judges in small towns. From this research, can we
conclude that the judge's residence in a large city is a cause of increased sentences?

However, the research is criticized for its lack of robustness. As each case has unique characteristics, the
validity of the result is questioned due to the possibility that the differences in the sentences result from the
peculiarities of each case, and not from the training of the judges. The position occupied by judges trained in the
best colleges could, for example, be associated with districts with more disputed entrances, which in turn
correspond to more populous cities. And the crimes committed in large urban centers may be characterized by
greater organization, thus explaining the more severe sentences. Thus, the severity of the decisions could be
explained not by the judge's training, but by the location where it was handed down.

Imagine an Orwellian example : an experiment aims to study the effect of musical therapy on the recidivism of
criminals. The therapy, consisting of lessons on any instrument and attendance at classical concerts, is
administered to a group of 100 criminals and the effects are compared with another group of criminals of the same
size who were not subjected to therapy. The result is that 89% of criminals who attended classes and concerts,
once released, did not repeat any criminal activity, compared to 52% of criminals in the group not undergoing
therapy. Given these results, the question that statisticians ask themselves is: assuming that the therapy would
not be effective anyway, what is the probability that 89 or more of the 101 men who would not reoffend would have
been, by coincidence, drawn into the same group? ? If this probability is small enough, the result of the experiment
is significant and we can say that anti-recidivism musical therapy actually works.

Machine Translated by Google
52

57

50

49

51

53

55

VII. Bias and sampling

The importance of sampling stems from practical circumstances related to the inaccessibility of the population
or 54 It is impracticable to directly question all voters and the cost and time required to carry out a census. each
survey regarding their voting intentions, since, due to the dimensions, the result of this census would be calculated
after the date of the election. Likewise, the cost of censuses on the audience of a television program with all viewers
or the degree of satisfaction of all consumers of a product can make the investment uneconomical. And, in addition,
even if it does not make the investment unfeasible, if the results of a correctly performed sampling are sufficiently
accurate, carrying out a costly census becomes an unnecessary waste of resources and time.

No sample is a perfect representation of the population and therefore there will always be a difference between
the estimator and the parameter. This difference is called sampling error.For the same reason, any two non-identical
samples will almost always show different results, simply because they are composed of different elements. One of
the efforts of sampling techniques is to control and reduce this error as much as possible. The idea behind error
control is that successive samplings will produce a distribution of values in which the central point, called the
expected value of the estimator, tends to coincide with the value of the parameter.

Because they allow estimation, the sample characteristics are

56 costly.

The calculation of the p-value and its comparison with a significance level can be illustrated through one more
example: an association filed a lawsuit against an industry, alleging that the use of a product it manufactures is the
cause of a disease. To prove this correlation, a laboratory study with rats was commissioned. This is an experiment
with control groups, in which 1,000 animals were exposed to the product and another 1,000 were not exposed. The
result of the experiment indicated that, from the group exposed to the product, 100 animals developed the disease,
while in the control group, 92 animals developed the disease. In percentage terms, we have 10% versus 9.2%, a
difference of 0.8% and a relative risk of 1.09 (10 ÷ 9.2). Some may consider this difference large enough to recognize
the cause of the disease in the product, while others may not.

To try to objectify this opinion, we have to calculate the p-value. Let us assume that the null hypothesis has not
been rejected and therefore the product is not a cause of the disease. Thus, the 192 rats would develop the disease
anyway, whether or not they were in the group exposed to the product. The 2000 rats in the experiment can be
randomly divided into two groups of 1000 rats, in which the 192 patients appear in varying proportions, from all 192
rats in the exposed group to the opposite, with the same 192 rats in the control group. In 54% of the draws (p-value
0.54) between 100 and 192 diseased rats would anyway be allocated concentrated, by sheer chance, into one of the
two groups (exposed or control), producing an equal or even more extreme result than observed. The p-value 0.54
is high enough for us to assume that the difference pointed out by the study may have been a coincidence and not
an effect of the product's action on people, in such a way that the null hypothesis of the experiment should not be
rejected.

the value of

The problem is that, if there is a bias in the sample, this coincidence will not occur and the estimator will
systematically differ from the parameter. For this reason, bias is defined as a systematic divergence between the
expected value of the estimator and the value of the parameter. The bias stems from problems in choosing the
sample estimator, in particular the intentionality in selecting its elements. Intentionality corresponds to a subjective
interference in the composition of the sample. Bias can be illustrated, for example, through customs inspection
processes. Given the large volume of goods that enter the country daily, customs inspection is carried out by
sampling. If inspectors do not conduct a random sampling and choose to select containers that are closer or easier
to access, traffickers and smugglers can bypass customs inspections by hiding illicit goods in containers that are
difficult to access.

The customs example shows why the most common sampling method is random or probabilistic. In random
sampling, each member of the population has an equal, or at least known, probability of being selected for the
sample. There is no intentionality because the researcher does not choose on a case-by-case basis which elements
of the population will be selected. Luck will define which elements will make up the sample. For random selection,
it is necessary that the population is delimited and all its elements are accessible. Problems in inference normally
arise when portions of the population are excluded from sampling because they are difficult to access or In addition
to making estimation viable, random sampling allows the calculation of the margin of error, which is the
statistical measure of sampling error. The margin of error corresponds to the amplitude of the confidence interval
around the estimator. This interval is random and may or may not contain the parameter, with fixed probability.

meaningfulness. The choice of p-value is a subjective choice, which depends on the degree of confidence required
by the researcher. It is evident, however, that the level of significance is a low value, varying, for most cases,
between 0.05 and 0.01.

characteristics of the population to which they
belong. called estimators, while the characteristics of the population are called parameters. estimate
is the estimate. It is important to clarify that samples do not represent populations and it is not correct to say, as is
commonly done, that a sample is representative of a population. Samples only allow, in certain circumstances,
knowledge by approximation of some of the characteristics of the population.

Sampling is the procedure through which a subset of elements is selected to estimate the

Machine Translated by Google
VIII. power and certainty

63

64

60

65

58

61

62

59

However, it is the belief in maintaining this
standard that allows us to bet, even under some degree of uncertainty, on the creation of new knowledge, since
deduction, totally averse to risk and entirely rational, ends up in creative sterility: deduction only organizes and
specifies what we already know. The inductive method is, therefore, the route of access to external objects and the
means capable of revealing, through repeated and systematic observation of objects, the universal order that science
believes exists.

In probabilistic induction, different degrees of probability are
attributed to scientific hypotheses, depending on whether the observed facts confirm their veracity or not. Probability
incorporates variability as part of its explanations. And the observation of occurrences that contradict a hypothesis
does not make it false, they only reduce its level of significance. There is, therefore, no falsifiability from a statistical
perspective, for the simple fact that there is no absolute truth to be falsified .

For this reason, there is no certainty in the result of a test, only different levels of statistical power. The power of
the test statistic is the probability that a hypothesis test will not make a Type II error.

Unlike deduction, which is based on fixed premises, induction relies on the belief that patterns observed in the
past will repeat themselves in the future. No logical or self-evident assumption guarantees that there is such an order,
whereby an observed pattern of the past must repeat itself in the future.

Within a deterministic perspective, inductive inference leads to a valid result only when all observations, without
exception, confirm the general law. Deterministic induction does not tolerate variations and a single divergent
observation, even among thousands of previous confirmations, makes the law false. This subjection to contradiction
is called falsifiability.

Denying the error is the quickest way to incur it.

Type II error is one in
which a false null hypothesis is not rejected. As the power of a test increases, the chance of a Type II error decreases.
The factors capable of influencing the power of a test are the statistical significance criterion and the sample size.

Concluding this part, I can even agree that the ideal of knowledge is certainty. However, the vast majority of issues
in our personal, professional and academic lives, especially those involving law, such as negotiating contracts or
defending a lawsuit, involve different degrees of risk and uncertainty.

Defining the sample size and structure is another important aspect. Contrary to common sense, the sample does
not need to be larger just because the population is large. The definition of sample size depends on two factors: the
population variance and the margin of error considered acceptable. Given a certain degree of precision with which a
parameter must be estimated, the greater the variance of that parameter, the larger the sample must be. Likewise,
given a variance of a parameter, the larger the sample, the smaller the margin of error in the estimation. Another
common sense to be demystified concerns the relationship between sample size and margin of error. Doubling the
sample size does not double the estimation precision. In fact, this relationship is expressed by the square root of the
ratio in sample increase. Thus, if the sample size is quadrupled, the estimation precision will only be doubled.

To understand what induction
is, I will also review the antithetical concept of deduction. While deductive inference is a path independent of
experience, which starts from the general (major premise) to the particular (minor premise) towards a valid conclusion,
inductive inference takes the opposite path, starting from the particular (observations of facts) to general (identification
of regularities that express a law).

To conclude, it is worth speculating a little more about the logic of statistical thinking and its confrontation with
the foundations of knowledge that, since Galileo Galilei and Isaac Newton, we have called exact sciences. The exact
sciences are associated with deterministic induction, replicability and falsifiability.

The results of a statistical test always include a component of uncertainty. Margin of error, p-value and significance
level are indications of approximation, which do not have the purpose or intention of eliminating uncertainty.

The divergence between the induction of probability and determinism lies in the degree of uncertainty that the
researcher is willing to assume.

Europeans first saw a black swan in 1697, when Willem de Vlamingh was
exploring the Swan River in Western Australia. The identification of the bird falsified the centuries-old conclusion that
all swans were white, became an allegory for the weaknesses of the inductive method and caused the expression
black swan to stop meaning an impossible event and refer to a highly improbable event. however capable of
drastically affecting our convictions.

For statistics, the truth is a wild animal that allows itself to be observed from some distance, but that will never allow
itself to be imprisoned. Error is part of human knowledge and, in addition to being a sign of honesty and intellectual
maturity, admitting its presence is the only known way to minimize and control its effects.

The deductive inference is exemplified by the famous syllogism: every man is mortal, Socrates is a man, therefore
Socrates is mortal. And inductive inference is illustrated by the famous example of observing swans. All swans seen
by Europeans until the 17th century were white. This regularity led to the generalization that all swans would be white
and caused the expression "black swan" to acquire, since ancient times, the popular meaning of something
impossible to happen, as in the poet Juvenal's phrase, ironically comparing honest people to a "rare bird on earth,
resembling a black swan".

Machine Translated by Google
FOOTNOTES

82.

CHURCHILL, Gilbert A. & BROWN, Tom J. & SUTER, Tracy A. Basic marketing research. São Paulo: Cengage Learning, 2011,
p. 65 to 67. COOPER, Donald R. & SCHINDLER, Pamela S. Research methods in administration. 7. Ed., Porto Alegre: Bookman,
2003, p. 390.

ZEIZEL, Hans; KAYE, David. Prove it with figures: empirical methods in law and litigation. New York: Springer-Verlag. 1997.
See especially chap. 1. The search for causes.

COOPER, Donald R. & SCHINDLER, Pamela S. Research methods in administration. Porto Alegre: Bookman, 2003, p. 420 to 471.

In the original: "In God we trust; all the others must bring data". Popularly attributed to statistician William Edwards Deming.

ACHENWALL, Gottfried. StaatssissenchaftdervornehmenEuropaischenReiche und Republiken. 1749. In the following editions
it was published as Staatsverfassung der Europaischen Reiche in Grundrisse (1752), in Portuguese: "Constitution of the main
European states". This work presents information regarding agriculture, manufacturing, commerce and population of the main
European states, whenever possible through statistics. This is the first work that summarizes the configuration of State
Economies, allowing a direct comparison between their main characteristics.

For an in-depth look at research planning: COOPER, Donald R. & SCHINDLER, Pamela S. Research methods in administration.
Porto Alegre: Bookman., 2003, p. 66 to 107 and 126 to 147. BUSSAB, Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6.
Ed., São Paulo: Saraiva, 2010, p. 1. MALHOTRA, Naresh. Marketing research: an applied orientation. 6. Ed., Porto Alegre:
Bookman, 2012, p. 57 to 198 and 365 to 368.

For various definitions: COZBY, Paul C. Research methods in behavioral sciences. 5. reprint São Paulo: Atlas, 2011, p. 30.
BUSSAB, Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6. Ed., São Paulo: Saraiva, 2010. p. 330 to 339.

HALD, Anders. A history of statistics and probability and their application before 1750. New Jersey: John Wiley and Sons, 2003, p.

ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneer Thompson Learning, 2006, p. 103 to 109 and 442
to 446. MALHOTRA, Naresh. Marketing research: an applied orientation. 6. Ed., Porto Alegre: Bookman, 2012, p. 365 to 367.

STIGLER, Stephen. The history of statistics. The measurement of uncertainty before 1900. Cambridge: Harvard University
Press, 1986, p. 1.

8
3

6
1

5
4

7
two

Consequently, we are forced to make decisions, armed with insufficient or contradictory information. Within
this world full of stochastic surprises, statistics is the research method capable of controlling uncertainty,
measuring the probability of success of arguments and, therefore, helping us make decisions with insufficient
information and surrounded by doubts.

Machine Translated by Google
BUSSAB, Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6. ed., São Paulo: Saraiva, 2010. p. 35. MALHOTRA, Naresh.

17

11

13

Marketing research: an applied orientation. 6. ed., Porto Alegre: Bookman, 2012. p. 363. ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneira Thompson

Learning, 2006, p. 389 to 391.

NUNES, Marcelo Guedes. Jurimetry of Corporate Law: A Statistical Study of Company Dissolution in Brazil. PhD final thesis. PUCSP, 2012.

BUSSAB, Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6. Ed., São Paulo: Saraiva, 2010. p. 9 to 252. ZICKMUND, William G. Principles of marketing research. São

Paulo: Pioneira Thompson Learning, 2006, p. 50 to 52.

Research methods in behavioral sciences. 5. reprint, São Paulo: Atlas, 2011. p. 261 to 263. MALHOTRA, Naresh.

BUSSAB, Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6. ed., São Paulo: Saraiva, 2010. p. 37 to 40. MALHOTRA, Naresh.

Technical note: Cost and time of the tax enforcement process promoted by the Attorney General of the National Treasury. Available at: [www.ipea.gov.br/agencia/images/stories/

PDFs/nota_tecnica/111230_notatecnicadiest1.pdf]. Accessed on: 07/30/2013).

COZBY, Paul C. Research methods in behavioral sciences. 5. reprint, São Paulo: Atlas, 2011, p. 286. BUSSAB, Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6. ed., São

Paulo: Saraiva, 2010. p. 261 to 266. ZICKMUND, William G.

BUSSAB, Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6. Ed., São Paulo: Saraiva, 2010. p 11 to 14. COZBY, Paul C.

14

18

9

12

15

Marketing research: an applied orientation. 6. Ed., Porto Alegre: Bookman, 2012, p. 360. CHURCHILL, Gilbert A. & BROWN, Tom J. & SUTER, Tracy A. Basic marketing research.

São Paulo: Cengage Learning. 2011. p. 374 to 375.

Marketing research: an applied orientation. 6th ed., Porto Alegre: Bookman, 2012, p. 364. ZICKMUND, William G. Principles of Marketing Research. São Paulo: Pioneer Thompson

Learning. 2006. p. 391 to 395.

Principles of marketing research. São Paulo: Pioneira Thompson Learning, 2006, p. 52 to 53.

10

379 to 382.

16

COZBY, Paul C. Research methods in behavioral sciences. 5. reprint., São Paulo: Atlas, 2011. p. 261 to 264.

ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneira Thompson Learning, 2006, p. 386. MALHOTRA, Naresh. Marketing research: an applied orientation.

6. Ed., Porto Alegre: Bookman, 2012. p. 58 to 67.

CHURCHILL, Gilbert A. & BROWN, Tom J. & SUTER, Tracy A. Basic marketing research. São Paulo: Cengage Learning, 2011, p.

There are also other types of summary measures, such as measures of symmetry in the distribution (obliquity) or measures of description of unusual or exotic results (outliers),

but measures of position and dispersion are by far the most relevant.

Machine Translated by Google
in:

DE GROOT, Morris H. Probability and statistics. Massachusetts: Addison-Wesley Publishing Co, 1989, p. 4.

BOLFARINE, Heleno & BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005, p. 37.

Guidelines.

MALHOTRA, Naresh. Marketing research: an applied orientation. 6th ed., Porto Alegre: Bookman, 2012, p. 269 to 288.

Available for download at:

According to the report, "the result - significant at the 1% level - indicates that the new Law provided a drop of approximately
87 bankruptcies per month, which means an average reduction of approximately 33% in relation to the period from June 2004
to June 2005. The result for requested bankruptcies indicates a reduction of 561 bankruptcy requests, after the entry into force
of the new Law. These numbers represent an average reduction of 54% in the number of bankruptcy requests in relation to the
period from June 2004 to June 2005". Thinking about Law Series Report n. 22/2010: Analysis of the new bankruptcy law, p. 21.

C0708AAE5DB1%257D&ei="v_XyUZu3GMXF4APlyoDgBw&usg=AFQjCNEAYy7B-37ipn9s_C9tlv4JjdSBYA&sig2=8foV
c7k14tMKCvJw1ht4A&bvm=bv.49784469,d.dmg]." Accessed on: 7/30/2013).

ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneira Thompson Learning, 2006, p. 357 to 360.

[www.portal.mj.gov.br%2Fservices%2FDocumentManagement%2FFileDownload.EZTSvc.asp%3FDocumentID%3D%257B68E6736C
4DF7-498B-ABC3-DBCFE29195F6%257D%26ServiceInstUID%3D%257B0831095E-D6E4- 49AB-B405-

Assessment

DEGROOT, Morris H. Probability and statistics. Massachusetts: Addison-Wesley Publishing Co, 1989, p. 2. BUSSAB, Wilson de
O. & MORETTIN, Pedro A. Basic statistics. 6. ed., São Paulo: Saraiva, 2010, p. 103.

BOLFARINE, Heleno & BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005, p. 42 to 47. COOPER, Donald R. &
SCHINDLER, Pamela S. Research methods in management. 7. ed., Porto Alegre: Bookman, 2003, p. 148 to 162. CHURCHILL,
Gilbert A. & BROWN, Tom J. & SUTER, Tracy A. Basic marketing research. São Paulo: Cengage Learning, 2011, p. 286 to 301.

See in general: BOLFARINE, Heleno & BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005. See also: BUSSAB,
Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6. ed., São Paulo: Saraiva, 2010, p. 262. COZBY, Paul C. Research Methods
in the Behavioral Sciences. 5. reprint, São Paulo: Atlas, 2011, p. 145 to 154.

Regulatory impact assessment is a mandatory phase of the legislative process in many countries. At the European Commission,
Impact Available [http://ec.europa.eu/governance/impact/commission_guidelines/docs/iag_2009_en.pdf].
They recommend regulatory impact assessment as a preliminary step to the promulgation of a new regulation or the
implementation of a public policy. In the USA, the Office of Information and Regulatory Affairs - OIRA, according to a statement
from its Executive Committee, seeks to evaluate the cost-benefit of implementing proposals and estimate the impact of future
regulations. Available at: [www.whitehouse.gov/omb/inforeg_default].

25

27

22

20

26

21

19

23

24

Machine Translated by Google
28

32

30

34

36

29

31

35

33

Quoted by: KEYNES, John Maynard. A treatise on probability. London: MacMillan, 1921, p. 85. The so-called "lottery paradox"
is an interesting example of criticism of probabilistic thinking. Accepting the idea of hypothesis testing, we assume that a
hypothesis should be rejected if the probability associated with it, from observations, is very small. In a lottery with 10,000
tickets, each one of them has a probability of 0.0001 of being drawn, which would lead us to reject, for each one of them, the
hypothesis of its drawing. The test leads us to the mistaken conclusion that no tickets will be drawn. The example is in
COHEN, Laurence Jonathan. An introduction to the philosophy of induction and probability. London: Clarendon Press, 1989,
p. 208.

The attribution stems from Cardano being the first to write a work dedicated to the study of probability. There are, however,
those who recognize the origin in the poem De Vetula, of controversial authorship, attributed by some to the Roman Ovid (43
BC-17 AD) and by others to the Frenchman Richard de Fournival (13th century). For more details, see HEIDE, C. & SENETA, E.

The full account of the points problem and the concept of expected value can be found in: HEIDE, C. & SENETA, E.

HACKING, Ian. The emergency of probability. New York: Cambridge University Press, 1975, p. 10-11.

Statistics of the centuries. New York: Springer-Verlag, 2001, p. 116 to 122.

MISES, Richard von. Probability, statistics and truth. London: Dover, 1981.

Statistics of the centuries. New York: Springer-Verlag. 2001, p. 227 to 269. See also: TODHUNTER, Issac. A history of the
mathematical theory of probability from the time of Pascal to that of Laplace. London: Mc Millian & Co, 1865, p. 7 to 21.

Not by coincidence, the word "fortune", disseminated in modern times by the work of Niccolò Machiavelli (The Prince), bears
both the meaning of luck and that of accumulated wealth.

Popper explains chance as follows: "One sometimes hears it said that the movements of the planets obey strict laws, whilst
the fall of a die is fortuitous, or subject to chance. In my view, the difference lies in the fact that we have so far been able to
predict the movement of the planets successfully, but not the individual result of a throwing dice. In order to deduce
predictions one needs laws and initial conditions; if no suitable laws are available or if the initial conditions cannot be
ascertained, the scientific way of prediction breaks down. In throwing dice, what lack is, clearly, sufficient knowledge of
initial condition. With sufficient precision measurements of initial conditions it would be possible to make predictions in this
case also; (... ) I speak of chance when our knowledge does not suffice for prediction; as in the case of dicing, where we
speak of chance because we have no knowledge of initial conditions." POPPER, Karl. The logic of scientific discovery. New
York: Routledge, 1959, p.205.

HEIDE, C. & SENETA, E. Statisticians of the Centuries. New York: Springer-Verlag, 2001, p. 128 to 134. HALD, Anders. A
history of statistics and probability and their application before 1750. New Jersey: John Wiley and Sons, 2003, p. 31 to 41.

PASCAL, Blaise. Les Pensées de Bl. Pascal suivies d'une nouvelle table analytique. Paris: Chez Lefrèvre Librarie, 1826, p. 252.

Machine Translated by Google
43

39

According to Ian Hacking: "Inductive logic is about risky arguments. It analyzes inductive arguments using probability. There are other kinds of risky arguments. There is inference

to the best explanation, and there are arguments based on testimony. Valid arguments are risk-free. Inductive logic studies risky arguments. A risky argument can be a very good

one, and yet its conclusion can be false, even when the premises are true. Most of our arguments are risky." HACKING, Ian. Introduction to probability and inductive logic. New York:

Cambridge University Press, 2011, p. 11.

ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneira Thompson Learning, 2006, p. 443. COOPER,

Hans Zeizel and David Kaye explain the importance that the identification of causalities has for the scientific effort, including statistical investigations into the functioning of the legal

order: "Among the many questions that are central to legal proceedings, the question whether one thing caused another is the most frequent. It occurs in civil and criminal litigation.

Does capital punishment deter crimes? Does a food additive cause cancer? Does a headache tablet work as advertised? Would additional information in a securities prospectus

have discouraged potential investors from an unwise purchase? Does the similarity in the names of two products lead consumers to buy one because of their familiarity with the

other, well-known respected brand? The list is endless.At least some of these questions can be addressed by collection and analyzing data rather than relying solely on seat-of-thepants
judgments". ZEIZEL, Hans; KAYE, David. Prove it with figures: empirical methods in law and litigation. New York: Springer-Verlag, 1997, p. 1.

40

Association is therefore different from causation. About association, see BUSSAB, Wilson de O. & Morettin, Pedro A. Basic statistics. 6. ed., São Paulo: Saraiva, 2010, p. 399 to

401. About the difference, see: ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneira Thompson Learning, 2006, p. 471.

Wilton Bussab and Pedro Morettin summarize what these investigation techniques would be based almost essentially on frequency distributions: "(omissis) the analysis of a set of

data through numerical and graphical techniques allows us to have a good idea of the distribution of this set . In particular, the frequency distribution is an important tool for evaluating

the variability of observations of a random phenomenon. From these observed frequencies we can calculate measures of position and variability, such as mean, median, standard

deviation, etc. These frequencies and measures calculated from the data are estimates of unknown quantities, generally associated with populations from which the data were

extracted in the form of samples. In particular, the (relative) frequencies are estimators of the probability of occurrence of certain events of interest. With appropriate assumptions ,

and without directly observing the random phenomenon of interest, we can create a theoretical model that reasonably reproduces the distribution of frequencies when the

phenomenon is directly observed". BUSSAB, Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6. ed., São Paulo: Saraiva, 2010, p. 103.

41

COOPER, Donald R. & SCHINDLER, Pamela S. Research methods in administration. 7. ed., Porto Alegre: Bookman, 2003, p. 137 to 142.

MALHOTRA, Naresh. Marketing research: an applied orientation. 6th ed., Porto Alegre: Bookman, 2012, p. 174. CHURCHILL, Gilbert A. & BROWN, Tom J. & SUTER, Tracy A.

Basic marketing research. São Paulo: Cengage Learning, 2011, p. 100. COZBY, Paul C. Research Methods in the Behavioral Sciences. 5. reprint, São Paulo: Atlas, 2011, p. 95 to

97.

42

38

37

ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneira Thompson Learning, 2006, p. 463. MALHOTRA, Naresh. Marketing research: an applied orientation.

6th ed., Porto Alegre: Bookman, 2012, p. 179 to 180.

Machine Translated by Google
44

46

53

51

49

47

45

52

50

48

54

COZBY, Paul C. Research methods in behavioral sciences. 5. reprint, São Paulo: Atlas, 2011. p. 296 to 300. COOPER, Donald
R. & SCHINDLER, Pamela S. Research methods in administration. 7th ed., Porto Alegre: Bookman, 2003, p. 395.

BOLFARINE, Heleno & BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005, p. 5.

Census is the exhaustive procedure of collecting and recording data about a population. See BOLFARINE, Heleno &
BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005, p. 21.

ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneer Thompson Learning, 2006, p. 442. COOPER,
Donald R. & SCHINDLER, Pamela S. Research methods in administration. 7th ed., Porto Alegre: Bookman, 2003, p. 392.

Zeizel and Kaye explain the concept of sampling as follows: "Sampling of some sort must be as old as trade and commerce.
If merchandise is shipped, the recipient, before accepting the goods, will try to determine through sampling whether the
oranges were fresh , whether the glasses were not broken, whether the tobacco was of the promised quality.In such
acceptance sampling, as it was later named, the careful recipient avoided the trap of looking only at the top layer of a case.

The correct statistical definition is that the p-value is the probability of obtaining a test statistic equal to or more extreme
than that observed in a sample, assuming the null hypothesis is true.

BOLFARINE, Heleno & BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005, p. 52 to 54.

COOPER, Donald R. & SCHINDLER, Pamela S. Research methods in administration. 7th ed., Porto Alegre: Bookman, 2003, p. 394.

COZBY, Paul C. Research methods in behavioral sciences. 5. reprint, São Paulo: Atlas, 2011. p. 300-301.

COZBY, Paul C. Research methods in behavioral sciences. 5. reprint, São Paulo: Atlas, 2011, p. 286 to 287.

By sampling from several odd places, he laid the roots to the basic modern sampling principle that gives all units of the
population from which the sample is drawn a known, nonzero probability of failing into the sample". ZEIZEL, Hans; KAYE,
David. Prove it with figures: empirical methods in law and litigation. New York: Springer-Verlag, 1997, p. 66.

Donald R. & SCHINDLER, Pamela S. Management research methods. 7. ed., Porto Alegre: Bookman, 2003, p. 392.

MALHOTRA, Naresh. Marketing research: an applied orientation. 6. ed., Porto Alegre: Bookman, 2012, p. 269 . BOLFARINE,
Heleno & BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005, p. 37 to 41. BUSSAB, Wilson de O. & MORETTIN,
Pedro A. Basic statistics. 6th ed., São Paulo: Saraiva, 2010, p. 261 to 264.

Machine Translated by Google
Prolegomena Apology Pragmatism. [www.existentialgraphs.com/peirceoneg/
prolegomena.html]. Accessed on: 08.07.2012. See also: ECO, Umberto. On mirrors and other essays. Rio de Janeiro: Nova Fronteira, 1989,
p. 159 to 161. COOPER, Donald R. & SCHINDLER, Pamela S.

In the original Latin: "rara avis in terris nigroque simillima cygno".

Methods of research in the field of administration. 7th ed., Porto Alegre: Bookman, 2003, p. 48-49.

Charles Sanders Peirce mentions a third genre, abductive reasoning, equated to a guess by free association ("guessing"). According to
Pierce, it is through abduction that we formulate hypotheses to be tested. "Let us now consider non-necessary reasoning. This divides
itself, according to the different ways in which it may be valid, into three classes: probable deduction; experimental reasoning, which I
now call Induction; and processes of thought capable of producing no conclusion more definite than a conjecture, which I now call
Abduction. [---] Abduction is no more nor less than guessing, a faculty attributed to Yankees. [---] Such validity as this has consists in the
generalization that no new truth is ever otherwise reached while some new truths are thus reached. This is a result of Induction; and
therefore in a remote way Abduction rests upon diagrammatic reasoning".

ZEIZEL, Hans; KAYE, David. Prove it with figures: empirical methods in law and litigation. New York: Springer-Verlag, 1997, p.

Available

106-107.

for

This modern meaning gained notoriety through the book by Nicholas Nassim Taleb. The black swan: the impact of the highly improbable.
New York: Randon House, 2007.

I'm

Cozby, Paul C. Research Methods in Behavioral Sciences. 5. reprint, São Paulo: Atlas, 2011, p. 303-304.

In:

As Ian Hacking explains: "We are determined by custom alone to suppose the future conformable to the past. When a see a billiard ball
moving toward another, my mind is immediately carried by habit to the usual effect, and anticipates my sight by conceiving the second
ball in motion. There is nothing in these objects - abstractly considered - which leads me to form any such conclusion: and even after I
have had experience of many repeated effects of this kind, there is no argument which determines me to suppose that the effect will be
conformable to past experience. It is not therefore reason which is the guide of life, but custom. That alone determines the mind in all
instances to suppose the future conformable to the past. However easy

in:

If the distribution of the variable to be estimated is too asymmetric, the sample can be stratified. For example, as there are marked
differences in the number and size of companies in the national territory, to study bankruptcy cases in Brazil, samples can be selected by
region (stratum), so that their results can later be combined into a more accurate estimate of the population. See BOLFARINE, Heleno &
BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005, p. 93 to 111.

BOLFARINE, Heleno & BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005, p. 28 to 29.

Cozby, Paul C. Research Methods in Behavioral Sciences. 5. reprint, São Paulo: Atlas, 2011, p. 19 to 21.

an

62

56

60

58

63

61

57

55

59

Machine Translated by Google
64

COZBY, Paul C. Research methods in behavioral sciences. 5. reprint, São Paulo: Atlas, 2011, p. 19.

Inductive reasoning is criticized for rushing uncertain conclusions based on limited observations. Sextus Empiricus, David Hume and Karl Popper
are remembered for discussing what became known in philosophy as the problem of induction.
According to Hume, we believe in induction because the patterns observed in reality keep repeating themselves. Such justification, however, is
fallacious by begging the question, as it justifies the validity of induction through induction itself. Still, Hume admits that induction is the only
method capable of creating knowledge and that even deductive reasoning depends on induction for the construction of its major premises. See:
HUME, David. An inquiry concerning human understanding.

66

As John Maynard Keynes explains, arguments based on some degree of uncertainty make up most of our reasoning and, the fact that they are
not deterministic, does not make them stop being rational. To ignore these methods of investigation is to leave aside most of the judgments we
make throughout our intellectual lives: "In most branches of academic logic, such as the theory of the syllogism or the geometry of ideal space, all
the arguments aim at demonstrative certainty. They claim to be conclusive. But many other arguments are rational and claim some weight without
pretending to be certain. In Metaphysics, in Science, and in Conduct, most of the arguments, upon which we habitually base our rational beliefs,
are admitted to be inconclusive in a greater or less degree. Thus for a philosophical treatment of these branches of knowledge, the study of
probability is required. The course which the history of thought has led Logic to follow has encouraged the view that doubtful arguments are not
within its scope. But in the actual exercise of reason we do not wait on certainty, or deem it irrational to depend on a doubtful argument. If logic
investigates the general principles of valid thought, the study of arguments, to which it is rational to attaches some weight, is as much a part of it
as the study of those which are demonstrative". Keynes, John Maynard. A Treatise on Probability. London: Dover, 2004, p. 8.

Indianapolis: Hackett Publishing Co, 1977. Paragraphs IV and V. See also: Empiricus, Sextus. Outlines of Pyrrhonism. Cambridge: Harvard
University Press, 1933, p. 283.

this step may seen, reason would never, to all eternity, be able to make it." HACKING, Ian. An introduction to probability and inductive logic. New
York: Cambridge University Press, 2001, p. 247.

© of this issue [2016]

65

Machine Translated by Google
dominated by statisticians and economists."

the special appeal due to jurisprudential divergence
or even the appeals against decisions that violate abstract norms, either
federal law (such as the special appeal

,

Consistency and predictability have always been ideals of law, expressed even in
the principles of legal certainty and equality. Knowing in advance that you will receive
the same treatment as your fellow man is a measure of the degree of civility in a
society. However, no matter how great the theoretical and practical efforts to
systematize the legal order have been, the uncertainties of Law have never been
eradicated. Even with the enactment of laws and the creation of consistency control
mechanisms, the act of judging continues to display an essentially human component
in which the law plays an important role, but no more than the intimate perception
that the judge has of the parties, their lawyers and the facts brought before them.

The relationship between law and statistics is old. The affliction of the party who
appears before a court and does not find ways to predict how his case will be decided,
or of the judge who tries, based on a set of evidence, to understand the truth of facts
that he did not witness, has always accompanied the life of Law. Overcoming this
anguish, bringing a greater degree of security and predictability to the Law, was an
aspiration nurtured by great exponents of this matter. The enactment of laws to guide
judges stems from the concern to give predictability to judicial decisions, reducing
arbitrariness and inconsistency resulting from the different opinions of magistrates.

"Today the study of Law is still an activity of booksellers; but in the future it will be

Those who work in the courts know how sensitive the dynamics of judgment are
and how the outcome of a case can be affected by apparently irrelevant occurrences.

In addition to issuing general laws, several mechanisms seek to give greater
consistency to the legal order, such as, for example, resources to re-discuss decisions
handed down by a majority (and not unanimity), as is the case of embargoes infringing
the creation of judicial instances aimed at reviewing conflicting judgments with those
of similar cases, such as the embargoes of divergence, the incident of standardization
of jurisprudence, and the summary
binding,
those relating to the jurisdiction (such as the
claim, violation of federal law) or the Federal Constitution (such as the extraordinary appeal).

Research conducted by Columbia University in New York and Ben Gurion University
in Tel Aviv over ten months analyzed 1,112 decisions handed down by eight judges in
Israel on parole. In order to classify the decisions, each judge's day was divided into
three periods with two meal breaks: lunch and a snack. The results indicated that
immediately after meal breaks, judges granted approximately 65% of requests, a
percentage that fell

I. Beginnings

3

8

9
7

4

1

two

5 6

Chapter 4. Origins of Jurimetria

2018 - 07 - 17

Jurimetry
CHAPTER 4. ORIGINS OF JURIMETRY

Machine Translated by Google
10

11

15

12

16

13

17

14 is

of life expectancy to determine the presumed date of death.

Another notorious example is that of the Bernoulli family, especially the
mathematicians Jacob and Nicholas. Jacob Bernoulli, Swiss mathematician (1654-1705),
is the author of Ars Conjectandi, a fundamental work of probability theory, in which the
law of large numbers is defined for the first time.

This research, although surprising in the way it quantifies bias, illustrates what
every experienced lawyer knows. A trial is a highly complex event, which can have
its outcome affected by seemingly irrelevant details. A clumsy answer, a cry at the
right time, a delay or an extravagant tie can change the outcome of a trial. This
complex interaction between facts, perceptions, personalities and beliefs increases
the degree of complexity of the process and, consequently, makes the meaning of
any judicial decision ultimately uncertain.

Faced with the impossibility of eliminating uncertainty from law, all that was left
was to try to control it. Blaise Pascal was still alive when some first-level intellects
realized that the new statistical theoretical tools could be used to model judicial
decisions. These intellects were few but valuable. Gottfried Wilhelm Leibniz, a
seventeenth-century German philosopher, graduated in law in 1655 with a thesis
called De Conditionibus, in which he studied what he called conditioned law, defined
as the future and uncertain events to which an obligation is subordinated.

Written between 1684 and 1689, this work, first published only in
1713, covers several topics related to probability and combinatorial analysis, such
as permutation, arrangement and combination, being considered the seminal work
in both areas. The title Ars Conjectandi (Art of Conjecture) alludes to the
work Ars Cogitandi (Art of Reasoning), a guide to exact rationality and the main
manual of apophantic logic at the time.The art of reasoning was the logical deduction
applicable to statements about which we assume we are certain, while the art of
cogitation would apply to statements about which we have some degree of doubt.

According to Leibniz, the conditioned right would occupy the intermediate space
between jus nullum (non-existent right) and jus purum (guaranteed right). Linking to
a random future event attributes a probabilistic nature to the condition, whose
chance of occurrence can be measured between zero and one.

to close to zero as the judge went without eating, returning to the original 65%
shortly after the second interval. As the cases were considered in the random order
in which the lawyers arrived, without being organized by complexity or severity, the
conclusion of the study is that the defendants with requests considered, right after
the judge had eaten, had an advantage over the others.

Nicholas Bernoulli was Jacob's nephew. He studied Law in Basel and defended a
doctoral thesis, in which he sought to explore the possible applications of his uncle's
Ars Conjectandi in the legal universe. The title of the thesis is a statement of purposes:
De usu artis conjectandi in juri, or (in a free translation) of the use of the art of
conjecture in law. In this work, Nicholas exercises several aspects of the relationship
between law and probability, stating, for example, that the value of contracts relating
to uncertain events can be calculated based on the frequency of these events.
Bernoulli is the first to carry out frequency research on judicial decisions and to suggest the use of tables

Leibniz did not develop questions related to the
calculation of conditional probability or the applications of these concepts in
concrete legal cases, but he deserves the merit of associating probability for the first
time with an epistemic state of doubt about an event (and not with frequencies of
results ) and to apply it to problems unrelated to gambling.

Machine Translated by Google
20

21

22

23

19

18

II. Holmes and the behavior of the courts

Jacob Bernoulli's explanation of what it means to conjecture about human
behavior is important because it links, for the first time, probability to a decision
theory, associating this set of methods with a guidance instrument for decision
making not only in gambling, but in any environment where there is uncertainty. For
Jacob, the essence of the wisdom of the philosopher and statesman is not in
mastering an arsenal of metaphysical or humanist concepts, but in the knowledge of
probability.

The notion that the objective of studying law is to anticipate the understanding of
the courts has strengthened mainly in the United States, due to the case law structure
and pragmatism of that country. Oliver Wendell Holmes, Jr., judge of the American
Supreme Court and one of the most prestigious and influential scholars in the USA,
in a text written at the end of the 19th century, removes the philosophical veneer of
legal academicism and returns law to its concrete plane, closely related to the
uncertainties of human experience, stating that the work of jurists and lawyers is
limited to anticipating what the courts will decide. In his phrase, which has become
a slogan: "Predicting what the courts will actually do, without any additional pretense, is what I

These seminal works, however, did not promote a sustainable line of research in
law. The legal studies that followed remained linked to a theoretical and dogmatic
stance focused on the interpretation of abstract norms and discussions of an
axiological essence. The seeds of the statistical approach to the study of law
hibernated for more than two hundred years, until the union of the pragmatic spirit
of a country undergoing rapid economic development with a customary caselaw
system allowed the resumption of this proposal in a new context .

The origin of court decisions was, for Holmes, not in rational logic, but in the
experience of those who judge. The job of legal operators was, therefore, to
anticipate, based on the history of previous decisions, in which direction future
orders would be issued. Due to the emphasis on predictability and the frequency of
decisions, it was natural that this way of thinking resulted in an approximation
between statistics and law, relativizing the importance of the dogmatic study of
principles and abstract general rules. Holmes' conclusion is, as expressed in the
epigraph of this chapter, exhaustive: the jurist of the future will have mastery of
Statistics and will be able to bring the study of Law closer to a true "science". The
ideal to be pursued is predictability of what the courts will decide, which statistics and probability can

I understand right."

The idea that the judge's work is not just limited to declaring the will of the law
and that, therefore, judicial activity involved other operations in addition to the effort
to find previously established general rules, was innovative. At the end of the 19th
century, American law schools continued to focus on the study of general legal
principles and rules, understanding the philosophical foundations of law, as well as
investigating the logical relationships between rules. American doctrine was then
experiencing the end of its Era of Formalism, based on the doctrine of stare decisis
and the generalization of logical principles and abstract rules, which, in the end,
were compacted into voluminous investigative treatises.There was no appreciation among jurists for
other approaches, today called consequentialist or functional, characterized by the
search for a greater understanding of the influence of social facts on judicial
decisions. Nor was much importance given to the practical consequences resulting
from this application and, therefore, support was not sought in other areas of knowledge such as

Machine Translated by Google
III. Pound and Cardozo

24

25

27

26

same genre, which are still successful today.

The path of law was first published (there have been several reprints over the years)
in vol. 10 of the Harvard Law Review, in 1897. Holmes conceived the ideas exposed in
the text in the midst of the American Gilded Age , a period between the years 1850 and
1900 in which the United States ceased to be an agrarian society to become an
industrial power. In this fifty-year interval, a structure of judicial precedents, designed
for disputes characteristic of rural communities and fairground traders, was subjected
to a new reality, in which large corporations, unions, investors in an emerging capital
market, and consumers increasingly interacted. more frequently. No wonder, Holmes'
words echoed deeply in the jurists of his time, reverberating in all centers of study
and were investigated in depth by two other great American jurists, Benjamin Nathan
Cardozo and Nathan Roscoe Pound, two leaders in the transition between "Age of
Formalism" and the "Age of Legal Realism", which then began.

Nathan Roscoe Pound, originally trained in botany, was one of the most influential
American jurists, dean of Harvard Law School between 1916 and 1936 and advocate of
a sociological approach to law ("sociological jurisprudence"). Pound published an
article in 1908, while he was president of the University of Nebraska, with the title
Mechanical jurisprudence, harshly criticizing those who saw in the activity

Faced with the relative impotence of old precedents, it was imperative that legal
formalism be relativized, giving way to more pragmatic approaches that were also
capable of illuminating interesting aspects of law. The relativization of formalism
presupposed a new vision of law, in which general rules were momentarily left aside
and concrete judicial decisions, including the analysis of the facts discussed therein,
occupied a more important position.

Cardozo dedicated each of his lectures to
one of the elements that affect the decision to judge: philosophy, history, sociology
and precedents.

of the judicial process. This work is considered the first attempt to explain in an
organized way how judges decide their cases and starting a lineage of books from the

In explaining the nature of the judicial process, Cardozo does not deny the role of
logic in forming the judge's conviction. However, his lectures are among the first to
state that the judge creates and not just declares the law ("judge as a legislator"), and
that the act of judging is not limited to deducing concrete solutions from abstract
principles. Cardozo states that there are several cases for which there are no
precedents or applicable general rules, forcing the judge to create the norm. When
assessing the set of elements capable of influencing the decision, the judge must
decide in a consequentialist way, with a view to the benefit of his community.

Alongside Holmes, Roscoe Pound and Benjamin Cardozo are considered the main
precursors of legal realism.

Psychology, Sociology, Statistics and Economics. This reality, however, was about to
change in the first decades of the revolutionary 20th century.

Benjamin Nathan Cardozo was a lawyer and renowned judge of the American
Supreme Court, occupying the vacancy left by Oliver Wendell Holmes Jr. On four
straight nights in February 1921, already as a judge on the New York Court of Appeals,
but before moving on to the Supreme Court Court, Cardozo gave a series of lectures
at Yale Law School (the Storrs Lectures), later published in book form under the title of The Nature

Machine Translated by Google
28

29

31

30

IV. legal realism

jurisdiction the mere mechanical application of principles and abstract general rules.
For Pound, the formalist view of law petrifies the judiciary and prevents the judge's
creative activity from being mobilized to resolve new controversies and to critically
adapt old solutions that, whether due to overcoming or due to incompatibility, are no
longer satisfactory.

At the origins of jurimetrics and other academic currents that use empirical
methodologies in the investigation of law, legal realism occupies a prominent place.
This importance arises from the realists' emphasis on understanding law not as a
set of abstract principles and values, but as an everyday and concrete fact integrated
into social reality. This emphasis on law as a social fact inaugurates a modern path
of interdisciplinary studies between, for example, Law and Sociology, Psychology
and Economics, based on field research, interviews, experiments and observations.
And also because of this emphasis on people's real behavior, realists believe that
understanding the law involves studying court decisions, which, ultimately, are the
institutions in which various social factors interact and combine into judicial orders.
concrete with obligations of defined content and certain recipient. The reality of the
courts is, for realists, true law.

The botanical Pound can be recognized in the jurist's writings. Pound defended
an organic law, sensitive to social changes and capable of evolving and adapting to
the social environment. He also believed that legal conceptualism, understood as
the search for theoretical solutions to cases, was destined to disappear. Concepts
are fixed and it is impossible to deduce from them all the solutions for the cases that
come before the courts. Solutions should not be based on logical calculations or
historical definitions, but on considerations of what is best for today's society.

Legal realism is a polymorphous movement, originating in both the United States
and Europe. In this, the movement appears in the 1940s through Scandinavian
realism, also known as the Uppsala school. The movement had as exponents the
Swedish lawyer Anders Vilhelm Lundstedt (1882-1955), professor of law at the
University of Uppsala; Karl Olivecrona (1897-1980), professor of law at Lund
University; and the Danish philosopher and lawyer Alf Ross (1899-1979), professor
of law at the University of Copenhagen. Alf Ross is arguably the most well-known
name.

Ross distinguishes three types of grammatical sentences (directive, assertive and
exclamation) and states that the rules of Law are of the directive type, as they aim to
direct people's behavior. For Ross, Law is created to influence human
behavior and, consequently, effectiveness is an essential quality of norms. Therefore,
a directive becomes valid not when it is promulgated, but from the moment it proves
capable of effectively directing people's behavior. A completely ineffective rule is not
a legal directive. In a chess game, it is not enough for a manual to list the rules for
moving the pieces for these directives to be valid. Players need to feel connected and

Oliver Holmes, Roscoe Pound and Benjamin Cardozo are thinkers who reached
the peak of their intellectual influence in the first three decades of the twentieth
century. They are considered precursors of legal realism, the most influential school
in the history of American legal thought, which emerged around the 1930s and
extended its influence throughout the 20th century, profoundly changing the profile
of American law schools and the way legal institutions of that country work.

Machine Translated by Google
37

33

36

35

34

38

32

based not on abstract law but on a reaction to the concrete facts brought before the court.

The division of law, into a scheme of interpretation and behavior in action of the
players, gives rise to what Ross called doctrinal and sociological studies of Law. The
doctrinal study investigates the abstract meaning of the directives and does not
intend. The sociological study seeks to understand the

The main target of the realists was the dogmatic way of studying law. The
expressions used by its defenders speak for themselves. Jerome Frank harshly
attacks the "basic myth", "legal fetishism" and the atavistic search for "paternal
authority", responsible for this belief in general rules capable of supporting all court
decisions.

As laws are addressed to judges (for application in disputes
submitted to jurisdiction), the sociology of law proposes to verify the degree of
adherence of these directives to specific cases brought to court. For Ross, the
Sociology of Law is an empirical science whose object does not end with the study
of the meaning of legal commands. Its main object is to verify the adherence of
human behavior to these commands and, therefore, their degree of validity.

Karl Nickerson Llewellyn referred to general rules as "cute
toys", which were manipulated, discarded and replaced at the judge's convenience.

American realists, like their European counterparts, believed that the judge was
not a mere enforcer of rules but a creator of law. However, some more radical
currents, driven by the flexible environment of Common Law, expressed an aversion
to the theoretical study of Law. The most extreme argued that the study of the
abstract plan should be replaced by the investigation of the behavior of the courts
and the interaction between parties and government authorities, and that the law
suffered from a congenital indeterminacy, which limited its operational capacity to
offer solutions to problems. cases. That would be the reason why most of the judges' decisions

master any forecasting ability. real law in
action and aims to predict the concrete behavior of the subjects to whom the

The attack was clearly directed at the idea, disseminated
by faculties, that law was reduced to a static and abstract body of principles, laws
and general rules and that the operators' job was to identify, for each specific case,
which of them would be applied.

Scandinavian realism had repercussions, but arguably the American movement
was historically broader, complex, influential, and perennial. Counting on ranks of
reformist intellectuals, American realism gained momentum at the time of the Great
Depression and the stock market crash of 1929, which explains the fact that many
were involved in the formulation of President Theodore Roosevelt's New Deal
policies . His influence on the history of American legal thought is remarkable and
definitive. From a historical point of view, realists were looking for a justification to
relativize the application of centuries-old legal rules, most of them liberal in nature
(binding of parties to the contract, property rights), paving the way for the institutional
reforms they intended to implement.

To combat old orthodoxies, realists also claimed that this body of

behave according to the rules, moving the knight in "L", the rook in rows and
columns, the bishop in diagonal and so on. The norm therefore serves as an
interpretation scheme for the phenomenon of Law in action, capable of giving
meaning and coherence to the set of behaviors of judges and players, being valid
only when it is complied with.

Realism was, in this sense, a

movement of reform and opposition to the classical way of researching and studying
law, which proposes a more proactive and pragmatic attitude, and acquires strength
in a time of economic crisis.

directives address.

Machine Translated by Google
39 40

41

In both the American and Scandinavian aspects, legal realism contributed to the
fact that, from the second half of the 20th century onwards, Law began to adopt
empirical research methodology in its investigation processes. This contribution
stems from the following factors.

It is interesting to note that among the main American realist leaders there were a
considerable number of lawyers and professors of Commercial Law. Karl Nickerson
Llewellyn, for example, an exponent of the movement at Columbia University, was
one of the authors of the first edition of the American Uniform Commercial Code , in
1952. William Underhill Moore, from Yale Law School, worked as a lawyer and taught
Banking Law. Jerome New Frank, also from Columbia, was a specialist in capital
markets and was chairman of the Security Exchange Commission between 1939 and
1941. Theodore S. Hope Jr., co-author of articles with Underhill Moore, was a
corporate law attorney in New York.

An explanation for this preponderance lies in the distinction between the dynamics
of public law and private law. In the branches of Public Law (such
as Criminal Law, Tax Law and Administrative Law), the relationship between the
jurisdiction and the State is one of subservience and is established "from top to
bottom", through rules whose obligation and sanction are delimited: if the citizen
earns income, he must pay tribute; if he kills someone, he must suffer a sentence of
deprivation of liberty. The justification for the norm is the will of the State, its best
interpretation is that which favors the public interest (almost always confused with
the interest of the State) and its analytical structure is clearly exposed in the law itself.

and Herman Oliphant ,

First, through a shift from the abstract plane of the rules to the concrete plane of
the courts. By placing the populations of judicial decisions in the position of main
object, realism created the conditions for statistics to enter the scene. As long as the
law orbited the plane of singular norms of the abstract plane, that space did not exist.

In the areas of private law, illustrated by commercial law, the parties are "equal to
equal" and build legal relationships whose meaning does not derive from the
supremacy of the interest of one of them, but from a pragmatic, complex and, often,
nebulous context. for judges and for lawyers themselves. Private norms appear
unstructured in verbal contracts, which are laconic, antinomic or summarily written
(to reduce costs), in addition to being surrounded by "recitals", which refer to facts
and social situations. As a result, private operators are often pushed towards a law
of uncertainty, in which the interpretation of facts is more important than the
interpretation of norms. This is why tax, criminal and proceduralists have greater
affinity with analytical schools of Law, while commercialists tend to adopt empirical
stances.

Second, but not least, realists were brave in assuming that the

general rules was incomplete, contradictory and unstable. If for Scandinavian realists
the law served as an interpretative scheme for human behavior, without which there
would be no Law, for Americans laws were dysfunctional (not to say useless), as
they were full of gray areas and gaps and, above all, because they are constantly
changing. For American realists, law is created by the courts and there is no abstract
legal plan capable of predetermining exactly what is the "correct" law to be applied.
Finally, realists believe that court orders respond to various stimuli such as the
peculiarity of the facts, the judge's training and values, and interaction with lawyers
and parties. For this reason, there would be an indeterminacy inherent to the activity
of judges, who are obliged to invent certainty where it does not exist.

Machine Translated by Google
V. The expression jurimetry

43

44

42

At the time of Loevinger, American jurisprudential precedents accumulated by the
thousands and began to be archived in the first computer systems of the courts. As
the study of precedents is the basis of American law, Loevinger offered to create a
mechanism capable of transferring these precedents to electronic media, in order to
facilitate the storage and location of decisions through search engines. In addition to
facilitating searches, such a mechanism would have a great advantage over the
casuistic practices of traditional research: measuring the frequency of decisions
through an objective methodology subject to veracity tests. For Loevinger, true
science must be falsifiable and this new methodology would describe the legal
phenomenon in an impartial and comprehensive way, measuring the Law within
falsifiability standards.

Third, while judicial decisions were seen as corollaries of general rules, it was up
to the jurist to analytically study this abstract plan and deduce from it the only correct
solution for each case. Legal realism broke with this dogmatic tradition by seeing the
decision as the result of a convergence of social, economic, political, ideological and
personal factors. The analysis of these factors brings with it different methodological
requirements and demands the use of statistical models.

step forward, published by Loevinger in 1949. The influence of American realism on

With its new formulations, legal realism opened a door for Statistics and
Right, after some historical delay, they finally found each other.

then as technologically impractical.

From a young age, Loevinger was interested in the relationship between law and
new technologies and produced articles on radio and television transmissions, as
well as on storage and retrieval of computational data. Loevinger was also interested
in issues related to research methodology in law, attested by his initial studies in
legal logic.

The expression jurimetrics was used for the first time in the article Jurimetrics: the next

law has an insurmountable component of uncertainty, arising from the complexity of
the human decision-making process. While law was dedicated only to the subjective
certainties of the duty plan, of opinions on how a judge should decide, discussions
revolved around what was the most theoretically correct interpretation. When realism
proposed to invade the swampy plane of judicial prognoses, of predictions about how
a judge might decide, a wide space was opened for statistics.

Probably due to his contact with economic
studies during his work in the antitrust division, Lee Loevinger intuited that a
methodology similar to that of econometrics could be used to describe the legal
phenomenon.

The word jurimetry is a neologism created by the American lawyer Lee Loevinger,
an admitted admirer of legal realism. Loevinger was born in 1913, in the city of Saint
Paul, Minnesota, and served as a judge of the Supreme Court of that state between
1960 and 1961, until he was invited by President John Kennedy to assume the position
of attorney general of the antitrust division of the American Federal Government, at
the time headed by the president's brother, lawyer Robert Kennedy. After three years
fighting cartels, Lee Loevinger assumed the position of director of the Federal
Communication Commission - FCC (the American version of the National
Telecommunications Agency - ANATEL), an opportunity in which, armed with a
pragmatic spirit, he demanded from the American Telephone and Telegraph Company (the AT&T) the creation

Machine Translated by Google
46

47

45

SAW. uncertainty in law

What exactly this general program
would be is not clear. Loevinger only clarifies that jurimetrics intends to describe the
behavior of witnesses, parties and judges, investigating why the former lack the
truth and how judges judge. In addition, it would help to make legal language more
objective, speed up processes, avoid inappropriate behavior and prevent crimes.

work is evident, starting with the title of the article, which refers to the classic text by
Karl Llewenllyn, A realistic jurisprudence: the next step.

The examples of jurimetry cited by Loevinger refer to the first programs for
computerizing judgments, which were beginning to be accumulated in data storage
systems in American courts. Much is said about the cataloging of decisions and the
use of keyword searches, which at the time were presented as a novelty, but which
are now commonplace. There is also great emphasis on the association of jurimetry
with the use of computers, which largely explains the confusion between jurimetry
and legal informatics. On the other hand, Loevinger is not concerned with defining
what this scientific approach to law would be, nor does he make direct reference, at
least in the first articles, to the use of statistics, which had already been applied in
econometrics with expressive results. Despite referring to jurimetry as a legal
investigation methodology ("methodology of legal inquiry"), Loevinger's texts lack a
more detailed explanation of what this methodology would become.

Loevinger presents jurimetry as a step forward from that already taken by
American realism a few decades earlier. However, when delimiting what this new
field would be, Loevinger chooses to leave the definition open, moved by a radical
rejection of conceptualism and theorization. When citing the expression for the first
time, Loevinger states in a footnote that the term itself would not be important as
long as it referred to a scientific discipline with a general program. The expression
jurimetry seemed adequate to him, because it was different and referred to other
similar expressions, such as econometrics and biometrics, but it could be replaced
by any other that adequately fulfilled the same function.
Loevinger is a supporter of
the realists' main criticisms of traditional research, especially the rejection of
mechanistic conceptions of law, the conceptual confusion of legal theory, decisionmaking
based on assumptions and the need for accurate predictions to replace
idiosyncratic opinions. Law needed to free itself from its superstitions and appropriate
scientific methods of analysis, getting rid of the old experts and their authoritative
arguments.

In another article, still faithful to his anti-theoretical convictions, Loevinger states
that no definition of Jurimetry would be compatible with the pragmatism of this field.
It would be up to future jurists to define the limits of this new area of knowledge in
an ostensive way and illustrate its content through practical activities. Loevinger
believed that this new discipline would be in constant mutation and expansion and
that its contours would be distinguishable only through a comparison with the
traditional method of studying law, called jurisprudence in the USA. The classical
method would be concerned with the study of the nature, sources, formal structure,
concepts and purposes of law. Jurimetry, on the other hand, would focus on the
quantitative study of judicial behavior, the application of information theory and
mathematical logic in Law, the recovery of legal data mechanically and electronically
and the development of predictive calculations on the outcome of cases.

Machine Translated by Google
Loevinger's main merit was to create the expression jurimetria. However, the misunderstanding of the
role of statistics in social research, the insistence on a deterministic view of knowledge, the excitement
about information technology and conceptual uncertainty ended up harming the idea. Like most scientists
until the 19th century, he believed that knowledge was associated with accuracy and that speculation
surrounding the indeterminacy of law undermined the foundations for the construction of true knowledge.
Bound by the concept of deterministic causality, Loevinger understood that uncertainty would deprive the
scientist of the means of identifying the causes of a phenomenon, preventing him from formulating
predictions regarding its future behavior.

Loevinger's approximation with the realists, to whom he pays significant tribute, ends in the
discussion of the problem of the indeterminacy of law. For Loevinger, by stating that Law would be
indeterminate, realists would have abandoned this in a space of irrationality inaccessible to scientific
thought and prevented the so-called legal science from progressing through pragmatic field investigations.
Indetermination would involve a speculative, non-measurable, non-falsifiable and therefore non-scientific
discussion.

Jurimetrics: science and prediction in the field of law, written in 1961, 12 years after the

Loevinger criticizes the works of realists Jerome Frank and TW Arnold for resuming what he calls the
"search for the philosopher's stone", that is, a discussion about the essence of law. Despite opting for a
new answer - the essence of law is indeterminate and its sources are hidden - realists tried to answer an
old question, and ended up falling into the same conceptual trap as their predecessors, whose answers,
old or new, are all the result of "armchair speculation". Legal realism would be a revamped speculative
approach, with new clothes and a According to Loevinger, realists should not speculate around the
indeterminacy of law. The correct scientific stance would be to advance in field research and objectively
investigate the law in all its dimensions, measuring the behavior of agents
(legislators, witnesses, parties and judges).

Loevinger did not realize that randomness is a precondition for the application of statistics in the study of
judicial decisions and that the recognition of uncertainty in Law does not compromise the construction of
consistent and useful knowledge.

publication of the first article on the subject.

modern vocabulary.

Despite continuing to define jurimetry in a generic

way, as the application of a supposed scientific method of law, Loevinger recognizes for the first and only
time that this application should take place in the branch of mathematics that is best adapted to the study
of social phenomena: statistics . Loevinger observes that the objective of statistics is to extract probability
judgments about how reality will shape itself in the future, which, moreover, would be the objective of any
and all sciences, always based on inductive studies of the regularity of observable and measurable events.
Furthermore, without realizing that the premise of the use of statistics is the randomness (and, therefore, a
controllable degree of indeterminacy) of events, he tries once again to dispel the specter of indeterminacy,
stating that the law would not be uncertain to the point of prevent its scientific study and that, if there is
any uncertainty in the legal field, it would be neither greater nor less than the uncertainty inherent to other
scientific experiments.

This radical pragmatism, which seeks to extract all knowledge from experience and which denies the
validity of any conceptual articulation, becomes unfeasible because science is also a field in which abstract
ideas are tested and developed. Without ideas, concepts and definitions it is impossible to build a science
capable of articulating data from reality with our intellects. Despite using empirical research as a means of

Loevinger's most interesting work on Jurimetry is the essay

48

50

49

51

Machine Translated by Google
53

52

54

55

VII. Developments of the idea

Hence the mention of Ludwig Wittgenstein's philosophical school and the centrality
of the computer, the only tool capable of absorbing the avalanche of judicial data and
carrying out the complicated probabilistic calculations associated with human behavior.

When publishing the collection,

The space left by Loevinger was filled by his successors. The first and foremost of
these was Hans Baade, Professor of Civil Law at the University of Texas. In 1963, when
he was a professor at Duke University, Hans Baade organized a symposium that gave
rise to a compendium of studies with contributions from several professors on the
concept of jurimetrics.

The association with electronic data and consistent logic ended up reducing
jurimetry to a branch of computer science. This diversion flowed directly into Italy and
Brazil ten years later. Relegating its realistic and statistical foundations, supporters of
jurimetrics combined computation, formal language and determinism to begin a
discussion about the possible development of programs capable of processing legal
problems in formal language and automatically rendering judicial decisions.
The initial purpose of jurimetrics, which was to measure the law and control its
uncertainty, analogous to that of econometrics, biometrics and sociometrics, was lost
among so many other references considered scientific by future generations of
jurimetricians, to whom Loevinger deposited the responsibility of defining the area.

Baade wrote a preface in which he
declared that he believed that jurimetry was the victim of a conspiratorial silence from
the academic class. The purpose of the symposium was to break this silence and
publicize the advances resulting from effervescent legal studies, based on databases
accumulated in previous years. Baade stated there that computational advances
allowed the creation of a data storage and search system capable of specifying legal
concepts and organizing millions of jurisprudential precedents into stable vectors.
For Baade, judicial decisions needed to be translated into a logical language suitable
for computer calculation. From this symposium onwards, jurimetrics became
associated with information technology and with an effort to reduce law to a language
that could be processed by electronic computers.

research, science has the ultimate goal of formulating and testing theories about the
world. It is therefore impossible to practice science without making use of abstract
ideas. If the Brazilian Society of Public Law - SBDP decides to carry out, as in fact it
did, an investigation into general repercussions for the Ministry of Justice, it will need
to articulate several legal and methodological concepts in planning its research, such
as general repercussion, jurisdiction, precedent, bylaws, sampling, population of
interest, statistical inference, among others.

Randolph Block, a professor at the University of Chicago, has summarized the
inconsistencies of what he calls the "fuzzy definition" of jurimetrics and the hodgepodge of subjects that,

Baade argued in his preface that, as a result of a computational (and not a statistical)
revolution, three areas of jurimetrics had emerged, related to the following activities:
(i) storage and retrieval of electronic data; (ii) behaviorist study of judicial decisions;
and (iii) use of symbolic logic. The explanation of what these three areas would be
combined unusual and, in some cases, epistemologically incompatible concepts, such
as computational language, philosophy of language and computer science, in an effort
to predict judicial behavior through mathematical calculations. As the explosion in the
number of texts caused by data storage could only be processed through the use of
computing, the new jurimetricians defended the reduction of the right to a modern and
sophisticated formal logic, so that decisions could be processed mathematically by
computers. .

Machine Translated by Google
56

58

59

57

VIII. Jurimetry in Brazil

It is important here that we go into detail in the criticisms. Losano defines jurimetry
as a method of applying exact and natural sciences to law through computers and
computer methods.

According to Block, the agenda of the Jurimetrics Journal, published by the University
of Minnesota, illustrates the confusion resulting from this lack of methodological
rigor. The agenda would involve three distinct areas: (1) legal logic, (2) data recovery
and (3) use of quantitative methods to predict judicial decisions, all with the purpose
of discussing the effects of technological development on law. Of the three areas,
only one is relevant. Legal logic has nothing to do with the measurement of law and,
therefore, nothing to do with jurimetry. Data recovery is an area of computing common
to any database structuring problem. Technological development involves everything
from bioethics to sharing the orbit of satellites. The only activity proper to a coherent
definition of jurimetry is the use of methods

In Brazil, jurimetry appears for the first time in 1973, in a series of lectures given
by the Italian Mario Losano, professor of philosophy at the Universities of Milan and
Turin, in São Paulo.

He also criticizes what is the most legitimate claim of
jurimetrics, with which even the most severe critics of Loevinger and Baade's
contributions agree, namely: the attempt to predict the behavior of courts. Losano,
accustomed to the idea of mechanical jurisprudence, understands that the sentence
would only be predictable if its content could be deduced from the law or from precedents. However, not

quantitative data to predict judicial decisions.

Mario Losano came to Brazil at the invitation of the then

rector of the University of São Paulo, Miguel Reale, who had been in contact with a
work published by the Italian professor, entitled Giuscibernetica. The work, focused on the study
of the relationship between information technology and law, caused some impact in
the Italian legal community and, due to the historical links between the Faculty of Law
of the University of São Paulo and the study centers of that country, soon arrived in Brazil .

Loevinger's "indefinite definition" allowed this confusion between, on the one
hand, a new methodology for investigating the concrete plane of law and, on the
other, strange elements (such as logic and technological developments) or accidental
(such as computation and retrieval). of data), which do not clarify anything about the
content and usefulness of this new methodology. Stating that jurimetrics is a
discipline that uses computers to understand law is the same as defining civil
engineering as the use of calculators to construct buildings. In both definitions,
excessive emphasis is given to a mere working tool and the set of methods and
techniques used to operate the two types of knowledge is neglected. Like the
calculator, the computer is just an inert tool that only knows how to perform
calculations ordered by the operator.

since the publication of Hans Baade's work, have been associated with this field of study.

Losano's attempt to associate cybernetics, law and technology adopts as a starting
point the need to overcome Jurimetrics. Based on the areas of application defined by
Hans Baade (analysis of judicial behavior, retrieval of legal databases and use of
legal logic), Losano claims that Loevinger's refusal to define Jurimetrics stemmed
from the heterogeneity of these three aspects and that the The only thing in common
between them was the use of the computer. In addition, the Italian scholar also did
not agree with the idea of quantifying law and rejected statistics for considering it
incompatible with law in all its scope, including the study of general norms, principles
and social values.

Machine Translated by Google
63

61

64

65

62

60

Assuming this link with computer science, for Losano jurimetry would be just an
initial historical phase already overcome of what he called juscybernetics.
Cibernétes (from the Greek ÿÿÿÿÿÿÿÿÿÿ) is a Greek expression that designated the
command of a vessel. In 1951, Norbert Wiener (1894-1964) created the term
cybernetics to refer to the study of the laws that govern the communication and
control of machines and animals.

Mario Losano's work has the merit of being a pioneer in tackling the concept and
proposals of jurimetrics in Italy and Brazil and, exactly for this reason, it deserves to
have some of its points rediscussed with greater care. There are four of Losano's
criticisms of jurimetrics, an expression that he suggests reserving to refer to an
initial phase of the application of information technology to law, located between 1949 and 1969.

information are called "cybernetic" or "adaptive" systems.

First, Losano believed that the use of statistics had been abandoned "due to the
lack of flexibility of mathematical instruments".

This concept was later re-elaborated to refer to the study of systems with
the capacity to monitor how the environment reacts to their actions and adapt their
action strategies to these reactions. These systems can coincide with an animal
population within an evolutionary competition, a company operating in a market or
even a robot that is assigned a domestic task. In common, the animal population,
company and robot share the quality of being systems in search of achieving certain
objectives and which have the ability to evaluate, through information received from
their environment (number of deaths, prices, owner satisfaction, etc.) whether or not
your strategies are contributing to the achievement of the objective. The information
received about how the environment reacted to your action is the feedback and the systems capable of

logical corollary of the law, given that the parameters that interfere with its
conformation are not formalizable, predicting the content of a sentence would be
unfeasible. Losano claims that the scope of discretion given to the judge to decide a
case creates a voluntaristic element capable of preventing any prediction. Thus, it
would be possible to program a computer to play a game of chess, which has fixed
rules, but not to predict the behavior of a judge, who responds to non-formalizable
rules.

The aim of juscybernetics would be to create a theory that explains the
communications and controls involved in the entire legal system, from the abstract
plane, in which law is seen as a social subsystem, to the concrete plane of its
application by the courts. According to Losano, juscibernetics would have four fields
of research: 1) Law as a subsystem of the social system. 2) Law as a normative,
dynamic and self-regulating system. 3) Formalization of legal language to adapt it to
the computer. 4) Treatment of legal norms as information accessible to computers.
The first two fields would comprise cybernetic modeling (theoretical field) and the
last two would comprise legal informatics (empirical field), based on the use of
computers and the technological treatment of legal information.

There is a confusion here
between mathematics and statistics. Mathematics is a unique body of axioms and
theorems. Statistics is a set of methods aimed at collecting, organizing and
interpreting data. Statistics uses mathematics, mainly probability theory, but is not
limited to it, encompassing other practical methods aimed at, for example, collecting
data or visualizing results. Its flexibility comes from the fact that it was created to
deal with situations in which the analytical rigidity of determinism proved impotent.
Furthermore, the criticism disregards the dissemination of statistical methodology
across the most diverse branches of knowledge such as econometrics, sociometrics,
statistical geography, demography and biometrics.

Machine Translated by Google
69

1

66

68

67

FOOTNOTES

A discipline depends on a

powerful methodology, capable of transforming this data into useful information for
decision-making. The jurimetrics methodology could not be the use of a computer, for the
simple fact that a computer is an object, not a method. The methodology of jurimetry is
statistics. how econometrics and
biometrics were born from the conjunction between an object of interest and statistical
methodology. The association of measurement in the right to computer use, and not
statistics, is a confusion that can be explained by the fact that the "reality" of jurimetrics
is largely documented within databases.

The computer is a tool and the data

is the input for jurimetrics.

Third, Losano states that the main value of juscybernetics would be in understanding
how the law could process informational feedback from society to improve its action
strategies. The question that remains is: but what would this feedback be, if not an
extensive database with judicial decisions, contracts, complaints, requests and resources
that can be investigated using statistical methods? There is no denying that a powerful
way of evaluating the impact of legislation is to measure the behavior of judges and
parties, before and after its entry into force, in order to isolate the concrete effects of a
public policy. Information in a computational society is always aggregated in databases,
the exploration of which is the essence of statistics.

Fourth, Losano criticizes jurimetrics for not explaining the entirety of law, leaving aside
the abstract plane of values without being able, on the other hand, to develop a model
capable of accurately predicting the behavior of subjects on a concrete plane. The mistake
here is twofold. Jurimetrics does not take abstractly considered values as its object for
the simple fact that nothing that is not concrete can be measured. Measurement
presupposes the existence of an object with extension, located in time and space.
Furthermore, jurimetrics, at least in the definition presented here, does not propose to
create models capable of accurately predetermining the behavior of the parties or the
meaning of judicial decisions. The prediction proposed by jurimetrics has a probabilistic
and non-deterministic character.

Overcoming these misconceptions and recovering the concept of jurimetrics are the
objectives of the next two chapters.

It is important to clarify that I am not denying here the importance for jurimetrics of the
accumulation of extensive databases and the development of computers capable of
processing the analysis of this information.

Second, Losano falls into Loevinger and Baade's mistake when defining jurimetrics as
the application of information technology to law. A discipline is not built through the
accumulation of databases and keyword search systems.

Furthermore, all other "metric sciences"

The problem arises when these undoubtedly

important technological advances begin to be taken as the very essence of the discipline.
The technological computing tools, widely used by Jurimetrics, should not be confused
with its methodology, especially because computers can operate with languages other
than statistics and statistical tests can be carried out without the intermediation of a
computer.

Machine Translated by Google
8
6
4
two

9
7
5
3

10

Art. 103-A of CF/1988.

Harvard Law Review. v. 10. The path of the law, 1897, p. 208.

Arts. 102, I, and 105, I, of CF/1988.

Nature Magazine Report : Hungry Judges Dispense Rough Justice. Available from: [www.scientificamerican.com/

article.cfm?id="hungry-judges-dispense-rough-justice]." Access

Art. 105 of CF/1988 and arts. 541 to 545 of the CPC.

On the modern applications of Statistics in Law, especially in the production of expert evidence, see: Kadane, Joseph

B. Statistics in the Law. New York: Oxford University Press, 2008.

Art. 546, I and II, of the CPC.

on: 7/30/2013.

Art. 103, II, of CF/1988.

In the original: "For the rational study of the law the blackletter man may be man of the present, but the man of the

future is the man of statistics and the master of economics" HOLMES, Oliver W.

Arts. 476 to 479 of the CPC.

Arts. 530 to 534 of the CPC.

Machine Translated by Google
The current condition of art. 121 of the Civil Code - CC.

London: Cambridge, 1986, p. 63-70.

The law of large numbers is a theorem that describes the result of performing an experiment a large

number of times. According to the law, the average of the values obtained in a large number of

experiments will tend to approach its expected value. The classic example is the fair dice, in which each

of the six faces has an equal probability of appearing with each throw. The expected value for the mean

of the results is 1+2+3+4+5+6= 21 ÷ 6 = 3.5. It is possible that, in the first three throws, the number 5

appears twice and the 6 once, with an average equal to 5.33; but after 1,000 throws, the average value

of the results is very close to 3.5, which is the expected value.

The full title is: Ars Cogitandi, sive scientia cogitationum cogitantum, cogitationibus necessarii instructa

et a peregrinis liberata (1702) by Gottlieb Gerhard Titius.

Hacking, Ian. The emergency of probability. New York: Cambridge University Press, 1975, p. 85-91.

Bernoulli, Jacob. The art of conjecturing. Together with letter to a friend on sets in court tennis.

Leibniz, Gottfried Wilhelm. From Conditionibus. Paris: Librarie philosophique J. VRIN, 2002, p. 9-66

(Introduction).

Hald, Anders. A history of statistics and probability and their application before 1750. New Jersey: John

Wiley and Sons, 2003, p. 375-396. See also the historical introduction to Bernoulli's edition, Jacob. The

art of conjecturing. Together with letters to a friend on sets in court tennis. Baltimore: The John Hopkins

University Press, 2006. (Introduction)

Baltimore: The John Hopkins University Press, 2006.

Heide, C. & Seneta, E. Statistics of the centuries. New York: Springer-Verlag, 2001, p. 421-489.

See also Stigler, S. The history of statistics: The measurement of uncertainty before 1900.

12

11

14

13

16

15

17

Machine Translated by Google
Grant Gilmore divides the legal thought of that country into three periods. The "Age of Discovery", which runs from the beginning of the

legal system until the Civil War, the "Age of Formalism", which runs from the end of the Civil War until 1920, and the "Age of Realism",

which begins in 1920 until the current.

"To conjecture about something is to measure its probability. The Art of Conjecturing or the Stochastic Art is therefore defined as the art

of measuring as exactly as possible the probabilities of things so that in our judgments and actions we can always choose or follow that

which seems to be better, more satisfactory, safer and more considered. In this alone consists all the wisdom of the Philosopher and the

prudence of the Statesman." Quoted by Hald, Anders. A history of statistics and probability and their application before 1750. New Jersey:

John Wiley and Sons, 2003, p. 220.

Friedman, Lawrence M. American law in the 20th century. New Haven: Yale University Press, 2001, Ll.6973.

In the original: ""The prophecies of what the courts will do in fact, and nothing more pretentious, are what I mean by the Law."" HOLMES,

Oliver W. The path of the law. Harvard Law Review, vol. 10, 1897. p. 208.

19

There are several bibliographical sources on the life of Oliver Wendell Holmes, Jr., including a Broadway show, The magnificent Yankee,

released in 1951. For an overview: White, G. Edward. Patterns of American legal thought. New Orleans: Quid Pro Law Books, 2010,

Friedman, Lawrence M. A history of American law. 3rd ed., New York: Simon & Schuster, 2005.

21

"Very likely it may be that with all the help that statistics and every modern appliance can bring us there will never be a commonwealth in

which science is everywhere supreme. But it is an ideal, and without ideals what is life worth?" Holmes, Oliver W. Law in science and

science in law. Harvard Law Review, v.12, n. 7, 1899, p. 231.

Gilmore, Grant. The ages of American law. New Haven: Yale University Press, 1977.

Part III: American law to the close of 19th century and Part IV: The 20th century, and Friedman, Lawrence M. American law in the 20th

century. New Haven: Yale University Press, 2001. Part I: The old order.

23

18

22

20

Machine Translated by Google
For biographical details, see introduction to: Pound, Roscoe. Criminal justice in America, with

a new introduction by Ron Christenson. New Jersey: Transaction Publishers, 1998.

See chapter on the "Age of Realism" in Gilmore, Grant. The ages of American law. New Haven:
Yale University Press, 1977. For a divergent view, which considers the opposition between
formalists and realists to be artificial, see: Tamanaha, Brian Z. Beyond the formalist-realist
divide. New Jersey: Princeton University Press, 2010.

As with the followers of most social movements with wide repercussions, realists identify

themselves more by what they fought than by the positive proposals they formulated. Explains

Lawrence Friedman: "It is not easy to say what legal realism consisted of; much easier to say

what it was not. It was not formalist, and it rejected the cold, deductive style of CC Langdell.

The "realists" were a variegated bunch. What they had in common was the idea that the law

needed fixing; it was out of step with reality - limping behind society, in a changing world. Legal

realism was of a piece, perhaps, with other strands of American social thought: skeptical,

inclined to look for social explanations and for social situations, critical of old

orthodoxies."Friedman, Lawrence M. American law in the 20th century. New Haven: Yale
University Press, 2002, p. 6845-6847. The very division between realists and formalists has been
criticized. Brian Tamanaha, for example, conducted extensive research into what authors
usually labeled formalists actually say and, based on a vast comparison of citations, concluded
that there is no such striking difference in positions.

Cardozo, Benjamin N. The nature of judicial process. New Orleans: Quid Pro Law Books, 2010.

Roolenberg, Richard. The World of Benjamin Cardozo. Cambridge: Harvard University Press,
1997.

The last one, authored by one of the most influential jurists of our time: Posner, Richard. How

judges think. Cambridge: Harvard University Press, 2008.

Pound, N. Roscoe. Mechanical jurisprudence. Columbia Law Review, vol. 8, 1908.

25

26

24

29

30

28

27

Machine Translated by Google
35

31

Brian Leiter explains how the idea of law's indeterminacy and irrationality led realists to disregard efforts to systematically

study the abstract plane of Law: "The realists

Sociology of Law. Sociology of fundamental Law: addresses general aspects of the functioning of Law. You can specialize in

large areas, such as criminal, commercial, etc.

Directive sentences are distinguished from assertion sentences, which describe a state of affairs, and exclamation sentences,

which express a state of mood. The law is made up of directive sentences, but the doctrine uses assertion sentences to

describe legal directives. Ross, Alf. On law and justice, New Jersey: The Lawbook Exchange, 2004. p. 38-51.

32

Applied Sociology of Law: conforms to practical problems. Ross, Alf. On law and justice, New Jersey: The Lawbook Exchange,

2004. p. 23.

34

Friedman, Lawrence M. American law in the 20th century. New Haven: Yale University Press, 2001, Ll. 6882.

Doctrinal study can be divided into stricto sensu, that is, History of Law and Comparative Law; The Sociology of Law can be

divided into fundamental and applied. Doctrinal study in the strict sense: studies a specific system in the present. Historical

doctrinal study: studies a specific system in the past. Comparative doctrinal study: studies the comparison of different systems

of the past and present. Ross, Alf. On law and justice, New Jersey: The Lawbook Exchange, 2004. p. 19-23.

36

how much the authors classified as realists want to make us believe. Tamanaha, Brian Z., Beyond the formalist-realist divide:

the role of politics in judging. Princeton and Oxford: Princeton University Press, 2010. See also Rubin, Edward. The real

formalists, the real realists and what they tell us about judicial decision making. Michigan Law Review, vol. 109, p. 863-882.

"a national law system, considered as a valid system of norms, can accordingly be defined as the norms which actually are

operative in the mind of the judge, became they are felt by him to be socially binding and therefore obeyed." Ross, Alf. On law

and justice, New Jersey: The Lawbook Exchange, 2004. p. 35.

33

Machine Translated by Google
39

37

38

42

41

40

Frank, Jerome. The law and the modern mind. With a new introduction by Brian H. Bix. New
Jersey: Transaction Publishers, 2008.

famously argued that the law was "indeterminate". By this, they meant two things: first, that
the law was rationally indeterminate, in the sense that the available class of legal reasons
did not justify a unique decision (at least in those cases that reached the stage of appellate
review); but second, that the law was also causally and explanatorily indeterminate, in the
sense that legal reasons did not suffice to explain why judges decided as they did. Causal
indeterminacy implies rational indeterminacy on the assumption that judges are responsive
to applicable (justificatory) legal reasons." LEITER, Brian. American legal realism. Public law
and legal theory research paper n. 2,2002. p. 4. Accessible at ssrn. com/abstract_id="339562." Accessed on 7/8/2012.

In Portuguese: "pretty playthings". Llewellyn, Karl: The bramble bush lectures: the classic
lectures on the Law and Law Schools. Oxford: Oxford University Press, 2008.

The distinction between Public Law and Private Law goes back to Ulpian's comments on the
Institutes of Gaius and Justinian, which states: Par. 4º. Hujus studii duae sunt positiones,
publicum et privatum. Publicum jus est, quod ad statum rei Romanae spectat, privatum,
quod ad singulorum utilitatem pertinent. In Portuguese: "For this study there are two
positions, public and private. Law is public when the Roman state of affairs is at stake, and
it is private when it deals with private interests." There are controversies about the accuracy
of this distinction. Mears, T Lambert. The institutes of Gaius and Justinian, the twelve tables,
and the CXVIIIth and CXXVIIth novels, with introduction and translations. London: Stevens and Sons, 1882, p. 3.

Information extracted from Lee Loevinger's obituary available at: www.pdfio.com/k
1626007.html#. Accessed on 7/8/2012.

Oliphant, Herman. A study of day calendars. New York: Johns Hopkins University Press, 1932.

Moore, Underhill. & Hope, Theodore. An institutional approach to the Law of commercial
banking. Yale Law Journal, v. 38, 1929.

Machine Translated by Google
As in any pragmatic discipline, the definition will be given by the activities of its practitioners,
and will undoubtedly change and expand as experiment and experience give answers to
specific questions. The distinction between jurisprudence and jurimetrics is already evident.

Loevinger, Lee. An introduction to Legal Logic. Indiana Law Journal, v. 27, 1952, p. 471-522.

After disdaining the talk around which the philosophy of Law would revolve, Loevinger pays
a significant tribute to the realists, to whom he refers in capital letters, who would have
removed Law from a celestial plane, transforming it from a supernatural superstition into an
object real, capable of being known by men: "Some service has indeed been rendered by
the modern thinkers, Benthan, Jhering, Holmes, Pound, the Realists, and others of similar

Jurisprudence is concerned with such matters as the nature and sources of the law, the
formal bases of law, the province and function of law, the ends of law and the analysis of
general juristic concepts. Jurimetrics is concerned with such matters as the quantitative
analysis of judicial behavior, the application of communication and information theory to
legal expression, the use of mathematical logic in law, the retrieval of legal data by electronic
and mechanical means, and the formulation of a calculus of legal predictability. Jurisprudence
is primarily an undertaking of rationalism; jurimetrics is an effort to utilize scientific methods
in the field of law. The conclusions of jurisprudence are merely debatable; the conclusions of jurimetrics are testable.

Loevinger, Lee. Jurimetrics: the next step forward. Minnesota Law Review, v. 33, 1949.

"Of course it is not important what term is used to indicate the scientific discipline
suggested. It is important that it has a distinctive name, as well as a general program. The
name suggested here, seems to the author, as good as any, since it seems to indicate the
nature of the subject matter, and corresponds to other similar terms, such as biometrics
and econometrics." Loevinger, Lee. Jurimetrics: the next step forward. Minnesota Law Review, v. 33, 1949. Note

Jurisprudence cogitates essences and ends and values. Jurimetrics investigates methods
of inquiry." Loevinger, Lee. Jurimetrics: the methodology of legal inquiry. Law and
contemporary problems, v. 28, 1963, p. 8.

"It is unnecessary, and perhaps impossible, to give a precise definition to the field of jurimetrics.

Llewenllyn, Karl N. A realistic jurisprudence: the next step. Columbia Law Review, v. 30, 1930.

44

43

45

48

47

46

Machine Translated by Google
51

49

52

53

50

Baade, Hans. Jurimetrics. New York and London: Basic Books, 1963.

Loevinger, Lee. Jurimetrics: the next step forward. Minnesota Law Review, V. 33, 1949, p.13.

volume no. 40 of the Thinking about Law Series, "General repercussion and the Brazilian
system of precedents": http://portal.mj.gov.br/main.asp?View="{329D6EB2-8AB0-4606-B054-
4CAD3C53EE73}" Access on July 30th.

"Frank insists that uncertainty is inherent in the legal process, and that the grasping for
certainty in general principles is simply an expression of infantile emotional attitudes which
had persisted into adulthood. Arnold finds the explanation of the inconsistencies and
absurdities in the fact that all our social institutions are mere symbols of our dreams and
aspirations. But all this is merely a continuation of the ancient quest for the philosopher's
stone. The new school seeks it in some scientific, rather than some moral explanation or
principle, but the fallacy is the same . This is simply a new jurisprudence with a new
vocabulary. The argument seeks to substitute a modern analysis for an ancient one, but the
traditional techniques are still in use. It is all armchair speculation." Loevinger, Lee.
Jurimetrics: the next step forward. Minnesota Law Review, v. 33, 1949, p. 18.

"The branch of mathematics that appears to be the most immediate practical utility in the
fields of law and the behavioral sciences is statistics. There is much in statistics that is of
present practical application in day-to-day legal problems and it has good claim to be included
in every law school curriculum.

The two conditions for the use of statistical methods are (1) that we be dealing with numerical
data and (2) that we be dealing with a universe of which we have either a complete census or
a representative sample." Loevinger, Lee. Jurimetrics : science and prediction in the field of law.
Minnesota Law Review, v. 46, 1961, p. 262.

views, in bringing law out of the sky and down to earth. (...) Their combined effect has been to

change law from a supernatural superstition to a human institution. The subject which could
formerly be known only by mystical intuition may now, at least, be studied by mundane minds."

Loevinger, Lee. Jurimetrics: science and prediction in the field of law. Minnesota Law Review,
V. 46, 1961, p. 255.

Machine Translated by Google
The uses of quantitative methods for the analysis of decision making; and

The collection was first published as v. 28 of the magazine Law and Contemporary Problems of 1963.

This magazine was later republished in the form of a book, under the title Jurimetrics, by Basic Books, in

that same year.

This remarkably indefinite definition, though it has been useful in the past, has become less a demarcation

of an area of endeavor than a reflection of the interests of a particular constituency.

55

"The term jurimetrics was introduced into the legal vocabulary by Lee Loevinger about fifteen years ago.

It signifies the scientific investigation of legal problems. Although this field is as vast as law itself,

jurimetrics research has up to now concentrated on three areas: electronic data storage and retrieval;

behavioral analysis of decisions; and the use of symbolic logic. To some extent, each of these areas is

independent of the others. Electronic data processing is a response to what might be termed the source

material explosion - a proliferation of textual material which has been said to present scholars with the

choice to be reading or writing. Behavioral research of the decision process reflects the growing self

confidence of American social scientism. The use of modern, sophisticated logical methods in the analysis

of legal problems can be traced to the spread of a new school of formalistic philosophy, commonly

associated with Ludwig Wittgenstein.Nevertheless, the three areas of jurimetrics are closely

interconnected; they all are, at least for present practical purposes, products of the "computer revolution".

Only the electronic computer, it seems, can cope with the continuing avalanche of relevant source

materials; only the computer, again, can efficiently undertake the complicated calculations required for

behavioral probability analysis. And the computer will not digest anything that cannot be dissected with

logical consistency." Baade, Hans. Jurimetrics. New York and London: Basic Books, 1963, p. 1. See

Preface.

The contests of any volume of the Jurimetrics Journal manifest the consequences of this indefinitiveness.

Indeed, its preamble sets out four distinct domains within jurimetrics:

The uses of modern logic in law;

The relationship between (a) developments in science and technology and (b) law.

56

The first of these areas is an application of logic: it has nothing to do with measuring anything, and

therefore is a misapplication of the term "jurimetrics". The second has nothing to do with "scientific

investigation" or measurement it deals with the use of computer technology in law for any number of

purposes. The four of these areas is a fan's classification, the inclusion of which

54

The uses of modern methods of information retrieval in law;

"Jurimetrics, we have been told, is a field devoted to the scientific investigation of legal problems.

Machine Translated by Google
Jurimetry is, therefore, the application of the method of the exact and natural sciences to Law: not, however,

in an abstract way, but through the use of the computer. Since Law expressed in natural language cannot

be treated directly with a computer tool, the 'use of scientific methods in Law' imposes, above all, the use of

methods and instruments made available to the (then) young computer science." Losano, Mario G. System

and structure in Law: from the 20th century to postmodernity, v. 3, São Paulo: WMF Martins Fontes, 2011,

p. 56-57. See also Losano, Mario G. Informatica per le scienze sociali. Torino: Einaudi, 1985.

Accordingly, jurimetrics is compared with econometrics and psychometrics in that each uses the method of

quantitative analysis to look for patterns in human behavior." Block, Randolph. Book review: Supreme Court

Policu Making: Explanation and Prediction. (by Harold J. Spaeth) .

58

American Bar Foundation Research Journal, v. 3, 1980, p. 618.

57

Translated into Portuguese: Losano, Mario G. Legal informatics. São Paulo: Saraiva, 1976.

59

60

Losano's lectures on juscibernetics took place in 1973 at the Faculty of Law of the University of São Paulo

and led to the creation of the discipline Legal Informatics, taught by Dinio de Santis Garcia, with the

assistance of Pedro Luiz Ricardo Gagliardi, the latter retired judge and Director of the School Paulista da

Magistratura. The subject was closed, but Pedro Luiz Ricardo Gagliardi recovered the idea of Jurimetry in

the swearing-in ceremony of the Board of the Escola Paulista da Magistratura for the biennium 2010/2012.

Accessible speech at www.epm.sp.gov.br/Internas/ArtigosView.aspx?ID="5039." Accessed on 07/08/2012.

See also Garcia, Dinio de Santis. Introduction to legal informatics. São Paulo: José Bushatsky, 1976, p.

7-11.

"In both systems ['Common Law' and 'Civil Law'], the judge has a range of discretion that depends on his

assessment, that is, on his will. It is precisely this voluntarist element that led the last Kelsen to review his

theory on the judge's activity, denying that it follows a deductive scheme of a logical type: the will is irrational.

Piero Calamandrei, sensible prince of Italian proceduralists, shared, in 1930, a logical-mechanical conception

of legal reasoning and saw 'the sentence as a progression of chain syllogisms'; twenty years later, the

has caused the Jurimetrics Journal to be something of a Popular Science for lawyers. That leaves the third

domain as the only legitimate claimant among the four to the title of jurimetrics.

"Jurimetrics is a pragmatic method for the use of the first computers.

See also the article by Ivan César Ribeiro, who in 2008 mentions the existence of "analyses and jurimetric

models" in valuations for provision of companies: Ribeiro, Ivan César, Good governance, provisions and

contingencies. Valor Econômico newspaper. São Paulo, p. E 3, March 26, 2008.

Machine Translated by Google
Losano, Mario G. Legal informatics. São Paulo: Saraiva, 1976, p. 14-15.

Massachusetts: MIT Press, 1965.

Some interesting facts about Norbert Wiener's life deserve to be mentioned. Norbert was an

American mathematician of the century. XX. Educated at home by his father, Leo Weiner, a

Polish Jew who emigrated to the United States, Norbert proved to be a child prodigy. He

graduated from school at 11 and got his degree in mathematics at 14. He went to Harvard where

he studied biology and mathematics, obtaining his Ph.D at the age of 17, under the guidance of Karl Schmidt.

He earned his living as a professor in the mathematics department at the Massachusetts Institute

of Technology - MIT, which since 1967 has offered the Norbert Wiener Prize in Applied Mathematics.

Steinbruner, John D. The cybernetic theory of decision: new dimensions of political analysis.
New Jersey: Princeton University Press. 1974. p. 47-87.

Losano, Mario G. Legal informatics. São Paulo: Saraiva, 1976, p. 76. A summary of Mario
Losano's proposal can also be found in: Pimentel, Alendre Freire. Cyber law, a theoretical and
logical application approach.Rio de Janeiro: Renovar, 2000, p. 166-206.

also at the expense of a long experience as a lawyer, he asked himself: 'Is it really that, in the

system of legality, the judge's sentence is surely predictable?; Let's say it in secret, among us

lawyers who can predict in advance the success of a case?' And he concluded by resigningly

accepting the Roman proverb habent sua sidera lites: "a just cause is also lost because the
stars are unfavorable". With the current improvements in computers and programs, it is possible
to program the computer to play a good game of chess, as the rules of the game are fixed;
however, the computer itself cannot predict a judge's behavior, as legal rules are interpretable
according to parameters that today's techniques cannot formalize." Losano, Mario G. System
and structure in law: from the 20th century to post modernity , Vol . _

Wiener, Norbert. Cybernetics: or control and communication in the animal and the machine.

64

62

61

66

63

65

Machine Translated by Google
Instructors are urged, however, to use computers in the course as much as is feasible. A small

(...)

Loevinger himself understood that the computer was not the key to the success of a new approach,

but the models to be used: "It is neither reasonable nor realistic to expect the invention of a machine

that will do for us, only more rapidly and with less human effort, the same thing that we have been

doing inefficiently for ourselves.Rather, what science offer us is tools that will allow the same things

to be done in a new way or things to be done that could not be attempted previously. (...) The most

useful and significant tools that science now offers to law are intellectual rather than mechanical."

Loevinger, Lee. Jurimetrics: science and prediction in the field of law. Minnesota Law Review, vol. 46,

p. 5.

In addition, developments in computer hardware and software have dramatically increased researchers

access to the necessary computational power. Desktop (even a lap top) computers now possess

computational capacities that are more than ample to perform functions that less than one generation

earlier required large, expensive mainframe computers. Fully exploiting exploding computation

capacity of computers, software developers have made statistical software programs for personal

computers that can manage many of the most sophisticated analysis."Heise, Michael. The past,

present, and future of empirical legal scholarship: judicial decision making and the new empiricism.

University of Illinois Law Review, v. 4, 2002, pp. 829-831.

"The development of data is critical, since without data, empirical research is not possible.

Losano believed that Jurimetria was limited to search programs for decisions by keyword: "Therefore,

with Jurimetria we are still on the threshold of legal informatics: informatics is applied to the document

that contains the Law, but it is not taken into the Law. ". Losano, Mario G. System and structure in

Law: of the century. XX to postmodernity, V. 3, São Paulo: WMF Martins Fontes, 2011, p. 64.

For the avoidance of doubt, Morris H. DeGroot, right in the preface of what is the main introductory

course in Statistics and Probability, gives the following recommendation to students: "Although a

computer can be a valuable adjunct in a course in probability and statistics such as this one, none of

the exercises in this book require access to a computer or knowledge of programming.For this reason,

the use of this book is not tied to a computer in any way.

Unfortunately, data gathering is frequently labor-intensive and time-consuming and, consequently,

often quite expensive. While literally billions of research dollars in the United States are directed

towards all sorts of research projects, only a minute silver of this research pie flows into legal research

projects of any kind, let alone empirical ones. Such research funding is crucial for the developments

of new datasets, the backbone of our knowledge base.

69

68

67

Machine Translated by Google
calculator is a useful aid for solving some numerical exercises in the second half of the book."
DeGroot, Morris H. Probability and statistics. Massachusetts: Addison-Wesley Publishing
Co, 1989. p. 4.

© of this issue [2016]

Machine Translated by Google
two

Jurimetry CHAPTER

5. CONCEPT OF JURIMETRY

I. Definition of Jurimetry

Chapter 5. Concept of Jurimetry

"In order to indicate how a good reform of the judiciary would be possible, it would be necessary to
have knowledge that I do not have, that is, to know the number of cases in each branch, the place of
each branch and each court, the number of judges in the courts, what schedule they perform and the
distribution of their achievements. This is a subject that must be studied in depth. It is not possible to appeal

The definition of a new area of knowledge is problematic. The difficulties stem from the
accommodation under the same conceptual roof of a not always homogeneous set of experiments,
methodologies and results witnessed for the first time.
This difficulty is further increased when this area of knowledge uses different methodologies and
references concepts from more than one discipline. Debates around the concept of econometrics are an
example of how the search for a definition of applied disciplines suffers from similar difficulties. Peter
Kennedy, econometrician, explains the difficulties faced in defining his field of research, econometrics.
The difficulty arises from the fact that econometricians wear different hats. According to Kennedy,
econometricians are economists capable of using statistical methods to test their theories; they are also
mathematicians capable of formulating theories in a language that can be statistically tested, they are
also accountants capable of collecting and storing economic data; and are statisticians capable of
estimating relationships and predicting economic events.

This issue of how certain problems are addressed in Law courses is also fundamental for
understanding Jurimetrics. When we learn law in colleges, we essentially study codes. Starting from a
mechanistic premise, Law is presented to students as a synonym of law, and the judge's activity is
described as the simple application of these general commands, predetermined in the legal system, to
concrete cases. The judge is, in the Francophile expression, the mouth of the law ("la bouche de la
lois"). The study of the concrete plane of Law, especially precedents

1

2018 - 07 - 17

Transposed to the legal universe, the difficulties are the same. Jurimetry has three operational
pillars: legal, statistical and computational. The ideal jurist would, therefore, be a law graduate capable
of speculating on the functioning of the legal order and familiar with concepts of procedural and
substantive law; a statistician capable of discussing research design and designing tests for your
working hypotheses; and a computer scientist capable of operating programs to mine and collect data.
Of course, bringing these specialties together in one person is now rare. In Brazil and in most of the
world, there are no courses capable of providing those interested with these three training courses, and
that is why the practice of Jurimetry has been developed in a laboratory environment in which bachelors,
statisticians and computer scientists join efforts in solving problems. .

to improvisation to carry out reform."

Machine Translated by Google
By taking the concrete plan as the main reference, Jurimetry requires the adoption of a
new methodology. The abstract plane of Law is composed of singular norms that are
intended to resolve alone a universe of cases to which their meaning refers. The concrete
plan is composed of thousands of norms, such as sentences, judgments or contracts,
aimed at resolving specific conflicts. The analysis of the meaning of an abstract norm can
be done through the different methodologies with which we are traditionally used to dealing
in colleges. We can interpret its meaning from a historical point of view, investigating the
social context at the time of its enactment; systematic, investigating the relationship that
the norm has with the other abstract commands of the order; grammatical, referring to the
pure semantic meaning found in dictionaries; or authentic, researching the purposes of the
author of the standard when he prepared it. To name just a few. The traditional study of
Law aims to elaborate menus of possible meanings for norms, construct definitions and
general typologies for their elements and undertake a taxonomic effort of classification.

In addition to being a tool for population description and inference, statistics also adds
other important features for the legal world. With regard to the description of populations,
statistics allow the study of the collective behavior of legal subjects (legal agents), whose
causes and characteristics, due to the discontinuity between the particular and the general,
are not confused with those of the behavior of each individual. By discontinuity we mean
the following: even though man's will is free and each person has intimate reasons for
making decisions, people's joint behavior manifests itself through regular patterns that
respond to collective causes. For example: there is no more act

Jurimetria proposes an epistemological turn, analogous to that proposed by the realists,
shifting the center of research interest from the abstract plane to the concrete plane. The
guiding concept of this turn is that the effective law, the one capable of affecting the
relationship between subjects, corresponds to the sentences, judgments, contracts and
other legal orders produced in the concrete plane. The law is a statement of intentions by
the legislator, which often proves to be plurivocal, contradictory and lacking. For jurimetry,
it is on the concrete level that the Law reveals itself, the law being just one of the factors -
alongside personal values, religion, empathy, personal life experience and many others -,
capable of influencing the process of implementing norms of Law. For this reason, the Law
cannot be reduced to a set of norms edited by competent authorities and must be seen,
yes, as an apparatus of conflict resolution, in which the law plays an important role, but not
enough.

From the moment that the central approach becomes the concrete plan, these traditional
methodologies are shown to be inadequate. It is easy to conceive of research into the
authentic interpretation, for example, of art. 50 of the CC, which deals with disregard of
legal personality. However, identifying what would have been the intention of thousands of
magistrates who, over the last ten years, delivered in the Labor Court more than one
hundred thousand decisions disregarding the legal personality is a very different task,
which requires, before any step , at least an approximate notion of what they are, when
they were made and where these hundred thousand decisions are. The most appropriate
methodology to enable the investigation of hundreds of thousands of concrete plan
decisions, allowing the description of the characteristics of these populations and enabling
inferences about possible associations and future behavior, is statistics.

jurisprudence, becomes secondary insofar as, by knowing the law, the student would
already know what the judge should do. And if the judge does not comply with the law, his
decision is nothing more than a judicial error, unworthy of academic study.

3

Machine Translated by Google
Thus, alongside the particular causes capable of explaining the motivations of each
suicide, there are collective causes that can explain suicide as a social fact.

As for the control of uncertainties in Law, statistics help in the following way: the
legal order produces uncertainties and the essence of the work of Law operators is
to try to mitigate them. Lawyers work to predict and control the outcome of the case:
they want to win. Judges work to predict and control the consequences of their
decision: they want to do justice. Lawmakers work to predict and control the outcome
of their public policy: they want to build a better society. All acts and decisions of
legal operators are taken in the present, but are aimed at the future. On the other
hand, the human will can be stimulated or repressed through positive and negative
sanctions, a possibility from which the Law operates, an institutional arrangement
whose function is to apply sanctions and motivate socially desired conduct. The
possibility of suffering the application of a custodial sentence, for example, represses
the subject's desire to commit a crime, just as the possibility of paying a fine
encourages the taxpayer to pay their taxes on time.

Having made this clarification, I can define Jurimetria as the discipline of
knowledge that uses statistical methodology to investigate the functioning of a legal
order. From it, it is clear that Jurimetrics is distinguished from other legal disciplines
both by the object and by the methodology used in its analysis.

The relationship between the stimuli created by law and the conduct observed in
society is the essence of the success of a legal order. The closer people's actual
behavior is to expected behavior (what we might call the adherence of laws to reality),
the more organized society will be and the more successful the law will be. As a
result, the use of statistics in the study of collective behavior as a function of legal
norms, either to understand how they are produced or, even, how they conform to
the reaction of the recipients when they are applied, allows not only an understanding
of the functioning of the Law , but, what is more important, it enables the creation of
models capable of bringing the results produced by the legal system closer to
society's expectations and aspirations. With this, the application of statistics to Law
provides valuable assistance so that, for example, the judge understands in greater
depth the possible consequences of his decisions, the lawyer understands the
factors that interfere with his strategy and can better advise his client, and the
legislator anticipates the results of the political proposals discussed in the legislature.

Likewise, once associations and causalities have been identified, it would be
possible to predict collective reactions to changes in the social environment using
probabilistic models. In the comparative example of suicide rates between Brazil and
Japan, it is to be expected that the growing tendency towards aging and the
urbanization of the Brazilian population also indicate a corresponding increase in
the annual rate of suicides in the country. For this reason, Statistics is the ideal
methodology for describing human behavior and evaluating how stimuli generated
through public policies can be managed in order to achieve socially desired goals.

intimate than suicide and it is correct to assume that each suicide has very personal
reasons, often unconfessable, for taking their own life. Despite this, suicide rates are
constant in Brazil (less than 6.5 per 100,000 inhabitants) and Japan (over 24 per
100,000 inhabitants), mainly due to social causes associated with the advanced age
of the population, the male majority and the high rate of urbanization.

From an objective perspective, the object of Jurimetry is not the legal norm
considered in isolation, but the legal norm articulated, on the one hand, as

Machine Translated by Google
6

4

5

7

8

9

II. Order, ordering and coordination

From a methodological perspective, Jurimetria uses statistics to re-establish an
element of causality and investigate the multiple factors (social, economic,
geographic, ethical, etc.) that influence the behavior of legal agents.

Under a consequentialist perspective, understanding the functioning of a legal
order basically consists of understanding the factors that influence the production of
norms, as well as monitoring the reaction that these norms will provoke in their
addressees. It is not, therefore, a formal study of the production of a law through the
investigation of previous procedures established in other norms, proper from an
abstract and analytical perspective. It is, on the contrary, an investigation of the
substantive factors (social, economic, geographic, cultural, values) that influenced a
certain judge or a group of legislators to, at a given moment, opt for the production
of a specific normative content, as well as as in an analysis of the behavior of
recipients against whom sanctions have been or may be applied. The object of
Jurimetry is not the norm considered in itself or in relation to other norms, but the
study of the conduct of those who regulate or are regulated by the Law, that is, human
behavior in terms of legal norms. Hence the term functioning of law. And precisely
for this reason, the legal norm is not an exclusive object of interest for Jurimetrics,
but an inflection point, which establishes the moment of a human decision, records
its meaning and demarcates important aspects of its motivation.

The object of Jurimetry is the investigation of the functioning of the legal order.
The legal order is defined here as the set of legal norms, which aims to influence
human behavior through the application of sanctions. All the legal
norms of an order are united by a formal foundation, namely, a common authority
foundation.

Because they are two defining elements of this discipline, in the topics below I will try to

But what is the legal order? The traditional study of Law emphasizes abstract
norms, laws and codes, with jurisprudence being an allegory that only serves to
illustrate cases and reinforce a theoretical position by example. This tradition also
treats the expressions "order" and "legal order" as synonymous, both referring to
the set of abstract norms of Law and relegating judicial decisions to the amorphous
and casuistic pile called jurisprudence. Jurimetrics subverts this approach and brings
the concrete norm, the courts and the law produced through the judgment of concrete
cases to the center of its interest. Jurisprudence becomes an important part of Law
and requires a methodology and a set of concepts appropriate to its study.

Both the threat of deprivation of liberty, which seeks to
discourage theft, and the threat of confiscation of property, which seeks to encourage
the payment of debts, or the threat of imposition of a sentence of confiscation, which
seeks to encourage the payment of taxes, are normative sanctions handed down by
authorities whose delegated power can be traced back to a fundamental norm.

describe in more detail the object and methodology of Jurimetry.

result (effect) of the behavior of regulators and, on the other, as a stimulus (cause) in
the behavior of their recipients. The legal norm is studied as a factor capable of
influencing the decision-making processes of judges and citizens.

The legal order is, therefore, a set of norms for regulating conduct imposed by the
State through the use of controlled institutional violence.

Machine Translated by Google
Hierarchical

Legal system

unsystematic

municipal

Legal order

Legal coordination

Legislative

An important point of Jurimetria is to understand how ordering and coordination are related. It is evident
that the legal system influences coordination through the issuing of commands from legislators to judges.
Improving this emission, making commands clearer and more effective, is one of the objectives of Law in
general and of Jurimetry in particular. This, by the way, has a relevant contribution to offer through the
investigation of the characteristics of general norms with greater adherence in coordination (the so-called
"laws that stick"). Furthermore, the study of coordination is capable of revealing circumstances that lead those
under jurisdiction to go to court and, indirectly, showing which afflictions must be regulated by the legal system,
so that the

The first redefinition concerns the very expression jurisprudence, which is associated with this idea of
alienation between scientific law and court decisions.

Abstract

jurisdictional

There are some distinctive traits that can help with this definition. The legal system is characterized by
being: abstract, as its norms do not refer to behaviors located in time and space; systematizer, for operating
an internal consistency control mechanism; and hierarchical, since the norms are organized according to an
order of superiority. Legal coordination, on the other hand, is characterized by being: concrete, as its norms
refer to situations in time and space; unsystematic, for not operating a consistency control mechanism; and
autarkic, because the norms are all on the same plane and are not organized according to an order of
superiority. Furthermore, the order usually has a legislative origin and the coordination has a judicial origin,
although there is the possibility of both powers carrying out atypical activities in which the roles are reversed.
And the legal order is the result of the sum of legal ordering and legal coordination.

Systematic

In short, the order has the function of ordering predetermined solutions for abstract conflicts, while
coordination coordinates the countless factors influencing the application of Law, including the commands
issued by the order, for the solution of concrete cases. The table below summarizes the characteristics of
each normative space:

When we talk about jurisprudence as opposed to the legal system, we have the image that Law is the legal
system while jurisprudence, subject to the mistakes and successes of judges, is just an accidental by-product
of questionable quality. Thinking of a way to reintegrate jurisprudence into scientific Law, I reserved the
expression legal order to refer to the set of all norms, individual and general, and I began to call legal
coordination the individual and concrete plan, in which all the judgments, judgments and interlocutory decisions.
The expression legal system was restricted to the general and abstract plane, in which the constitution, laws,
decrees, instructions, etc. are found.

Concrete

Machine Translated by Google
10

This preponderance, it is important to repeat, does not exclude the possibility of a
Jurimetry of general norms, analyzing, for example, the direct consequences of the
promulgation of a new legal regime on social behavior or, even, changes in interest
in the voting agenda of a legislative house. Such an analysis, however, will depend
on a realization of the general norm that, to be investigated as a cause or effect of
other influencing factors, needs to be situated in a specific place and period of time.
For example: Fundação Getúlio Vargas carried out research on 100 legislative
proposals on criminal matters that were processed in the Chamber of Deputies after
the Federal Constitution was enacted between 1988 and 2006. The object of analysis
was both the content of bills and constitutional amendments, as well as the
justifications presented.

The results are as interesting as they are alarming. Of the 579 proposed standards
of behavior , none sought to decriminalize conduct. Leaving aside 7 that promoted
mere style changes, 531 created new crimes, 39 expanded the scope of application
of already foreseen crimes and only 2 reduced this scope. Of the 891 proposed
sanction rules , no less than 837 of them created new sanctions and only 54 modified preexisting
sanctions. In addition, the research shows an increase in the severity of the
legislator in the penalties, since 509 of the 837 norms of creation proposed prison
sentences. In addition to being severe, the research showed that the legislator is
succinct in explaining his reasons. The survey identified that 38% of justifications for
legislative proposals had up to half a page and 31% between half and one and a half
pages. The same was observed even in the proposed amendments to the Federal
Constitution. Of the 23 justifications analyzed, 13 were no longer than one page. The
conclusion of the study is that the Brazilian legislator, in terms of penalties, is more
severe and laconic. When proposing reforms that restrict citizens' freedom, the
legislator fails to explain the problem sufficiently, does not inform the experiences
that served as the basis for the proposal, nor does it justify its adequacy to the
solution of the problem. The discussion of Brazilian criminal law lacks minimal
elements for the study of legislative impact and laws are enacted in a rush and based on intuitive impulses.

In an effort to understand the functioning of the legal order, Jurimetria analyzes
the factors that interfere in the production of sentences. The meaning of the laws, despite not being

As stated, Jurimetrics also (but not exclusively) focuses on the study of individual
norms, especially judicial decisions, and is sometimes confused with a type of
sociology of the courts. This preponderance stems mainly from the interest of
Jurimetry in changes in social behavior produced by the legal order, the occurrence
of which depends on some degree of effectiveness of the norms. The general norm
is, as a rule, an abstract formulation subject to divergent interpretations, whose
effectiveness depends on the mediation of a judge. It is the individual norms and the
processes from which they are issued that register the characteristics of the conflicts
and the legal solutions operated in their overcoming.

The research classified the norms into four categories:
behavior (dealing with conduct), sanction (dealing with the penalty), process (dealing
with the criminal process) and allocation (dealing with the allocation of the crime in a
regime of heinous conduct). In addition, we sought to verify the length (in pages) and
quality of the reasoning for each justification.

law is less idiosyncratic and more realistic. In short, investigate the relationship
between planning and coordination plans, learn about the reality of the Judiciary and
legal practice, study the results of the application of law by the courts, analyze the
different degrees of adherence to laws and understand the situations in which they
are no longer applied, these are the objectives of Jurimetria. In a sentence, the
proposal of Jurimetria is to understand how the legal order works in practice.

Machine Translated by Google
11

15

12

14

Traditional law tries to convince us that all norms, general (such as laws) and
individual (such as sentences) make up a consistent legal system, capable of This is,
evidently, an objective to bridge its gaps and overcome its antinomies. ambitious,
even more so if we consider, on the one hand, the profusion of general norms enacted
(laws, decrees, regulations, ordinances, normative instructions) and, on the other,
the mass of conflicts that knock on the doors of the courts. To give an idea of the
legal hypertrophy that we are experiencing, only the Brazilian National Congress has
created, among legislative decrees, resolutions, constitutional amendments, ordinary
and complementary laws on the initiative of the Chamber of Deputies, the Federal Senate, the Superior

13 2010.

In order to systematize this profusion of norms, the legal system adopts one based
on the criteria of hierarchy, specialty
and precedence. Once two antagonistic norms have been identified, this mechanism
would allow the antagonism to be overcome and indicate which should prevail: the
later one in relation to the previous one; the special in relation to the generic; and the upper in relation

consistency control mechanism

Jurimetrics seeks to observe human behavior according to a legal order.
Observation, in a broad sense, characterizes research as empirical, because the

Thus, in the event of a contradiction between general norms, there would be ways to
identify which one should prevail and thus find a unique solution for each conflict.

The mechanism, however, is not enough to extirpate any vestige of uncertainty
from the Law. On the one hand, the criteria for overcoming antinomies can also
collide with each other. There are several cases of general later norms that conflict
with special earlier norms. So which would prevail? Posterity or specialty? On the
other hand, there is also the possibility of contradiction between the meanings of the
same norm. There are no criteria for overcoming hermeneutical divergences regarding
the same article of law, which also opens space for the emergence of divergent
doctrinal and jurisprudential currents regarding the application of each norm. It is for
this reason that Jurimetry is stochastic. Instead of seeking to find univocity in the
legal order, Jurimetrics admits that Law is contradictory and begins to study the
relationships between legal uncertainty and people's behavior.

Republic and the Attorney General's Office, 3 general norms per working day in
2008, against 93,076 new cases per working day received by the Brazilian Judiciary in

Methodology is a coordinated set of methods used to solve theoretical or practical
problems. The jurimetrics methodology corresponds to a set of statistical methods
capable of gathering objective information about the functioning of a legal order, as
well as making predictions regarding its future behavior. Through this set of methods,
the researcher is able to collect, describe, summarize and critically analyze the
production of standards, as well as anticipate their consequences.

determinant, is an important factor influencing the judge's decision. The discussion
about a conflict goes through the investigation of the possible solutions previously
established in the law. Usually, however, the law includes more than one solution,
either because of the existence of more than one rule applicable to the case, or
because of the plurivocity of meanings of each general rule. The final solution,
therefore, goes through a process of volitional confirmation by the judge, who can
interfere, change or even reject the solutions indicated by the law when issuing a concrete decision for

III. Methodology, empiricism and Jurimetry

Machine Translated by Google
16

17

18

However,

observation also has a strict meaning. An object can be empirically investigated
through two types of study: observational or experimental. Observational research is
one in which the researcher has no control over the composition of the study groups,
limiting himself to observing the arrangements generated spontaneously in the
population.

The fundamental difference between the jurimetric and dogmatic approaches is the
use of empirical methodology. In Jurimetrics, a researcher presents his assumption
about some characteristic of the legal order and then goes into the field to collect data
to confirm or reject it. For example, one could test the claim that medical malpractice
lawsuits in Brazil have grown 140% in the Supreme Court of Justice in the last 4 years.
Testing this assumption depends on collecting information about this type of
conflict in the courts, which can be done through the extraction and summarization of
data relating to these actions. In Jurimetry, it is the confrontation between empirical
data and the assumption of the researcher that gives rise to knowledge.

independent of the effects of other variables.
Experimental research is research in which the researcher controls the
composition of groups with the aim of isolating the effects produced by a variable

The dogmatic approach does not require the empirical step. As his object of study
is the meaning of legal texts, the researcher builds his thesis through the elaboration
of a rhetorical speech, which seeks to attribute a specific meaning to these norms.
This discourse can be logical or political. As logical discourse has narrow limitations,
in particular the impossibility of overcoming the multiple meanings of each norm, the
dogmatic researcher almost always resorts to political expedients to attribute the
meaning of his preference to the norm, thus operating a reversal of procedure. Instead
of testing his assumption by comparing it with data from reality, the researcher
assumes that his intuition is true and starts to defend it rhetorically, through doctrinal
citations and intentionally selected jurisprudential positions. Dogmatic research
dispenses with the systematic verification of data about reality and uses only the
collections of legal libraries in the construction of a rhetorical discourse capable of
supporting a position.

For example, the study of the abortifacient effects of a medicine on a group of
women is observational because the researcher cannot control which women will or
will not consume the product. It simply observes the history of a group of women who
spontaneously took the medication and compares it with the group of women who did not take it.

19
years.

On the other hand, a study in which two groups of mice are randomly selected and
each of them is given the same medication is experimental because the researcher
controls which mice will receive the medication in order to try to isolate its effects
from other variables. Jurimetrics is, in this sense, predominantly observational,
because the carrying out of social experiments is limited by ethical, legal and budgetary
restrictions.

For this reason, most legal research is based on observing the legal order and the
spontaneous behavior of regulators and recipients, without carrying out experiments.
It is true that institutional reforms implemented by the government and civil society,
which have an impact on the functioning of the legal order, can sometimes be treated
as semi-experiments. Although they are not controlled by researchers, these reforms
can make it possible to test hypotheses by monitoring their effect on different groups.
Legislative impact assessment, for example, should be mandatory. If we cannot test
the success of a policy before its implementation, let us at least take the opportunity
of enacting the law to learn from its results.

legal. Observation, in a broad sense, characterizes research as empirical, because its
objective is to understand a portion of the reality in which we live.

Machine Translated by Google
Jurimetry subverts this logic and presents another vision, of an inductive nature,
which seeks to understand Law from the bottom up. Let me explain: for Jurimetry,
the real Law is the applied Law, the Law practiced by the courts, applied and
complied with by the addressees of the norms. It is no use for the law to state that
the legal consequence of certain conduct is prohibited if the courts understand that
it is permitted. For example, art. 1077 of the Civil Code states that the shareholder of
a limited liability company can only withdraw (ask to leave the company with the
reimbursement of his share) if there is a modification of the contract, merger or
incorporation of the society. The overwhelming majority of case law, however,
recognizes the Right of Unjustified Withdrawal. What is Law in this case? What do
the courts decide or what is written in the law? By the principle of legality, which is
in the law. But the principle of legality is also an abstract rule, which depends on the
courts for application and, after all, what good is a law without a minimum of effectiveness?

I recognize that an important part of the study of law is based on the interpretation
of the law and the investigation of the legal system. But the identification of decisionmaking
standards from the administrative and judicial instances makes the
understanding of coordination equally important. The law cannot be understood in
an abstract way, separated from its application. And the application of Law is not studied on a case-byBefore
I finish, a last word about inductive inference in law. In the topic dealing
with the distinction between statistical power and certainty, I quickly explained the
distinction between deduction and induction and recalled that it is through the
observation of regularities and inductive inference that scientists formulate general
laws. The dogmatic approach, however, aims at deductive certainty, which has little
to do with the production of scientific knowledge. In procedural law lessons, we
learned that a good initial petition has a logical-deductive structure. The final request
would be the logical conclusion of the copulation (in Law, called subsumption)
between a major premise, which is the law, and a minor premise, which are the facts:
whoever causes harm to another must pay compensation; Joseph harmed John;
therefore, José must indemnify João. If the judge concludes something off the rails,
the conclusion is wrong and must be disregarded. This is the top-down view of the
dogmatic approach, in which the definition of the meanings of the law determines a
single and necessary legal consequence to the facts. Hence the efforts that traditional research invests

The ideal, of course, is that these two plans (ordering and coordination) work
harmoniously, but the reality of law is not docilely subject to the will of the legislator.
Laws sometimes don't catch on, sometimes they catch on unexpectedly, and
inevitably every law depends on the courts for its ultimate meaning to be defined.

Therefore, it is not surprising that the arguments used in courts (where there is
no scientific commitment to the truth) have been transplanted to academia, where
much of the work presented is based almost exclusively on quotations, categorical
statements and fallacious arguments. ad verecundiam, ad populum and ad hominem.
The investigation of reality is replaced by research in libraries, as if the answers to
all legal problems had already been answered by some author from the past.
Hypothesis, from the Greek ÿÿÿÿÿÿÿ or "hipothesi", literally translates as subposition
or supposition. Thesis, therefore, is a hypothesis that has passed a confirmation
test. As there are no confirmation tests or assumptions in the dogmatic approach to
Law, there is also no need to talk about legal theses. What is traditionally written in
academia as a prerequisite for obtaining degrees in Law would be better designated
by the neologism etíthesis. If the hypothesis is an assumption that can be confirmed
or rejected after a test, etíthesis, from the Greek ÿÿÿ ("etí" meaning that which is
superimposed), is the superposition or repetition of the same idea with the aim of
emphasizing a position .

Machine Translated by Google
IV. Kelsen and sociologism

22
20

24

23

21

Finding, through induction, the meanings of a law when applied by the courts requires
rigorous statistical methods, capable of identifying, describing and organizing
jurisprudential patterns and trends.

It is interesting to note that Kelsen, when delimiting his idea of the pure science of
Law, sought to distance two influences: political ideology and what he calls elements
of natural science. Regarding political ideology, I have little to add. It seems correct
to me to assume that the aim of science is to describe reality objectively and without
interference from the researcher's subjective inclinations. Reality is what it is, whether
we like it or not, and it is not up to the researcher to distort the facts so that reality
accommodates their political preferences. On the other hand, the question of the
relationship between Law and natural science arouses curiosity. If it is not part of
nature, what would be this reality that Law tries to understand? For Kelsen, Law has
as its object the description of the sphere of social values that determine how human
conduct should be processed. This social reality makes Law a unique science, "which
does not describe how human conduct determined by causal laws is processed in
the domain of natural reality, but how it is determined by positive norms, that is, by
norms established through human acts , must be processed".That is, the reality investigated by
Law is not that of effective human action as part of natural reality, but rather the
norms that establish how man should act. Hence Kelsen's distinction between
normative science, which describes how human conduct should be, and natural
science, which describes how conduct is.

When criticizing the
sociologism of the realists, Kelsen objects that values are not located in natural
reality, but in a parallel social reality and that "legal norms are not, as already pointed out,

Kelsen admits that a natural science on human behavior would be theoretically
possible and recognizes that "there is no sufficient reason not to conceive human
behavior also as an element of nature, that is, as determined by the principle of
causality, that is, for the not explain, like the facts of nature, like cause and effect.

Hans Kelsen is a jurist for whom I have deep admiration. He was the philosopher
who worked hardest to keep the Law away from political interference and who made
a consistent effort to problematize issues related to the concept of the science of Law.

It cannot be doubted that such an explanation - at least to some degree - is possible
and (...) such a social science cannot be essentially distinguished from the natural sciences".

The account he gives of the objectives of his work is illuminating regarding his main
concerns: "For more than two decades I have undertaken to develop a pure legal
theory, that is, purified of all political ideology and all the elements of natural science,
a legal theory aware of its specificity because it is aware of the specific legality of its
object. of the spirit. It was interesting to explain, not its tendencies addressed to the
formation of Law, but its tendencies exclusively directed to the knowledge of Law,
and to approximate as much as possible its results to the ideal of all science:
objectivity and accuracy".

intentionally chosen to bias what we would like the law to say.

This

natural science of man would be the sociology of law. However, the sociology of Law
should not be confused with legal sociologism, a nickname intended for the studies
of "eminent representatives of the so-called 'realistic' American jurisprudence who
claim that Law - the Law - is nothing more than a prophecy about how the courts will
decide, that the law is a predictive science".

Machine Translated by Google
27
25

29

26

Kelsen makes an important contribution by distinguishing the imputation
relationship, typical of the legal norm, from the causality relationship, present in
nature, as well as by defining the boundary between being and having to be. However,
the question that underlies this discussion is uncomfortable: is Law just a set of
deontic propositions about what an ideal world would be like, insensitive to people's
real behavior? Or, to put it another way: Is the purpose of law just to indicate how
people should act, without any concern about how they actually act? And, finally, if
this is true, how is it possible to distinguish valid State law, which obliges us to pay
taxes and not to drive above the speed limit on a highway, from other orders of
behavior, such as that of the Church or our teachers in school?

statements either about future events or about past events. As a rule, they actually
refer to future human conduct. However, they say nothing about this conduct, but
prescribe it, authorize it or permit it".

This answer works if we are facing a single robber, who issues only one order.
But if we enter an extensive territory, dominated by a group of robbers hierarchically
organized, around a system of rules and under a single command, what is the
difference between the band of robbers and the State? Saint Augustine, in Civitas
Dei, says that the difference lies in the justice of the rules and asks: what are empires
without justice, if not large bands of robbers? And what are bands of robbers, if not
small empires? Kelsen, however, averse to the relativism of the idea of justice,
disagrees: "If justice is taken as a criterion of the normative order to be designated
as Law, then the capitalist coercive orders of the Western world are not at all Law
from the point of view of the communist ideal of Law, and the coercive order of the
Soviet Union is also not at all Law from the point of view of the capitalist ideal of
justice".

These issues are tackled by Kelsen with the Augustinian example of the band of
robbers. By defining the legal norm as an order of coercion, Kelsen wonders about
the difference between the command of a government tax collector (pay tribute,
under penalty of imprisonment) and the command of a highwayman (money or life). .
Why is the first command valid and the second not? The initial response is that the
robber's command is an isolated act, while the tax collector's command is part of a
legal system: "If it is a question of the isolated act of a single individual, such an act
cannot be considered a legal act and its meaning cannot be considered as a legal
norm, due to the fact that Law - as we have already highlighted - is not an isolated
norm, but a system of norms, a social order, and a particular norm can only be
considered as a legal norm to the extent belonging to such an order".

In other
words, law would not care about what will happen in the future, but only with it being
the mistake of realists to try what should happen according to norms, to reduce
a statement about what should be (a legal norm) to a statement about what will be (a
conduct), dissolving the normative character of the Law.

For Kelsen, the distinction between the State and the band of robbers is not the
justice of the norms, but their effectiveness and their ability to exclude other orders
of coercion. Thus, if "this order of coercion is limited in its territorial domain of
validity to a certain territory and, within that territory, is so effective that it excludes
any and all other orders of coercion, it can be considered as a legal order and the
community through it constituted as a 'State', even when it carries out externally -
according to positive international law - an activity considered criminal ".

Machine Translated by Google
It can be seen here that, by justifying the law through force and not justice, Kelsen
admits that norms are not commands alien to reality, which are limited to issuing
statements about how people should behave. To be Law, these commands need to
have an effect on the behavior of their recipients and need to be, to some extent,
effective. If the commands of a legislator are not applied by jurisdictional authorities
and are not obeyed by anyone, if these commands do not have sufficient force to
exclude other coercive orders from their territory, then they are not Law. Had Kelsen
been a jusnaturalist like Saint Augustine, the exclusively deonic sense of Law could
have been preserved. Valid legal norms are those that declare an ideal of justice,
even if no one respects them. But by basing the Law on its effectiveness, on the
conduct of its recipients, this exclusively deontic sense has an ontic dimension, in
which the understanding of how people behave assumes a fundamental weight.

Hence the danger of Kelsen's precocious criticism of legal realism and the
investigative effort of the courts. If the 16,000 judges active today in Brazil refused
to apply the laws promulgated by the National Congress, the Law as we know it
would cease to exist and a new Law, of praetorian origin, would come into force.
Likewise, if citizens decided to disregard court orders and the police and army
refused to execute them, the Law would also dissolve and give way to a new
spontaneous order or anarchy. Therefore, knowing the Law is not limited to the
interpretation of abstract norms contained in codes and laws. Knowing the law also
means understanding how judges and, in the end, how people react to these
commands. As it is the magistrates who first attribute effectiveness to the Law,
defining how norms should be interpreted, removing inappropriate norms and filling
gaps, it is not possible to understand what Law is without investigating the
relationship between abstract norms and the behavior of the courts.

The legal order has the immediate objective of regulating conduct and applying
sanctions, but its immediate objective is to promote changes in people's behavior.
In creating and enforcing norms, operators don't just want to publicly declare what's
right and punish disobedients. They aim to spread socially desired attitudes, promote
the reduction of crimes, the prevention of illicit acts, the satisfaction of debts, the
preservation of companies, the payment of taxes, the protection of the family and
minorities. The legal order is not a set of statements about what ought to be. It is a
social control tool, which aims to repress unwanted behavior and disseminate
desirable ones.

Hence the importance of conceptualizing and studying the legal coordination
plan, which is the institutional space where legal norms are individualized and
implemented. If the study of the planning plan reveals the abstract meanings of the
norms enacted by the legislator, the investigation of the coordination plan shows us
if and how these norms are fulfilled. Therefore, a discipline that proposes to
understand in a holistic way how the Law works needs to overcome reductionisms.
Legal positivism reduces Law to the abstract commands of the legal system and
legal sociologism reduces it to concrete commands of legal coordination. For
Jurimetry, however, the study of Law combines the relationship between these two
plans, in order to investigate, first, which abstract norms are socially effective and,
second, to understand the reasons for the ineffectiveness of those that are not
obeyed. A norm is not complied with for three reasons: because it was poorly drafted,
because it collides with society's values or because sanctions are not sufficiently
dissuasive. Each of these reasons implies different solutions for the improvement of
coexistence in society, being one of the most important tasks of the jurist to face these questions.

Machine Translated by Google
I return once more to Kelsen, who urges the pure science of Law to leave aside
the natural behavior of men as an object, to focus on the "normative order of human
conduct", which is an "object different from the causal order of nature". .

In these passages, Kelsen begins to construct the distinction between the
concepts of causality and imputation. The so-called normative science of law has as
its object the legal norm and describes deontic propositions about how man should
behave: which norms regulate certain conduct and how these norms are
systematically adjusted to predetermine whether the conduct is permitted, prohibited
or obligatory. The natural science of legal sociology, on the other hand, has human
action as its object and describes ontic propositions about how man behaves: what
factors are associated with a behavior and how these factors combine to determine
how the conduct will be. For Kelsen, Law deals with the study of imputation
relationships, that is, relationships in which the occurrence of an antecedent implies
a consequent that should occur: if A is, then B must be; and natural science
describes causal relationships in which the occurrence of a cause implies a necessary effect: if A is, then

In an effort to recover the importance of the study of causality for Law (as, indeed,
for any field of knowledge that claims to be something more

To

elaborate the distinction between natural science and normative science, Kelsen
states that "nature is, according to one of many definitions of this object, a certain
order of things or a system of elements that are connected with each other as cause
and effect, or be, therefore, according to a principle that we designate as causality.
The so-called natural laws, with which science describes this object - such as, for
example, this proposition: when a metal is heated, it expands - are applications of
this principle. that intervenes between heat and expansion is cause and effect."
However, in relation to law, when "one proceeds to the analysis of our statements
about human conduct, it is found that we connect acts of human conduct with each
other and with other facts, not only according to the principle of causality, that is ,
as cause and effect, but also according to another type of principle that is completely
different from that of causality, according to a principle for which there is as yet no
generally accepted designation in science".

Jurimetry sees the main function of law in improving social interaction. There is,
therefore, a causal relationship between legal regulation and the quality of
coexistence between people, which is, in a certain sense, a platitude. If criminal
justice does not function properly, crimes increase. If taxes are too high, the economy
slows down. Although intuitively these cause and effect relationships are quite
evident, there is an interesting discussion about the pertinence of talking about
causality in Law.

This distinction between natural science, based on causal relations, and normative
science, founded on imputation relations, caused Kelsen and, to a large extent, all
faculties, to reduce Law to the study of the abstract rules of the legal system and to
refrain from dealing with of the methodologies and problems that involve the study
of causality in human behavior, depriving operators of the necessary tools to
evaluate the success or failure of regulatory solutions.
Studying Law has become synonymous with studying the meaning of laws, without
major concerns about understanding the effects and consequences that these laws
produce in society. Jurists are able to interpret what the letter of the law
predetermines, but they are unable to go beyond conclusions and analogies with
personal experiences to intuit the practical consequences of its application.

31 32

33

V. Causality and attribution

Machine Translated by Google
34

The criticism of the false expectation of future repetition of past facts is a reedition
of the well-known problem of induction, addressed by several philosophers
of science, such as Hume, Sextus Empiricus and Karl Popper. Basically, the problem
states that induction (non-deductive arguments, whose conclusions are more general
than the premises) is not capable of leading to secure knowledge. For example,
although all forms of life known to date have carbon, hydrogen, oxygen, nitrogen,
phosphorus and sulfur (the biogenic elements), the possibility of finding an unknown
form that organizes itself without using any of these elements will always exist.

The criticism is unfounded, because it accuses science of not being something it
never sought to be: infallible. If logic and faith aim at absolute truths, the empirical
sciences are aware of the precariousness of their laws and their subjection to
dissonant observations. By the principle of falsifiability, any pattern of regularity, no
matter how long, runs the risk of being broken by an unexpected event. If every time
we mix acid and base the reaction produces water and salt, then we can generalize
our observations into a law of chemical transformation. What guarantees that there
will not be a disruption of this regularity and the law will be invalidated? From a
logical point of view, there is no guarantee, but scientific law is not logical: it is an
inductive law and therefore falsifiable. And induction (deterministic or probabilistic)
is the only means of access to knowledge and investigation of an order in reality, be it natural or social.

Freedom would be a

component of order disturbance capable of making predictions unfeasible.

The first criticism is directly related to the idea of a pure science of law, in such a
way that all the comments in the topic above (Kelsen and sociologism) about the
proposal of a normative science and about the importance of court behavior for
understanding Law fit here . I would just add that, ultimately, the effort to reduce Law
to the study of the meaning of the letter of the norms fails to create a category of
science that does not take as its object a portion of reality. All science, by definition,
studies objects that are part of reality. Science describes its object by placing it in
time and space, explaining what caused it and how it affects the existence of other
objects. Reality, causality, falsifiability and predictability are, therefore, essential
qualities of scientific thinking. Access to abstract meanings, involving values and
opinions about how the world should be, are not science due to the insurmountable
incompatibility with these qualities.

Thus, it does not matter how many species we have discovered, nor the fact that all
species found to date are built from biogenic elements. Still, we cannot infer from
these observations that all forms of life in the universe, including those we may never
encounter, have this structure.

The third criticism concerns the role of free will in law. Kelsen states that the
intervention of human will in shaping the legal norm (the choice of a judge before
sentencing; or of a group of legislators before enacting a law) would make the
characterization of causality relationships unfeasible. Being free, the will would not
allow the kind of regularity, typical of cause and effect relationships, which makes
inductive generalization possible in the natural sciences. Furthermore, natural
causality would present mechanical traits and would act independently of the
intervention of any volitional action and, therefore, would not be applicable to human behavior.

than mere literature), I will summarize and comment here, due to their relevance,
Kelsen's three main criticisms of this idea: first, the notion that imputation, and not
causality, is the relationship that translates the specific meaning of the law ; second,
the belief that the past will repeat itself in the future does not serve as a basis for
prediction; and third, the idea that interference with free will would be incompatible
with regular causal relations.

Machine Translated by Google
However, the practice of Law operates on a daily basis with considerations of a different
nature, consisting of judgments about what is likely to happen. These are not judgments
about what happens, or about what should happen, but about the chances that something
may happen, the so-called probability judgments. Take the example of a mother who visits
a lawyer because her son has been arrested on drug trafficking charges. The mother
confirms the possession of the drug and questions the lawyer about what could happen.
From a normative perspective, the lawyer would observe the abstract legal norm and
restrict himself to answering that, according to the law, the boy should be arrested.

Criticism errs in overestimating the disorder produced by freedom. The human will,
although free, manifests itself through observable patterns. It is partly a free will, but partly
restricted by natural, psychological, cultural, ethical, legal and economic barriers.
Unfortunately, we are not as free as we think we are. For this reason, it seems to me
perfectly feasible to identify patterns of regularity in people's behavior and, in some cases
and under certain conditions, laws capable of predicting human behavior. There are
numerous surveys that have detected behavioral patterns (consumption, crime, habits)
and there are entire markets that are based on this type of forecast, such as insurance. In
addition, the creation of a legal order is based precisely on the premise that men will, at
least the vast majority, react positively and comply with legal commands, and therefore, it
is possible to anticipate and control social behavior through the administration of
penalties. . Therefore, it seems to me that the existence of regular causes of the human will
is not contrary to the Law, being in fact the basis of the functioning of any legal order.

How old are you? Study or work? What social class do you belong to? What is your
education level? Lives with the parents? Have you ever been hospitalized for addiction?
On the other hand: is there already a judge? How old are you? Where did he study? Do you
have published books? Does the court specialize in criminal law? Are there precedents judged in similar cases?

However, the lawyer knows that this answer is insufficient. The creation of a defense
strategy goes through the implementation of the analysis, in order to obtain information
about the behavior, on the one hand, of the boy and, on the other, of the judge. Is the offender black or white?

One of the characteristics of scientific knowledge is the ability to anticipate the future
states of an object. A scientist describes how something works not only to know it in the
present, but also to be able to predict its behavior in the future. It is predictability that
makes science a tool of control over the external environment and thus enables the
development of technologies capable of improving our living conditions. The possibility of
creating a social science capable of reducing human action to a set of laws and formulating
predictions about its future states is the subject of long discussion. Many believe that the
human will is too complex to model and that freedom would make the study of man
incompatible with materialistic or mechanical laws. In the case of Law, its object would not
be to predict how people (be they judges or parties) will act in the future, but rather to
identify in legal commands the meaning of how they should act. This classic distinction
between what is and what should be is what led to the classification of Law as a normative
science, as opposed to the concept of causal science. Law would be the only normative
social science and its object would not be man, but the legal norm that regulates his
conduct.

What the lawyer intuitively seeks with these questions is to identify specific
characteristics both in the behavior of the regulator and in that of the addressee, capable
of influencing the chances of a conviction and, therefore, adjust his defense strategy to a
model that increases the probability of acquittal, reduction or conversion of the sentence.
The judge's decision (individual norm) is therefore not a corollary of a general norm, but

Machine Translated by Google
SAW. Regulatory and effectiveness jurimetrics

35

36

Analyzed from a concrete perspective, Law is all stitched together by probabilistic
causality relationships. The lawyer wants to know which strategy has the greatest
chance of winning, the legislator wants to know which proposed law could be more
socially beneficial and the judge wants to know the likely effects of his decision.
The development of models that allow correct predictions to be made is the main
function of scientific thinking, in its mission to expand our control over the
environment in which we live. This is the main reason for Jurimetrics' effort to
reestablish causal relationships in the study of Law, making scientific theory more
useful and closer to the daily practice of those who work in courts, offices and legislative houses.

An example of effective
Jurimetrics are studies that seek to verify, for example, the effects of the new
bankruptcy law on the behavior of banking institutions, especially in reducing
interest rates and lengthening loan terms.

A central element of regulatory Jurimetry is the legal process. The legal process
is the process of production of the legal norm. Legal processes can be classified as
jurisdictional, administrative, legislative or business. What defines the type of
process is the origin of the power of those who create the norm. Jurisdictional
processes are those in which someone endowed with jurisdictional power, a judge
or an arbitrator, for example, produces an individual norm. For example, an ordinary
action or an arbitration proceeding. Administrative processes are those in which a

Through this explanation, it is clear that the relationship capable of affecting the
functioning of the legal order is not one of deterministic causality, as in the classical
natural sciences, nor of imputation, as in Kelsen's normative science, but of
probabilistic causality. As already explained, probabilistic causality is defined as a
relationship in which the cause increases the probability of occurrence of the
corresponding effect, keeping all other conditions constant: if A is, then there is a
greater probability that B will come into being. It differs from deterministic causality,
in which the effect necessarily follows from the cause - if A is, then B will be - and
from imputation, where the occurrence of the antecedent implies the should-be of the consequent: if A

It is important to reiterate that probabilistic causal relationships are present not
in abstractly considered legal norms, but in the concrete relationships established
between the human conduct of regulators and recipients of the norms. As a result,
Jurimetrics does not practice what Kelsen and Vilanova call sociologism, since its
objective is not to reduce the normative must-be to the being of causality. The duty
to be is placed on the abstract plane of the norm and can be described by dogmatics
as a legal proposition, while the causal relationships described by Jurimetrics arise
on the concrete plane of intersubjective human conduct. They are, therefore,
approaches that do not collide, but complement each other.

Jurimetry has two dimensions: regulation and effectiveness. Regulatory
Jurimetrics concerns the analysis of the behavior of those who produce the norm,
in contrast to effectiveness Jurimetrics, which focuses on the analysis of the
behavior of the norm's recipient. A classic example of regulatory Jurimetrics are
studies on "judicial decision-making", which attempt to isolate the cultural,
educational, religious and evaluative forces involved in the way a judge decides cases.

an effect of a set of probabilistic causes organized in an explanatory model. Having
mastered this model, the lawyer will be able to go beyond the simple abstract
statement about what should happen, to make concrete statements about what is
likely to happen.

Machine Translated by Google
37

authority, endowed with administrative power, produces a norm, which can be
individual or general. Examples include the administrative sanctioning processes of
the Securities and Exchange Commission and the issuing of a normative instruction
by a regulatory agency or autarchy. Legislative processes are those through which
the bodies of that power produce a general norm. Examples are the ordinary
legislative process of the National Congress. Negotiation processes are those in
which any person without public power produces an obligatory legal act. For example,
the process of negotiating a contract or a unilateral donation.

The legal process is a type of stochastic process, as (i) it is a chain of successive
random variables indexed in time and (ii) its final result cannot be predetermined.
The random variables correspond to the stages of evolution of a
process. For example, a collection action can be contested in several ways: with or
without preliminaries, using or not a jurisdictional exception, with or without a
counterclaim. The variation of possible defenses tends to influence the evolution of
the process and, thus, produce different results. A competency exception may have
a very low chance of success, but it may delay the judgment of the action by up to two years.

Some processes are faster than others because they are less viscous or because
they have suffered less institutional attrition. Likewise, groups of analogous
processes can behave very differently depending on the institutional environment in
which they are inserted. Thus, measuring these properties helps predict expected
behavior and the final result of a process.

As for effectiveness Jurimetry, the central element is the study of normative
impact, that is, the investigation of the effects produced by legal norms on the
behavior of their recipients. Legal norms aim to influence the behavior of their
addressees, preventing them from acting in a socially undesirable way or provoking
them to act in a socially desirable way. However, between the production of the legal
norm and the reaction of its recipient, there are a series of factors that end up
interfering with the fulfillment of the norm and that are capable of depriving or
diverting its effects from the desired objectives. The investigation of the effects of
the law can be on its degree of indirect effectiveness, that is, how much the law
impacted on the behavior of magistrates when judging their cases, or of direct
effectiveness, evaluating to what extent and in what way the law impacted on the
behavior of its final addressees, the citizens.

The processes of creating legal norms are composed of a succession of preestablished
procedural phases, each one allowing its participants to make choices.
As a rule, no participant has control over all choices, the result of each step and also
the final result of the process are unknown. For example, a judicial collection process
unfolds in several phases, such as the presentation of an initial petition, defense,
sanctioning order and sentence. The plaintiff chooses what to ask for, the defendant
how to answer and the judge grants the evidence, according to the alternatives given
to the parties. No one, not even the judge, has absolute control over the process.
Seen in this way, processes in law are stochastic. A stochastic process is a collection
of random variables, indexed by a set of indices and which presents an evolution in
time.

The study of the flows of legal processes, or procedural rheology, is a discipline
of Jurimetry capable of collaborating in the identification of strategies to influence
the production of norms. Procedural rheology studies the dynamic properties of legal
processes, such as speed (how many procedural acts are practiced in a certain time
interval), viscosity (how many procedural acts are practiced per phase) or institutional
friction (what is the effect of the institutional environment on the speed of the process). process).

Machine Translated by Google
39

41

38

40

VII. Jurimetry as science and technology

According to Tercio Sampaio Ferraz, science and technology are distinguished by their
purposes and means. Science is concerned with the problem of veracity and is concerned
with describing objects. Its objective is to create a model of accurate predictions and to
develop theses that can be verified and falsified. He says that a "scientific investigation is
always faced with the problem of truth. We admit, therefore, that all science intends to
obtain statements independent of the situation in which they are made, insofar as they
aspire to erga omnes validity. (...) Now , scientific statements are basically descriptive, the
others [prescriptive, resolving and informative] appearing secondarily in the establishment
of methods, in the choice of themes, etc. Being descriptive, they are statements that
confirm what existed, exists or will exist, thus having a manifest operational meaning ,
constituting a system of probable and safe predictions, as well as reproductions and
interferences in the phenomena it describes. These are, therefore, certain findings whose
evidence, according to the verification criteria of

Transposed to Law, the ideas of science and technology gain specific meanings.

judges will feel compelled to apply the norms in their decisions, if
the reaction of the final addressees to the imposition of these norms will be the socially
desired one.

Jurimetry is a science that aims to describe the factors that interfere in the

functioning of a legal order, notably in the production of norms and in the identification of
the effects they produce in social behavior. In one sentence, Jurimetry aims to describe in
detail, preferably measuring, the true Law. And what is true law? It is not the abstract
article of a code, which obtains its validity through the authorities that promulgated it,
whose competence to legislate, in turn, comes from other norms, based on other
competences, in a self-sustaining pyramid of legal rules. True law corresponds to the rules
actually applied by the courts, which have a minimum level of effectiveness and which,
excluding other rules from their jurisdictional territory, are obeyed by their addressees.

each epoch of scientific development, tells us to a high degree that they are true.

as well as understand

Technology, on the other hand, is an extension of science, which appropriates scientific
descriptions and models in order to control reality and develop solutions to practical
human problems. Technology is therefore concerned with the problem of the technical
usefulness of scientific knowledge and seeks to employ it to satisfy needs.

Per

The distinction between direct and indirect effectiveness is important insofar as the
judge is at the same time a producer and recipient of norms. When introducing a new legal
regime, it is important to assess whether the new norms will have indirect effectiveness, that is, whether the

In Tercio's words: "Technological thinking, characteristic of dogmatics, takes, so to speak,
the factual possibilities shown by science and transforms them into possibilities of human
action, on the assumption that, at certain points in the occurrence of phenomena, it is
possible a practical intervention. Thus, technological thinking is not a normative system,
although it hides something prescriptive. It does not oppose science, but extends it,
carrying out transformative operations consistent with the relevance attributed to certain
conclusions of scientific theories for the future. solution of practical problems. Therefore,
it does not go beyond the very premises of science". For example, the discovery of
the relationship between mass and the speed of light by Albert Einstein (E=m.c2) is
scientific knowledge that, used by the allies during World War II, solved the practical
problem of building a weapon (the atomic bomb ) powerful enough to end the conflict.

Machine Translated by Google
42

43

Returning to the concept of science, Jurimetry is classified as a human, causal
and stochastic science. It is science because it works with the problem of veracity,
formulating statements that are true to the extent of the precision with which they
manage to describe and predict the behavior of their object. It is human because its
object of study is the behavior of men in two specific situations: as regulators or as
recipients of a legal order. It is causal because the relationships investigated by
Jurimetry connect elements of probabilistic cause and effect with each other. And it
is stochastic because its explanation models are not deterministic, being, on the
contrary, based on the purpose of only controlling (and not extirpating) the
uncertainty, which is inherent to Law.

More than simply describing the functioning of a legal order, Jurimetria provides
elements so that these operators can make the decision that best meets their needs:
in the case of the lawyer, to better advise his client on his chances of success; in the
case of the legislator, to perfect law and society; and in the case of the judge, to
deliver a sentence with expected consequences.

No doctor worthy of the name prescribes medicines and treatments without knowing
what the patient is suffering from and why. If our purpose, eg, is to speed up the
Justice machine, we need to know which parts are performing less, and how the
sand that wears them down penetrates the mechanism. Without this prior verification,
we will have no solid criteria for undertaking the work of reform. We run the risk of
going out to attack windmills, while leaving the real enemies in peace and quiet.
After reforming the law, it is necessary to closely monitor, with adequate lenses, the
appropriate impact of the reform on the forensic day-to-day. There is no other way to
find out what has really changed, in what sense and to what extent. Nor is it possible
to conceive, without this elementary precaution, a minimally objective assessment,
in the light of which we can decide whether it is worth continuing in the same direction or whether it is
Returning to the parallel with medicine: given the medicine, starting the treatment,
the doctor's mission is not finished: it is up to him to observe how the patient's body
is reacting and, as the case may be, increase or decrease the dose, when not
necessary. replace therapy that has proved to be anodic or counterproductive".

From a technological point of view, jurimeria presents relevant applications,
contributing to the solution of legal decidability problems. As it is an extension of
science, the technological use of knowledge is directly proportional to the predictive
power of scientific models. If science has high predictive power, it tends to produce
technologies of high relevance. In this perspective, Jurimetria can, for example,
recommend to the legislator a change in the law capable of reducing the time of the
processes, or the application of a type of penalty that reduces the level of recidivism
of offenders, or even provide the judge with elements that allow anticipating the
concrete effects of a sentence. Jurimetria models are still in an early stage of
development. However, as we accumulate data on the functioning of legal orders
and refine our analysis tools, it is expected that operators will have a greater
understanding and, consequently, a greater ability to predict their behavior.

Aware of this relationship between observation, prediction and follow-up, typical
of rigorous and consistent action, José Carlos Barbosa Moreira compares the activity
of a lawyer dealing with problems of justice with that of a doctor dealing with an
illness and explains the importance of empirical diagnosis and follow-up of the
practical results of any treatment for its success: "Before reforming the procedural
law (rectius: any law), logic and common sense dictate that a diagnosis be made, as
accurate as possible, of the evils that one wants to fight and of the causes that generate or feed it.

Machine Translated by Google
Pontes de Miranda, in an interview for the newspaper O Estado de São Paulo, on August 5,
1979, in: DANTAS, Lourenço (coordinator). The lived history [interviews]. São Paulo: O
Estado de São Paulo, 1981. p. 213-214. I remember that today we have this knowledge thanks
to the surveys by Justiça Aberta and Justiça em Números by the CNJ. Now we need to learn how to use it.

FOOTNOTES

The distinction between science and technology goes back to another distinction,
between positive science and normative science. Here, however, the concept of
normative science is used in a different sense than Kelsen's. In this alternative
meaning, normative science is one that investigates its object in accordance with a
political or ethical preference, with the investigator acting without axiological neutrality
towards the object. On the other hand, positive (or descriptive) science is one that
describes its object independently of a specific purpose, with the investigator
exempting himself from the purposeful usefulness of his discoveries.

Jurimetrics is, therefore, not a normative science in the Kelsenian sense, because
its object of interest is not a legal norm in itself (the legal ought-to-be), but the behavior
adopted by men in function of a legal order (the legal being). .
But within this alternative Friedmanian meaning, we can think of positive Jurimetry,
whose main objective is to describe in a neutral, realistic and impartial way which
norms are actually being applied and what effects they are producing; and a normative
Jurimetry, which is based on the positive, and which establishes final purposes for
the reforms to improve the Law, in order to make it faster, more effective and, why not,
fair. History shows that the discussion around the problems of Law has evolved little
since Euthyphro's dilemma and the famous conversation between Plato and
Thrasymachus in the Republic.
I believe that Jurimetrics is one of the possible ways to overcome these difficulties,
teaching us a little about how the legal order works and how to develop better policies
in a global, complex and frantically changing society.

Milton Friedman explains, in a classic essay, the relationship
between positive and normative economics, in an excerpt that could be directly
transplanted to the discussion about the object and usefulness of Jurimetrics. For
him, all "normative art" is based on a positive knowledge about a reality. It is this
positive knowledge about the practical consequences that makes the choice about
whether or not to do this something viable: "The art of normative economics, on the
other hand, cannot be independent of positive economics. Any political initiative
necessarily rests on a prediction about the consequences of doing one thing rather
than another, a prediction that needs to be based - implicitly or explicitly - on positive
economics. There is, obviously, no direct relationship between political initiatives and
positive economic conclusions; if so If it were, there would be no separate normative
science. Two individuals may agree on the consequences of legislation. One may view
them as desirable and be in favor of the legislation; the other as undesirable and
oppose it."

We are, after two and a half thousand years, more or less in the same place.

44

47

1

45

46

two

Machine Translated by Google
3

4

5

"What is Econometrics? Strange as it may seem, it does not exist a generally accepted answer
to this question. Responses vary from the silly 'Econometrics is what econometricians do' to
the said 'Econometrics is the study of the applications of statistical methods to the analysis of
economic phenomena', with sufficient disagreements to warrant an entire journal article devoted
to this question.

KENNEDY, Peter. A guide to econometrics. 6th ed., Cambridge: Wiley-Blackwell. 2008, p 1-2.

This distinction between, on the one hand, the norm and, on the other, behavior, is approached
by Hans Kelsen in the first topic of Part III of Pure Theory of Law. In a footnote, Kelsen states
that taking the legal norm as an object is the distinction between pure theory of Law and the socalled
egological theory of Law, which takes human conduct as the focus of interest. See
KELSEN, Hans. Pure theory of Law. 6th ed., Coimbra: Armênio Amado, 1984, p. 109.

KELSEN, Hans. Pure theory of Law. 6th ed., Coimbra: Armênio Amado, 1984, p. 56-65. In
Kelsenian theory, the legal order is a system of norms that have in common the same basis of
validity: the fundamental norm. The fundamental norm, in turn, is the logical legal starting point
for the creation of positive law, from which the successive dynamic procedures for editing legal
norms are established, from the constitution to a

This confusion stands from the fact that econometricians wear many different hats. First, and
foremost, they are economists, capable of using economic theory, to improve their empirical
analysis for the problems they address. At times they are mathematicians, formulating economic
theory in ways that make it appropriate for statistical testing. At times they are accountants,
concerned with the problem of finding and collecting economic data and relating theoretical
economic variables with observable ones. At times they are applied statistics, spending hours
with the computer trying to estimate economic relationships and predict economic events. And
at times they are theoretical statisticians, applying their skills to the development of statistical
techniques appropriate to the empirical problems characterizing the science of economics".

"The legal norm issued by the competent authority (Constitution, law, etc.) is the main reference
of the system for resolving conflicts of interest developed in current democratic societies. This
system is called Law and is very complex. So much so that, due to Due to its complexity,
knowing it adequately presupposes years of study, professional practice and introjection of
values.Law, therefore, cannot be defined as a set of norms published by the competent
authorities in accordance with the political and institutional organization of the State. Law is
not only law, nor essentially the law. It is more than a set of positive legal norms; it is, once
again, a complex system for resolving conflicts of interest, in which positive norms serve as
the main reference". COELHO, Fábio Ulhoa. Civil Law Course. 2nd ed., São Paulo: Saraiva, vol.
1, 2006, p. 32.

Machine Translated by Google
8
7
6

9

10

"The legal order (like any normative system) is a set of norms. This general definition of
legal order presupposes a single condition: that in the constitution of an order more norms
(at least two) concur together, and that there is no order composed of a norm only".
BOBBIO, Norberto. Theory of the legal order. 10. ed., Brasília: Universidade de Brasília,
1999, p. 31.

"[The fundamental norm] serves to explain the unity of a complex legal order. Its core is
that the norms of a legal order are not all on the same plane. There are superior norms and
inferior norms. The inferior ones depend on the superior ones.

For Kelsen, law is a normative science because its object is the legal norm. Human conduct
is only an object to the extent that it becomes the content of the legal norm: "In the evident
affirmation that the object of legal science is the Law, there is contained the affirmation -
less evident - that the legal norms are the object of the science legal, and human conduct is
only so insofar as it is determined in legal norms as a presupposition or consequence, or -
in other words - insofar as it constitutes content of legal norms". KELSEN, Hans. Pure
theory of Law. 6th ed., Coimbra: Armênio Amado, 1984, p. 109.

Ascending from the lower norms to those above, we arrive at a supreme norm, which does
not depend on any other higher norm, and on which the unity of the order rests. [...] it is the
fundamental norm. [...] It is what gives unity to the order".

In Hans Kelsen's definition: "An 'order' is a system of norms whose unity is constituted by
the fact that they all have the same foundation of validity. And the foundation of validity of
a normative order is - as we will see - a fundamental norm of the which the validity of all
norms belonging to that order is withdrawn. KELSEN, Hans.

BOBBIO, Norberto. Theory of the legal order. 10. ed., Brasília: Universidade de Brasília,
1999, p. 49.

Vol. 32 of the Thinking about Law Series : Analysis of the justifications for the production of criminal norms.

verdict.

Pure theory of Law. Almedina. P. 33.

Machine Translated by Google
15

14

13

11

12

16

[http://portal.mj.gov.br/main.asp?View="%7B329D6EB2-8AB0-4606-B054-

"the legal order constitutes a system because incompatible norms cannot coexist in it. (...) the

legal system is not a deductive system, as in the first sense: it is a system in a less incisive sense,

if you like, in a negative sense, that is, an order that excludes the incompatibility of its simple

parts." BOBBIO, Norberto. Theory of the legal order. 10. ed.

There were 800 regulations in 2008, the last year in which the Statistical Yearbook of Legislative

Activities was published. Divided by 260 working days in the year, we arrive at 3.0769 general

standards per working day. See [www2.camara.leg.br/atividade-legislativa/legislacao/publicacoes/

anuario-estatistico do-processo-legislativo]. Accessed on: 7/30/2013).

"Due to the presence, in a legal order, of lower and higher norms, it has a hierarchical structure.
The norms of an order are arranged in hierarchical order."

in:

Brasilia: University of Brasilia, 1999, p. 80.

BOBBIO, Norberto. Theory of the legal order. 10. ed., Brasília: Universidade de Brasília, 1999, p.
49.

,

There were 24.2 million new cases in 2010. Divided by 260 working days, we arrive at 93,076.92

new cases per working day. See Justice in Numbers 2010 report www.cnj.jus.br/programas-de-aaz/

eficiencia-modernizacao-e-transparencia/pj-justica-em numeros/relatorios (accessed on

July 30, 2013).

Gregory Mitchell explains how observation differentiates empirical research from other approaches

based on guesswork, imagination, or pure logic: "[T]he basic point of separation between empirical

and non-empirical research is the role that observation plays in the research:

VILANOVA, Lourival. Logical structures and the positive law system. São Paulo: Max Limonad,
1997, p. 185-188.

Available
4CAD3C53EE73%7D]." Accessed on: 30.07.2013.

[E]mpirical research [is] explicitly founded on direct observations of the world or inferences from

observations; non empirical research does not intend that its claims about the world are

Machine Translated by Google
18

17

19

21

20

22

Empirical legal scholarship as scientific dialogue. North Carolina Law Review, vol. 83, 2004, p. 197-
198.

COZBY, Paul C. Research methods in behavioral sciences. 5. reprint, São Paulo: Atlas, 2011, p.
171-215. ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneira Thompson
Learning, 2006, p. 239-273. COOPER, Donald R. & SCHINDLER, Pamela S.
Methods of research in the field of administration. Porto Alegre: Bookman., 2003, p. 318-333.
MALHOTRA, Naresh. Marketing research: an applied orientation. 6. Ed., Porto Alegre: Bookman,
2012, p. 173-195.

in:

03/29/2015.

Logical structures and the positive law system. São Paulo: Editora Max Limonad, 1997, p. 62.

KELSEN, Hans. Pure theory of Law. 6. ed. Coimbra: Armênio Amado, 1984, p. 132.

COZBY, Paul C. Research methods in behavioral sciences. 5. Reprint, São Paulo: Atlas, 2011, p.
123-138. COOPER, Donald R. & SCHINDLER, Pamela S. Research methods in administration. Porto
Alegre: Bookman, 2003, p. 302-317. ZICKMUND, William G. Principles of marketing research. São
Paulo: Pioneira Thompson Learning, 2006, p. 216 to 238. MALHOTRA, Naresh. Marketing research:
an applied orientation. 6. Ed., Porto Alegre: Bookman, 2012, p. 139-169.

KELSEN, Hans. Pure theory of Law. 6. ed. Coimbra: Armênio Amado., 1984, p. 118.

See report from the newspaper O Estado de São Paulo: [http://saude.estadao.com.br/noticias/

geral,em 4-anos-numero-de-processos-por-erro-medico-cresce-140-no- stj-imp-,1655442].

KELSEN, Hans. Pure theory of Law. 6th ed., Coimbra: Armênio Amado, 1984, p. 7. Lourival Vilanova
also believes that scientific knowledge of Law has particularities: "Legal, anthropological-social,
sociological, philosophical knowledge. Each species has its own investigation techniques, and
other common ones. But there is a type of knowledge that stands out of the others: that of the
Science-of-Law ( dogmatic knowledge )". VILANOVA, Lourival.

founded on anything other than imagination, supposition, or logic". MITCHELL, Gregory.

Access

Machine Translated by Google
KELSEN, Hans. Pure theory of Law. 6. ed. Coimbra: Armênio Amado, 1984, p. 135.

"If the domain considered by these sciences is contrasted, as a sphere of values, with the
sphere of natural reality, it must be taken into account that these are values constituted by
positive norms, that is, norms that are placed in space and time by human acts, and that,
therefore, the object of these social sciences is not unreal, that any reality also belongs to
it or corresponds to it - except that, in this case, it is a reality different from the natural one,
namely, a social reality. " KELSEN, Hans. Pure theory of Law. 6. ed. Coimbra: Armênio Amado, 1984, p. 132-133.

KELSEN, Hans. Pure theory of Law. 6. ed. Coimbra: Armênio Amado, 1984, p. 136.

"A natural law says: if a metallic body is heated, it will expand; a legal law says: if an
individual steals, he will be punished by the court. (...) Against this opinion we must, first
of all, note- It is clear that the statement that legal laws are, like natural laws, assertions
about a future to happen cannot refer to norms established by legal authority - either to
general norms established by the legislator or to general norms set by the courts in its
decisions - that is, it cannot refer to the Law, but only to the descriptive propositions of the
Law formulated by legal science. (...) Natural laws are based on our experience and our
experience resides in the past, not in the future . As a prediction of the future, a natural law
is only applicable under the problematic assumption that the past is repeated in the future.
(...) The prophecies of realistic jurisprudence are distinguished from the legal propositions
of the normative science of Law only by the fact that be statements of being and not of
ought. But, as assertions of being, they do not translate the specific meaning of Law. To
the extent that the courts, in their decisions, create new Law, its prediction is as little
possible as the general prediction to be produced by the legislative body." KELSEN, Hans.
Pure Theory of Law. 6. Ed., Coimbra : Armênio Amado, 1984, pp. 135-136.

As Lourival Vilanova explains: "From this formal angle, all the criticism that Kelsen makes
of sociologism (not of the sociology of Law, which is an area of legitimate investigation)
can be taken as criticism of the reduction of p-deontics to p-descriptives. Holmes or
Cardoso (sic) consider Law as the prediction of how individuals and, especially judges and
courts, will behave, dissolving the normative character of Law. Making probability
judgments about future conduct, based on current conduct , it is important to reduce the
norm to a proposition that describes: 'under certain conditions, an individual will probably behave in this or that

24

23

25

27

26

Machine Translated by Google
31

32

33

34

28

29

30

It rests on a presumed (always based on the experience of certain cases) functional
relationship, that is, here, on a law. Now, the formula of the law of nature is 'if A is, then B is',
while the legal law 'A is, then B must be'." VILANOVA, Lourival. Logical structures and the system of positive Law .

KELSEN, Hans. Pure theory of Law. 6th ed., Coimbra: Armênio Amado, 1984, p. 118-119.

KELSEN, Hans. Pure theory of Law. 6th ed., Coimbra: Armênio Amado, 1984, p. 81.

KELSEN, Hans. Pure theory of Law. 6th ed., Coimbra: Armênio Amado, 1984, p. 118.

São Paulo: Editora Max Limonad, 1997, p. 73.

"Like a natural law, a legal proposition also connects two elements together.

KELSEN, Hans. Pure theory of Law. 6th ed., Coimbra: Armênio Amado, 1984, p. 119.

KELSEN, Hans. Pure theory of Law. 6th ed., Coimbra: Armênio Amado, 1984, p. 70-80.

However, the connection that is expressed in the legal proposition has a completely different
meaning from that which natural law describes, that is, that of causality. (...) The fact that the
meaning of the copula or connection of the elements in the legal proposition is different from
that of the connection in the elements of natural law results from the fact that the connection
in the legal proposition is produced through a norm established by the legal authority -
through a act of will, therefore - while the connection of cause and effect, which in the natural law
is affirmed, is independent of any intervention of this kind" . p.120.

KELSEN, Hans. Pure theory of Law. 6. Ed., Coimbra: Almedina, 1984 p. 78.

mode'. The descriptive proposition of facts, to be scientific, rests on the assumption of the
regularity of phenomena, that is, conduct C is a function of the factors F', F'', F''' or C = f (F', F'' , F''').

Machine Translated by Google
See HEISE, Michael. The past, present, and future of empirical legal scholarship: judicial decision

making and the new empiricism. University of Illinois Law Review, vol. 4, 2002, p. 832.

in:

DocumentID="%7B68E6736C-4DF7-498B-ABC3-

FERRAZ Jr., Tercio Sampaio. Social function of legal dogmatics, 1998, p. 91. See also FERRAZ JR.,

Tercio Sampaio. Introduction to the study of Law: technique, decision, domination. 4. Ed., São Paulo:

Atlas, 2003, p. 83-91.

A notorious example of deprivation of indirect effectiveness are the rules of disregard of the legal

personality of art. 50 of the CC in the Labor Court.

DBCFE29195F6%7D&ServiceInstUID=%7B0831095E-D6E4-49AB-B405-C0708AAE5DB1%7D]."

Research "Analysis of the new bankruptcy law". Academic coordination Aloisio Pessoa de Araujo.

Fábio Ulhoa Coelho works with different concepts of legal science and technology. Legal science

describes the reasons why a given society decided to produce a certain legal norm. Technology, on

the other hand, deals with the problem of convincing interpretation and persuasion regarding the

meaning to prevail for a given norm. "In close terms, the objective proposed by the knowing subject

who focuses on the legal norm defines the character of the knowledge to be produced. If he intends

to explain the reasons why society, at a given time in its history, created certain legal norms and not

others, his mental work will have a scientific nature. On the other hand, if the objective is to research

the legal decisions that the norm makes possible, his knowledge will have a technological nature. In

the first case, he will have to take into account the alternative between true and false; in the second,

this alternative makes no sense. The question that tends to reveal the reasons for the production of

the legal norm leads to the

Available

[http://portal.mj.gov.br/services/DocumentManagement/FileDownload.EZTSvc.asp?

FERRAZ Jr., Tercio Sampaio. Social function of legal dogmatics, 1998, p. 86.

BASS, Richard F. Stochastic processes. Cambridge: Cambridge University Press, 2011.

36

41

37

38

39

40

35

Machine Translated by Google
42

45

43

44

Right and power. São Paulo: Saraiva, 1992, p. 17.

Many law and social science scholars wonder, 'Are we a science yet?. Because of ELS, the answer is

'yes' for L&E. The maturation of L&E into normal science is intoxicating. However, the peripheral of

L&E on law's content is sobering. To make ELS and L&E central to law's content, scholars must show

that correct legal reasoning of requires scientific prediction of law's effects. If judges become

convinced that law's content depends on its effects, then understanding legal science will become

necessary to pass the bar exam. The correct interpretation of law has always depended significantly

on its consequences. People make laws for their own benefit, so the benefits of alternative

interpretations of a law help to determine which interpretation is correct.

The next task of ELS is to make the correct interpretation of law depend significantly on its scientific

consequences, not merely on its intuitive consequences". COOTER, Robert. Maturing into normal

science: the effect of empirical legal studies on Law and economics. University of Illinois Law Review.

vol. 5, 2011, p. 1475.

SABINE, George H. Descriptive and normative sciences. The philosophical review, vol. 21, no. 4, 1912,

p. 438-450.

Commenting on the effects of empirical studies (ELS) on the economic analysis of law, Robert Cooter

explains how advances in statistical research will allow a better understanding of the consequences

of the law and, therefore, the consolidation of normal scientific knowledge, that is, basic knowledge.

empirical and predictive: "Kuhn distinguished between normal and revolutionary science. Normal

science proceeds by incremental improvements. Hypothesis are deduced from current theory and

then tested empirically - a process that is similar to sequencing a gene. Confirmation or disconfirmation

prompts small adjustments in the theory. As normal science proceeds, anomalies accumulate.

Resolving the anomalies requires a new theory at the science's core. Revolutionary science proceeds

by abrupt jumps that rearrange the core's elements into an unfamiliar pattern, as with postulating the

double helix in genetics.

FRIEDMAN, Milton. Essays in positive economics. Chicago: University of Chicago Press, 1953, p. 5.

MOREIRA, José Carlos Barbosa. Civil procedure matters. São Paulo: Saraiva, 2004, p. 10-11.

(...)

knowing subject to a scientific enterprise, while the inquiry about the meaning of just the same norm

leads him to a technological enterprise. Between one and another level there is no hierarchy or

opposition, since there are different objectives to be achieved" COELHO, Fábio Ulhoa.

Machine Translated by Google
The dilemma appears in the Platonic dialogue Euthyphro, when Socrates asks the namesake
of the title: is piety loved by the gods because it is piety, or is it piety because it is loved by
the gods?

Thrasymachus, Greek philosopher from Chalcedon, states in Plato's Republic that justice
is just a name given to the will of the strongest.

© of this issue [2016]

47

46

Machine Translated by Google
It's science, just as a pile of bricks is not a house."

Jurimetry starts from theories about the functioning of the legal order to

formulate its working hypotheses. In the construction of these theories, concepts specific to
legal dogmatics, general theory of law and legal logic are used, to name a few examples, without
which there would be no way to delimit the object of study and articulate basic concepts for the
formulation of a questioning. .

With these premises in mind, we can say that Jurimetry seeks to develop a set of
generalizations capable of explaining and predicting the behavior of the agents involved in the
production and fulfillment of legal norms, whose performance will be positive insofar as these
generalizations are able to predict the states futures of these

In addition to conceptualizing the object and methodology of Jurimetry, it is important to
provide an additional explanation regarding the objectives of this new discipline, as well as some
statements about the type of approach that, it is believed, will make its realization possible.

"Science is made of facts like a house is made of bricks. But a pile of facts is not

For this reason, despite being deductive or abductive, the concepts of legal logic and the
statements of dogmatic doctrine are not dispensable for Jurimetry. Quite the contrary, they
constitute a reference to practical problems faced by operators, in addition to being a rich and
abundant work material for the elaboration of hypotheses to be tested.

One of the objectives of Jurimetria is to build a theoretical model regarding the functioning of
the legal order, which depends on the formulation of working hypotheses able to be measured
through statistical tests. The creation of the theoretical model of Jurimetry, therefore, must start
from a set of hypotheses about the factors involved both in the production of legal norms by the
authorities and in the effects of these norms on the social behavior of people in general, and
capable of being empirically tested.

Contrary to the more radical strands of American realism, and approaching Scandinavian
realism, Jurimetry does not rule out law and doctrine as factors that influence the decision of the
courts (influence of the order on coordination). The abstract meaning built by the doctrine around
the law is not seen as a myth devoid of any relevance, composing a group of factors that,
although insufficient to determine, is capable of influencing the behavior of judges. In addition,
doctrine is an important repository of working hypotheses from which jurimetric research can be
designed. There is, therefore, no opposition, but a complementarity between dogmatic doctrinal
work and jurimetric research.

Research projects, also empirical, depend on a theoretical model from which the hypotheses
to be tested will be formulated. The failure to formulate this model was Lee Loevinger's biggest
mistake and is, even today, the biggest source of criticism of interdisciplinary empirical
approaches.

Chapter 6. Characteristics of Jurimetrics

Jurimetry
CHAPTER 6. CHARACTERISTICS OF JURIMETRY

2018 - 07 - 17

I. Theoretical model

3

1

two

Machine Translated by Google
abstract

stochastic

Qualitative

concrete

Deterministic

Comparative table

Foresight

Populational

Jurimetry

Perspective

Individual

dogmatic

Quantitative

4

behaviors. Jurimetry will only be successful if it is capable of disseminating knowledge
that can, for example, identify the factors involved in reducing the time of the judicial
process and assist in the creation of a set of norms that effectively reduce the period of
judgment of actions; or that makes it possible to describe the optimal level of a prison
sentence capable of repressing criminal behavior without making the social reintegration
of the criminal unfeasible, reducing crime rates and increasing prison recovery rates.

Dogmatics has five characteristics. It is: deterministic, since it makes assertions that
are supposedly certain; individual, insofar as it takes isolated norms as its object (usually
general norms); abstract, because its object is not situated in time and space; perspective,
because it uses methods that only allow the description of present states; and qualitative,
since it attributes immeasurable qualities to its object.

To better organize this distinction, the comparative table below contrasts the
characteristics of Jurimetry with those of dogmatic disciplines.

Each of these five defining traits will be discussed separately in the topics below.

For supporters of scientific determinism, knowledge about the initial state of a system
and its transformation laws makes it possible to predict, with any degree of precision, all
of its future states. Predictability is based on the existence of relationships of

On the other hand, Jurimetrics differs from dogmatics because it is: stochastic, since it
admits the presence of uncertainty in legal decision-making processes; populational,
since it takes as its object not isolated individuals, but groups, samples, subpopulations
and populations; concrete, as it situates its object in time and space; prospective, as it
uses methods that allow the formulation of predictions about future states; and
quantitative, since it proposes to measure its objects, attributing to them characteristics
endowed with magnitude and multitude.

An enlightening way to define Jurimetrics is to compare it with the dogmatic disciplines
from which universities organize their curriculum and the Brazilian Bar Association bases
its professional selection test for lawyers. Traditional disciplines are defined here as
dogmatic because: (i) they are based on binding premises - legal dogmas (constitution
of codes and main federal laws); (ii) such dogmas cannot be denied; and (iii) play a role
of decidability, indicating how concrete conflicts should be resolved according to dogmas.

II. Comparative table

III. Dealing with uncertainty

Machine Translated by Google
10

9
7
8
to the freedom of

In order for there to be certainty that B is the legal consequence of A, the legal system must
unequivocally indicate this relationship, and cannot leave doubts as to the existence of other consequences
for the same conduct A. Hence the statement that the legal system would be organized in the form of a
consistent system, devoid of antinomies and gaps and capable of assigning, in advance, a single legal
consequence to each possible social conduct.

Thus, for example, legal science does not state that if someone commits a crime (A), then the criminal will
necessarily be arrested (B). It only attests that, according to current norms, the criminal must be arrested:
if A is, then B must be.

5 chance.

decision of the Judiciary which, not coincidentally, constitute the three main sources of law. For this
reason, the search to minimize this complexity and its consequent uncertainty through the promotion of
legal certainty becomes a paradox in which the law faces a battle against itself. By trying to predict a
solution in advance for each possible conduct, the order becomes a jungle of rules that are difficult to
operate. The jurist stops being an interpreter and becomes a detective involved in a complex tangle of
rules, which paradoxically end up increasing, not reducing, the predictability of decisions.

It is easy to see that the problem of anticipating the legal consequences of a fact is directly linked to
the social aspiration for legal security. If there is a determination, there would also be the possibility of
absolute legal certainty, insofar as the entire normative content of the Law would be previously established
in the general norm and the application would be an act of mere revelation of this content, immune to
subjectivism, political disturbances and the

causality - if A is, then B will be
certain of the occurrence of effect B.

This indeterminacy arises, in part, from the structure of law itself, especially from the plurivocity of
general norms and the political action of the Judiciary. General norms, in addition to being numerous and
inevitably plurivocal, are often contradictory and meaningless. Within this space of normative
indetermination, judges exercise powers to arbitrate and construct solutions that are not entirely
predetermined in the law.

Even if it were possible, the effort to automate justice by predicting all concrete cases would not make
sense. Conflicts that are less frequent and have a low social impact (economic or ethical) do not justify
the effort to move the company in advance.

Jurimetria refutes the "predeterministic" view of law and offers a stochastic view as an alternative. The
stochastic view understands that, for systems of greater complexity, with numerous factors and complex
interaction mechanisms, the exact predetermination of future states is unfeasible. This is undoubtedly the
case of social facts in general and, in particular, of legal facts. The infinity of factors that interact in the
functioning of the legal order prevents the formulation of models capable of accurately predicting which
norms will be produced in the future and which legal consequences will be attributed to each behavior.

-,

Humberto Ávila attributes the complexity of the law to the plurality of norms, and the
vagueness of the doctrine

Juridical dogmatics also operates affirmations equipped with certainty from an initial state. The initial
state is a consistent system of general and abstract norms whose knowledge allows the deduction of a
single legal consequence for each fact. With the difference that the relationship between a fact and its
legal consequence is not causal in nature, but is an imputation between antecedent and consequent
located in the world of what should be.
in such a way that, if cause A occurs, the researcher will have

Judging implies creating law and the idea of a consistent set of abstract norms capable of attributing a
single solution to each and every conflict under jurisdiction is an unrealizable aspiration .

Machine Translated by Google
11

12

IV. large populations

Due to these difficulties, Jurimetria admits that decision-making processes in law are
stochastic. The stochastic process is composed of a family of random variables (that is,
variables whose values cannot be anticipated) indexed in time. While the deterministic
process evolves in only one direction and has only one possible outcome, the stochastic
process evolves in different ways and is capable of producing a range of possible outcomes.
Law is stochastic because, as all operators know, it is not possible to predict with certainty
how a judicial process will be judged, what will be the final conformation of a contract or
what will be the final text of a law under discussion in parliament.

To become fully jurimetric, Ávila's explanation would only need to inform that the meaning
of a rule has the behavior of a random variable and that its possible meanings are equivalent
to the sample space of this variable. Having made this clarification, Jurimetria can take the
analysis a step further and, in addition to identifying the possible meanings of the rules,
assign a probability of occurrence to each one of them (for example, based on the frequency
distribution of judicial decisions), quantifying and controlling this uncertainty.

Assigning a probability to each possible result of a legal process is the most concrete,
practical and palpable way to give effect to the principle of legal certainty and, with that,
accomplish what Jurimetry proposes as a cornerstone: controlling uncertainty in the
process. right, since it cannot be extirpated.

The jurisprudence of the courts, when it appears, is occasionally referred to

as an allegory of a general position regarding a theoretical issue addressed in the

There is, therefore, an unavoidable randomness in these processes and their results are
not subject to exact predetermination. It is once again Humberto Ávila who explains how
legal rules do not produce univocal meanings, but rather ranges of possible meanings to be
assumed according to the evolution of the decision process, so that the law can only
produce a "relative certainty", dependent on postulates of interpretation and application, such as

To conclude, it should be clarified that the term stochastic should not be confused with
irrational or chaotic. Saying that law is stochastic means leaving aside predeterministic
presumptions to recognize the highly complex functioning of the legal order, motivated by
causes so numerous that any pretense of exact predetermination of its results is beyond
reach. It also means recognizing that the production of law contains the imponderable
element of freedom given to judges, legislators and negotiators to form their judgments, a
freedom that would ultimately be incompatible with the existence of direct relationships of
implication between values, facts and norms, on the one hand, and the functioning of order,
on the other. And it means, finally, admitting with courageous humility that this interaction,
at least in our current stage of knowledge, can only be the object of a study by approximation
and of inferential knowledge, in which uncertainty is embedded.

proportionality, coherence and reasonableness.

legislative machine for discussion and approval of a law. Laws should only regulate the
most relevant conflicts, leaving the decision of cases of reduced incidence to the exclusive
discretion of judges. The order also provides for this possibility when it states that the
absence of law does not exempt the magistrate from judging the case based on principles,
usages and customs and that, therefore, it is the judge's obligation to fill in the gaps and
hand down sentences for all concrete conflicts that arise. are presented to you.

Dogmatic disciplines study law through the individual interpretation of general norms. In
dogmatics, each norm represents a solution for a set of concrete cases and understanding
its meaning, what Vilanova calls the semantic irradiation field of the norm, would be
sufficient for its application in all conflicts subsumed under its hypothesis.

Machine Translated by Google
13

14

15

Jurimetry, on the contrary, leaves aside the isolated study of general norms and places
the concrete plan as the central object of interest. If, on the abstract level, each general norm
refers to a specific factual reality and, therefore, to a very unique conceptual universe,
capable of supporting an entire theory alone, on the concrete level, large populations of
facts and individual norms travel, which share collective characteristics among themselves. .
For this reason, for Jurimetria, the life of law is in the large populations of conflicts,
processes and norms that are born, migrate and die in the daily life of institutions. It is these
populations that populate the law and are responsible for its success and failure.

That is, it is not enough to study each individual in isolation to

understand the properties of the population. The set must be taken as an autonomous object
of interest and studied through a specific methodology capable of describing its
characteristics and understanding its internal relationships. Using an example from physics,
the states of matter are properties that are only revealed in aggregates and are qualities that
cannot be applied individually to each particle. Molecules are not themselves solid, liquid or
gaseous; Only matter assumes these states.

Transplanting the problem to law, even if a large number of isolated studies accumulate
regarding articles of law, sentences and processes, the results of these intentionally chosen
studies cannot be extrapolated to statements regarding the legal order. A general view of
the functioning of the order will only be achieved through a study of collective behavior and
the relationships established between parties, judges, mediators, requests, arguments,
evidence and guarantees, not as isolated individuals (each case is different), but as elements
of a population. It is from the set of these individual trajectories that the movement vectors
of the courts will result.

The proposal of Jurimetria is, therefore, to study the legal order not through isolated
individuals, but by observing the behavior of populations, the general characteristics of
conflict groups and the movement flows they describe.

For example, the CNJ's 2010 Justice in Numbers program report identified that, of the
83.4 million cases then underway in Brazil, approximately 27 million were tax foreclosures.

An important aspect, which is also related to the concept of social fact and the foundations
of the statistical revolution, concerns the discontinuity between the individual level and the
population level. Despite being made up of individuals, populations behave differently,
responding to factors different from those that motivate the behavior of each member. In a
sentence: the whole is greater than the sum of the parts. A collection action, for example,
may be motivated by the bad faith of a debtor who refuses to pay and hides his assets.
However, a significant increase in the number of collection actions may result from an
economic crisis directly related to the movement of other indicators, such as the
unemployment rate or the slowdown in the Gross Domestic Product (GDP).

The disclosure of these data alarmed the legal community, especially the

With the help of the Institute of Economic and Applied Research (IPEA),

in a second survey focused on the Federal Justice, the CNJ discovered that tax executions
had an estimated cost for the judicial machine of R$ 4,368.00 (four thousand three hundred
and sixty-eight reais) per process and that, contrary to the idea that the Federal Union would
be by far the biggest litigant, the liberal professions councils accounted for 36.4% of
executions. These numbers became even more alarming when the research showed that
council executions were proposed to recover, on average, just R$ 1,540.00 (one thousand,
five hundred and forty reais) per case.

According to Ilya Prigogine, the distinguishing mark of the statistical revolution is this
"break in equivalence between the individual description (trajectories, wave functions) and
the statistical description of sets".

doctrine. With this, the basis of legal education is conceptual and theoretical: the abstract
concepts of the law and the theories used for its revelation are studied.

Machine Translated by Google
16

V. Everything in its time and place

This preference for studying extraordinary cases leads to the so-called platypus
problem, which can be defined as follows: individually considered, the platypus is a
fascinating animal due to several characteristics, among which is that it is a monotreme,
that is, an oviparous mammal . The platypus so dazzled English naturalists that the first
stuffed specimen sent to London was considered a forgery. But despite being
extraordinary in isolation, the platypus is numerically negligible and its endemic
population only inhabits the eastern coast of Australia. Thus, for those who want to
understand how the earth's ecosystem works, platypuses are negligible.

What do “cause harm” and “harm” mean? What is done in dogmatics is to operate an
effort of conceptual taxonomy in which these ideal objects are defined and then classified
according to other previously established conceptual categories, without any reference

Freely comparing, traditional jurists are like biologists obsessed with legal platypuses.
Instead of worrying about studying the numerous cases that populate the courts,
researchers go in search of rare cases that defy traditional taxonomy. Petty crimes,
special courts and legal gratuity are neglected in favor of eccentric analyzes focused on
themes that are not very familiar with the daily life of the Brazilian legal order. For
example, the bank of theses and dissertations at USP.

This example shows how well-designed quantitative studies can diagnose

Hence the insistence of Jurimetria on the study of common cases. The emphasis on
the study of populations proposes that jurists stop giving so much emphasis to legal
platypuses and start studying common cases, such as traffic accidents, medical errors,
moral damage due to denial and many others, which perhaps are even less interesting in
one sense. individual point of view, but which together make up the large populations
that explain the functioning and populate the legal order.

real problems faced in the courts.

Law theses are usually dedicated to hermeneutical disputes, not always related to the
actual problems faced in the courts. These same theses give preference to the isolated
analysis of peculiar cases and are little concerned with studying large populations of
cases. A peculiar case may be individually more interesting than another common and
ordinary one. However, it is the common cases (low-value executions, traffic accidents,
medical errors, undue protests, indemnities for moral damage) that move the courts, and
only through the study of the migrations and movements of these populations is it
possible to that we will be able to understand how the legal order works.

professional councils, and motivated a series of political measures to improve the
management of processes. A minimum value for proposing executions began to be
discussed and the councils, fearing the revocation of their benefit, began a movement to
withdraw and As a consequence, Law 12,514/2011 was enacted, which regulates their
executions. limited the legal collection by professional councils to a minimum equivalent
to four times the value of the due annuity.

When studied through traditional dogmatics, law is reduced to deductive or abductive
formulations located outside any system of spatial and temporal coordinates. Take, for
example, the concept of subjective civil liability: anyone who by intent or negligence
causes damage to another is obliged to indemnify the damage. The dogmatic definition
of civil liability is presented within a theoretical framework in which the meaning of each
element is decomposed and analyzed separately. Who can be "the one" and "the other"?

Machine Translated by Google
17

The categories of legal logic are another example of the difference between abstraction
and concreteness. Logical forms are accessed by merely intellective effort and are
objects located outside of time and space. In the formulation of the legal norm as
imputation of a legal relationship to a fact [D {F -> (A Op B)], the variables do not refer to
a moment or a place and are not located within a system of coordinates. This is because
they were conceived to represent all the values that, at different times and locations, can
be assumed by the variable. Only the values that these variables take are temporal and
spatial.

An additional clarification is important. The fact that a given doctrinal or jurisprudential
position is associated with a historical moment does not make it concrete. The
concreteness of a study comes from the use of spatial and temporal coordinate systems
as part of the analysis method. When a work mentions, for example, that legal realism
was born in the United States, in 1930, but develops an analysis based only on the
theoretical value of this current, it is still abstract. The reference to the moment and
location of this current of thought is a mere exercise in erudition that does not integrate
the premises and assumptions of the analysis. If we remove the mention of work, its
rationality remains intact.

For the same reason, the fact that comparative law confronts legal positions in other
jurisdictions, or that doctrine occasionally refers to changes in jurisprudential
understanding regarding a given subject, does not make these analyzes concrete. In the
case of comparative law, the reference to understandings is not an integral part of the
study, but only an allegorical mention of the conceptual value of each theory. It is usually
an argument from authority, which seeks to assert the superiority of a position because
it is adopted by a foreign jurist or by the legislation of a developed country.

Jurimetrics studies the legal order as a concrete object, composed of objects located
within spatial and temporal coordinate systems. The characteristics of the order are
therefore accessed through empirical efforts and, furthermore, correspond to concrete
legal situations, here defined as situations whose moment and position can be identified.
Locating an object in time corresponds to the possibility of, within a chronological line
organized in order of precedence, (i) identifying the moment of occurrence of the object
and (ii) calculating the time interval between the occurrence of the object and other
points on the line . Locating an object in space corresponds to the possibility of, within
an area organized in a coordinate system, (i) identifying the position of the object and (ii)
calculating the distance between the object and other points in the area.

And in the case of jurisprudence, what is seen in traditional doctrine is not a
discussion about the concrete changes in the understanding of the courts over time, but
just an illustration of the conceptual evolution of those judged towards a position
considered superior from the point of view. theoretical. Here, the use of a methodology
capable of associating time and space with the characteristics of decisions, which are
intentionally selected to reinforce the author's theoretical proposal, is also not identified.

Contrary to dogmatics, the object of Jurimetry accesses the reality of law by taking
into account the concrete values assumed by variables in different places and times.
Thus, for example, while the traditional study of civil liability seeks to explain this concept
by discussing the theoretical meaning of each particle of the definition, a legal study on
the same topic verifies the evolution of jurisprudence in Brazilian state courts between
1997 and 2011. Unlike the generic concept of civil liability, each trial is situated at a time
and place, which allows observing regional differences between cases or even analyzing
their evolution over the years.

of time and place.

Machine Translated by Google
The more the determinist knows and the closer he is to the truth, the less time and space matter
to him. For the infinitely wise Laplacian demon, there is no uncertainty and, therefore, there is
also no time and space, which present themselves simultaneously to his eyes. Omniscience
implies omnipresence.

SAW. See the future

The first objective of a science is to make accurate predictions. Science tries to understand
how a system works in order to, aware of its present state and its laws of transformation,
anticipate what future states will be like. This knowledge will serve as the basis for several
technological applications, in which future states will be manipulated according to the operators'
convenience. Likewise, the first objective of any researcher is to develop knowledge that allows
for greater control over reality. Knowing something under a scientific approach means
understanding the factors that shape this object with the necessary depth to control its future
behavior.

When doctrine presents a description of the norm (positive assertion), it is at the same time
saying what the law should be (normative assertion).

On the other hand, indeterminists do not believe in the possibility of absolute knowledge and
live with a degree of uncertainty, resulting from the structure of the world itself, which is constantly
evolving. The universe is stochastic and can evolve in more than one direction. And as the future
is not predetermined, it is the passage of time that develops situations and defines how things
will be. Therefore, given this impossibility of witnessing all objects, in all places and at all times,
the most we can do is observe as many situations as possible and wait for time to reveal other
experiences. Henri Bergson, quoted by Ilya Prigogine, explains in an almost poetic way this
relationship between uncertainty, time and reality: "What is the use of time? (omissis) time is
what prevents everything from being given at once. It delays, or rather, it is the delay. It must,
therefore, be elaboration. Wouldn't it be, then, the vehicle of creation and choice? Wouldn't the
existence of time prove that there is a certain indeterminacy in things?".

The dogmatic disciplines of law are perspectives because they do not lend themselves to
making predictions about the future behavior of the legal order. This legal perspective stems in
part from the ideality of the objects of the dogmatic approach which, outside of time and space,
does not include analyzes of evolutions and transformations. Analyzed abstractly, the concept
of subjective civil liability is timeless and is not subject to transformation processes.

18

It is not by chance that dogmatic studies on this topic, as on all other legal topics, never present
analyzes regarding trends or projections of future transformation.

In this context, jurimetric research is nothing more than a summary of the past, a way of
compressing time and space with the aim of enabling the analysis of cases that occurred in
distant places and times. It is, therefore, an effort to compress observations to optimize our
accumulated experience and, with that, expand the limits of our learning. With a jurimetric
research, a newly formed lawyer or judge can observe in a few days the behavior of tens of
thousands of processes, a set greater than the accumulated experience of dozens of lawyers
throughout a lifetime.

One last epistemological comment. Within the deterministic perspective, time and space are
illusions. They are, moreover, illusions proportional to our degree of ignorance.

Furthermore, legal dogmatism merges the positive and normative aspects of Law into a single
state. When the study of Law is restricted to the abstract plane, the statement of what the general
norm is simultaneously implies the statement of what it should be. Simultaneity breaks the time
factor, preventing any type of prospective judgment independent of the descriptive judgment.

Machine Translated by Google
19

21

Humberto Ávila defines the capacity for approximate control over the future effects of
Law as calculability. Calculability concerns an ideal state in which the citizen has an
approximate capacity to anticipate today the effects that will be attributed to him by the Law
tomorrow, reducing and controlling the spectrum of legal consequences that could be 20
Humberto Ávila's position is impeccable , fitting only one attributed to his conduct. single
comment. Considering that the expression calculability was used by Karl Popper in the
opposite sense to that proposed by Humberto Ávila, Jurimetrics chose to reserve its
Popperian meaning for this expression and refer to the ability to predict and control legal
uncertainty as the prospectiveness of Law .

We also saw that, because they are stochastic, legal processes do not evolve in a single
direction. The use of Statistical inference to investigate the relationships between past and
present states with the possible future states of a process is the essence of prospective
analyzes in Jurimetrics. Its temporal dynamics always point to two or more possible
outcomes, corresponding to a probability distribution. Understanding how the behaviors of
variables affect each other and what possible results will arise from this relationship allows
us to understand the uncertainty inherent in these processes and, to some extent, control it.

Jurimetry, on the other hand, addresses the legal order as a concrete object, with a past
and a present, and therefore it is possible to anticipate its future behavior. Still in the
example of civil liability, when analyzing the results of judgments in state courts handed
down between 1997 and today, it is possible to carry out analyzes aimed at detecting
possible trends in future behavior. Is the number of moral damages convictions growing?
Is this growth associated with any other indicator, such as the HDI or GDP per capita? Based
on this association, is it possible to predict the number of moral damages convictions that
will be handed down next year?

Modern society demands greater training and demands results from social managers,
including legal professionals. These results depend on predictions regarding the effects
that new laws, legal strategies and judicial decisions will produce in reality. Operators can
no longer propose legislative reforms based on intuition, they can no longer put into practice
legal strategies based on idiosyncrasies and they can no longer manage the courts
responding to merely political incentives. The consequences of legal interventions in society
need to be known, even if approximate, before decisions are made.

One last point must be clarified. Prospectivity is based on the idea, already discussed, of
probabilistic causation, distinct from deterministic causation and the implication relation. In
Law, concrete relationships are of probabilistic causality. Not everyone who commits a
crime is penalized, but committing criminal conduct increases the likelihood of being
penalized. Not everyone who adopts a certain strategy wins the process, but its adoption
increases the probability of success. Likewise, a majority interpretation of the law is not
capable of determining the meaning of a judicial decision, but its existence increases the
probability of a decision that accompanies it.

Combating uncertainty in Law through reasonably accurate predictions is the job of an
operator. The lawyer needs to know how a case will be judged and how the judge will react
to different types of arguments. The judge needs to foresee which social effects his sentence
will produce, in order to decide according to the consequences that seem most appropriate
to him. The politician needs to foresee the consequences of his bills, in order to adapt his
legislative policy to the demands of his voters. The idea is simple and prosaic, but powerful:
we have to understand the practical consequences of our decisions. All the decisions of a
Law operator in the exercise of his office are taken in the present, but are always based on
an intuition regarding the effects that it will produce in the future.

Machine Translated by Google
The concepts of magnitude and multitude refer to the definition of density. Density is an
indicator that expresses the relationship between two measurements, usually a multitude and a
magnitude , indicating the concentration of one quantity within another. For example,
demographic density is the relationship between the number of people within a territory,
generally expressed in inhabitants per km2, and volumetric density is the relationship
between a quantity of mass and the volume of a body. In Jurimetry we can speak, for
example, of inmate density (ratio between the number of inmates by the area of internment)
and procedural density (ratio between the number of cases and the area of courts).

The dogmatics of Law are based on the definition of the semantic limits of norms and
the rhetorical persuasion of society regarding their valid, true or useful meanings.
It is, therefore, an essentially rhetorical approach, in which the values of Law and the
characteristics of the legal order are presented in a discursive way and are not subject to
measurement.

and for its methodology.
The first and most elementary function of a science is to quantify its object, measuring
its main characteristics. Jurimetrics is quantitative from two angles: due to its objectives. It
is objectively quantitative because it is capable of attributing to the
legal order characteristics endowed with magnitude and multitude. Magnitude is the
measurement of the extension of an object. The length of a ruler, for example, expresses a
magnitude, because there are infinite points along its length that can divide it without it ever
being reduced to a fundamental unit. Magnitude is usually linked to continuous variables
such as weight, height or volume.

On the other hand, Jurimetrics is methodologically quantitative, distinguishing itself
from other so-called qualitative approaches. The distinction between quantitative and
qualitative research is an intricate subject, but for didactic purposes we can define
quantitative research as that based fundamentally on the use of statistical inferences. For
example, an academic who decides to understand how state court notaries work in Brazil
has two ways of planning his work. On the one hand, he can intentionally choose a few
registry offices, say ten, and designate a team of researchers to visit them for a month and
interview their employees, judges and users. This researcher can also occupy a function in
the registry office for a few days, hold discussion groups among employees to deepen
certain issues, including the perception of employees in relation to the main problems
experienced. On the other hand, the academic can identify the 27 databases of the Brazilian
state courts and, with the help of a data scientist and a statistician, extract information in
the last decade on the evolution of the number of employees, the value of salaries, the
volume of processes, the length of time the records remain in the registry office and other
quantifiable information available in the database.

Multitude consists of counting the number of occurrences of a unit. The set of appeals
judged by a court over the course of a year expresses a multitude, because there is a finite
number of cases that can be counted. The multitude is discontinuous, can be divided up to
the limit of a fundamental unit and is associated with discrete variables, such as the number
of people in a city and the number of cars stuck in traffic jams.

The first research is qualitative. Through it, the researcher will be able to delve into
several issues that are not documented in databases and will build from interviews a very
in-depth view of the day to day of a registry office. The limitation is that, as the choice of
registry offices did not respect a sampling plan, the research conclusions cannot be
expanded to all registry offices in Brazil. The second research is quantitative. It is
undoubtedly limited in relation to the number of questions that can be answered, since, in
the example, the works were restricted to the information available in the database. However,
its conclusions are valid for all registry offices in the country,

22

VII. Right to measure

Machine Translated by Google
26

28

23

27

32

29

33

24

30

25

31

VIII. The principles and their ends

Third, quantitative research makes it possible to associate magnitudes and thus identify
association and causality relationships, which allow prediction (and therefore control) over
the functioning of the legal order.

Second, quantitative research enables statistical inferences about the general
characteristics of the population, allowing access to a view of the legal order in all its extension.

On the other hand, qualitative research is linked to other techniques, such as focus

groups, capable of exposing the researcher to aspects of cultural experience and
experimentation not apprehensible by the "quantitative ruler", but which can add knowledge
about the characteristics of objects devoid of magnitude and multitude.

the ethnography,

on Law based on statistical analysis.

and can serve as a foundation for the development of large-scale public policies.

and conducting in-depth interviews

While ELS is a

grounded approach that tests positive theories about the functioning of law through
quantitative methods, NLR is defined as an approach based on a methodological eclecticism
that embraces qualitative and quantitative work, that is, not only statistical analysis, but also
observation. participant and interviews.

Despite not being aimed at resolving the evaluative issues of law, Jurimetrics

The emphasis on the use of statistical methods, on the one hand, and an empirical
eclecticism, on the other, is manifested, in the United States, in the differences between,
respectively, the Empirical Legal Studies (ELS) movement and the New Legal Realism movement ( NLR).

the projective techniques

It is important to be clear that quantitative research is not restricted to available databases
and that it comprises steps of collecting information in the field. Its distinction lies essentially
in the use of statistical inference to validate and expand conclusions, which gives it some
relevant differentials.

While open proposals are presented as spaces of methodological diversity, Jurimetria
proposes a specialized quantitative methodology, based on statistical hypothesis tests and
aimed at building a rigorous and coherent set of generalizations

First, the possibility of measurement generates knowledge about the size of the problems,
the proportionality of investments to solve them and the expected time for resolution.
Measuring the legal order is essential for the administration of justice.

Following the example, there would also be the possibility of mixed research. The
academic could carry out an initial qualitative exploration, which would help him understand
the day-to-day life of the ten initial registry offices. Based on these preliminary results, a main
questionnaire would be developed and a large quantitative survey would be carried out to
apply it to a national sample of registry offices generated through rigorous statistical
planning. As a result, we would have a broad and deep survey at the same time, which would
account not only for the information available in the databases of the 27 courts, but also for
new information collected in the field. There is only one restriction on this type of work:
financial resources. Research at a national level in a heterogeneous country with continental
dimensions like Brazil is extremely expensive. It is necessary to hire numerous teams of
researchers, who will incur transportation, food and accommodation expenses to reach the
most remote registry offices that make up the sample.

In Brazil, this same distinction appears, for example, between open proposals for empirical
research, such as the Network of Empirical Studies in Law (REED) and Jurimetry.

Machine Translated by Google
35

36
34

37

The principle of celerity is defined by the doctrine as a guideline for the procedural law to offer
solutions to simplify and speed up the process and give it a reasonable duration. For example, for
Ada Pelegrini Grinover, procedural celerity is "making the procedural legislation offer skilful
solutions to reducing bureaucracy and simplifying the For Cassio Scarpinella Bueno, procedural
celerity "must be understood as a process". reasonable duration of the process' - and there
is no harm in enunciating this guideline as the 'principle of the reasonable duration of the process'
or, to avoid repetition with the text used by the norm under examination, the principle of the
timeliness of the judicial protection"

speed could be calculated and, what is fundamental, what the magnitude of this measurement would be.

.
In José Afonso da Silva's classification, procedural celerity would be a special constitutional
guarantee, given that it confers "the holders of fundamental rights, means, techniques, instruments
or procedures to impose respect and enforceability of their rights".

Measuring the speed of the process makes it possible to investigate the causes of its
acceleration or deceleration, among which what we call "institutional friction" and "procedural
viscosity". Institutional friction corresponds to the set of external factors capable of offering
resistance to procedural progress, such as, for example, the efficiency of the notary. The use of
the virtual process is one of the most effective policies for reducing institutional attrition, by
eliminating steps such as assessment, numbering of sheets, transport and loading of records.

Despite the consideration I have for these authors, we are faced with a series of tautological
definitions, in which the word celerity is replaced by equivalent expressions such as reducing
bureaucracy, simplification, reasonable duration of the process or timeliness of the guardianship.
Although the idea of a speedy process is an understandable aspiration, the formulation worked on
by legal theory is generic and leaves open the essential question of the problem, which is
surprising because it has not been faced until today. How can we measure the speed of legal
action?

The graph below illustrates two physical processes that took place at the Penha Regional
Forum, in São Paulo. The succession of points that revisit the remittance and receipt movements
of the MP and Police District in the case of Process 1 (represented in pink) illustrate points of
institutional friction. This process took around 200 days to reach the sentence.

Starting from the premise that the "speed" attribute is applicable to the process, it is reasonable
to assume that a lawsuit has a maximum speed (to avoid it being too fast and restricting the
exercise of the right of defense, for example), a minimum speed (to prevent it from stretching too
far and causing the right to perish) and an expected speed (calculated based on its complexity and
the characteristics of the court and the notary where it runs). Surprisingly, there is no news of
doctrinal studies on how this

has a lot to contribute to the great political controversies of Law. Some examples related to general
principles are capable of showing how Jurimetry is able to attribute concreteness and give
practical operability to concepts that were previously restricted to the abstract plane of legal
theory.

Machine Translated by Google
Figure 1: Procedural flow of two processes processed at the Penha Regional Forum, in
São Paulo, capital.

Another interesting concept is that of viscosity. Procedural viscosity can be defined as
the set of structural characteristics of a process, capable of affecting its speed. Continuing
the analogy with fluids, if an observer separates two glasses, one filled with honey and
the other with water, and turns them simultaneously upside down, the water will fall faster
than the honey. The greater speed of water does not result from the resistance offered by
an external obstacle to its displacement, but from differences in the intimate structure of
each substance: honey is viscous and moves more slowly than water, which is more fluid.

Following the analogy, also some processes are more viscous than others. Cases
involving complex matters, multiple parties or the production of elaborate technical
evidence have a more complex intimate structure and tend to proceed more slowly than
simple, two-part cases involving the production of documentary evidence only. This
internal complexity is what we call procedural viscosity, and its measurement is
fundamental to manage the workload and goals of justice officials, such as, for example,
in the creation of rules to weight the distribution of resources for the reserved chambers.

If we add the time intervals elapsed between the steps related to notary administration
activities until the write-off, we arrive at an approximate value of the total time wasted in
the process as a result of institutional attrition. In the case of the first process, this time
is 207 days (within a total duration of 241), and in the case of the second, it is 118 days
(within a total duration of 558), which corresponds to a relative loss of 86% and 21%,
respectively. These are high percentages. The expansion of a study of this type to a
sufficient number of processes and districts in the three branches of the Judiciary, as well
as the comparison with groups of virtual processes, will make it possible to calculate the
expected acceleration of judgments after the implementation of the electronic process.

The first reserved chamber of business law was created by the Special Body of the
Court of Justice of São Paulo through Resolution 538/2011. It was born as a fractional
body, specialized in judging issues related to business law, with the declared objective of
giving the judgment of these appeals greater agility, legal certainty and uniformity.
According to art. 2 of the Resolution, the chamber is composed of justices and alternates,
who will act without prejudice to their attributions in the chambers, subsections and
sections of origin, with compensation in the distribution of facts. In other words, judges
from the reserved chambers continue to work in the common chambers and are called
only when a new case of specialized matter comes in. Once the specialized appeal has
been distributed, the "one to one" rule determines that the judge, on the other hand, no
longer receives a common appeal.

Machine Translated by Google
For De Plácido e Silva, security "derived

from holding, expresses, grammatically, the action and act of making secure, or of ensuring and
guaranteeing something. (...) Security, whatever its application, insert the

39

For Canotilho, "the idea

of legal certainty leads back to two material principles that materialize the general principle of security: the
principle of determinability of laws expressed in the requirement of clear and dense laws and the principle
of protection of trust, translated in the requirement of laws tends to be stable, or, at least, not harmful to the
predictability and calculability of citizens in relation to their legal effects".
42

38

Another interesting example refers to the principle of legal certainty. Legal certainty is defined as the
principle that guarantees stability and predictability in the legal order. For Celso Antônio Bandeira de Mello,
"the legal order corresponds to a normative framework proposed precisely so that people can guide
themselves, knowing, therefore, in advance, what they should or what they can do, in view of the further
consequences attributable to their acts. The Law proposes to give rise to a certain stability, a minimum of
certainty in the conduct of social life. Hence the so-called principle of 'legal security', which, precisely
because of this, if not the most important among all the principles of Law, is undoubtedly one of the most
important among them".

IX. Legal certainty and judicial discrepancy

The creation of the reserved chamber was one of the great successes of the court's management, even
recognized by the World Bank, but the "one for one" compensation rule created problems. Because they
involve multiple parties, laws that are foreign to the day-to-day of the judiciary, high amounts and specialized
lawyers, business law cases tend to be more viscous than common cases. The study of the corporate
resource implies more extensive research, receiving lawyers in the office for personal dispatch more often,
the elaboration of entirely new votes, without the use of a base draft, and less space to delegate work to
lesser advisors. experienced. In the language of procedural rheology, the business resource is a typical
example of a process with greater viscosity, which takes a lot more work and therefore requires a much
greater personal effort from the judge.

40 For Paulo de Barros Carvalho, legal

certainty is "a specific value, namely that of coordinating the flow of inter-human interactions, in the sense
of propagating within the social community the feeling of

In addition, pressured by the excess of work, some of the judges already sworn in have threatened to leave
the chambers, jeopardizing the continuity of an initiative that is of interest not only to the court, but to the
economy of its largest commercial center and to Brazil.

predictability regarding the legal effects of the regulation of conduct".

The lesson is clear, and a traffic comparison helps to make it clearer. Judicial traffic management
requires elaborate engineering and daily monitoring of flows with well-designed metrics and clear goals.
Just as it is unthinkable to manage traffic in the city of São Paulo, with its 5.5 million vehicles, based on the
concept of reasonable car speed, we will not be able to manage the traffic of lawsuits based on the idea of
reasonable duration of the process . The processes need to have a minimum speed, quantifiable according
to a previously defined magnitude. Excessive slowness must be detected and those responsible, whether
judges, parties or lawyers, must be fined. Procedural pathways need to be planned according to the flow
and time expected for judgment. Expressways, like reserved chambers, need to be sized for the type of
process they will receive.

No wonder, the court realized that, despite the apparent numerical equivalence, the "one to one"
compensation rule hid a serious disproportion in the allocation of the workload. After some time, the
workload of the judges of the reserved chambers increased disproportionately, generating discontent and
a strong incentive to abandon them. The selected judges were worn out, showing signs of stress, fatigue
and health problems. As a result of this situation, despite the work involving important processes and
projection in the media, most judges did not express interest in taking up positions in the reserved chambers.

41

Machine Translated by Google
43

Imbued with the conviction that such a level of discrepancy was bad, the American authorities
approved, in 1984, a sentencing guideline (judicial guidelines), in which the judge was obliged to respond
to an objective questionnaire about the characteristics of the case and that at the end indicated the
penalty to be applied. Research conducted after the reform indicated that the roadmap significantly
reduced judicial discrepancy. However, a discussion began about the constitutionality of this solution.
Critics of the script said that it was no longer the court that was judging the case, but rather the commission
that drafted the script, which would violate the constitutional right to an independent trial.

Twenty years after the screenplay was created, this discussion reached the Supreme Court, in the
case of United States v. Booker. The story is as follows: Freddie Joe Booker was convicted by a jury, for
possession of 92.5 grams of cocaine and intent to distribute the drug, to 1 year and 9 months in prison.
The judge's sentence, however, found that Booker had more cocaine than the jury admitted and, based
on the script, increased the sentence to three years. Booker appealed to the Seventh Circuit Court,
claiming that the sentence, by admitting facts not found by the jury, violated the Sixth Amendment. The
Court, however, found that the script violated the Sixth Amendment and the right to be tried by an
independent jury and overturned the decision. The US Government appealed to the Supreme Court,
which also considered the mandatory script unconstitutional. From this precedent, the roadmap became
a mere non-binding recommendation and the courts were free not to apply its decision parameters.

45

Once again, despite the admiration I have for these authors, we have here another series of circular
definitions, in which the term legal certainty is replaced by equivalent and equally open expressions, such
as stability, minimal certainty, determinability, protection of trust, guarantee of rights and waiver of
damages and losses. Such definitions illustrate what the principle would generally aim for, but they do not
provide references so that we can, in practice, evaluate how far the real situation of our order is from the
proposed ideal situation. What does it mean to say that legal certainty is the protection of trust or the
guarantee of citizens' rights? Not a lot. The truth is that without the ability to articulate practical references,
any definition is inoperative and therefore useless.

44

The concept of legal security takes on a new aspect when, based on the idea of predictability, it is
defined as a situation in which citizens who engage in equal conduct find equal treatment in the legal
system. With this, legal certainty becomes the expression of the variability between different decisions
handed down by judges for similar cases, manifested through measures of dispersion between precedents.
It is, therefore, the guarantee of a standard of punishments capable of providing security by accurately
anticipating the consequences of each conduct and in which, in an ideal limit situation, very similar cases
would always suffer very similar penalties.

Ten years after the script was made more flexible, new research once again assessed the situation of
judicial discrepancy by comparing 600,000 interjudicial cases, with interesting results. The disparity in
decisions doubled after the declaration of unconstitutionality, especially due to the actions of judges
(female) ideologically linked to the Democratic Party. Judges admitted after the declaration of
unconstitutionality were also less anchored to the script.

An interesting example of work on judicial discrepancy involved the sentencing script of the American
Sentencing Reform Act, passed under intense controversy in 1984. In the 1970s, concern arose in the
US about discrepancies in the adjudication of criminal cases. It was discovered, on that occasion, that
similar criminal offenses could suffer the application of sentences whose duration could vary by up to
700%, depending on the judge.

sense of making the thing free from dangers, free from uncertainties, assured of damages or losses,
removed from all evil".

In other words: the script was associated with a reduction

in the discrepancy between penalties and, therefore, with an increase in legal certainty.

Machine Translated by Google
First, Jurimetrics is not a discipline resulting from the application of information technology to Law.
Computer science is an accidental tool and Jurimetrics would exist, albeit at the cost of greater effort,
independently of any computer. The methodology of Jurimetrics is statistical inference and its object is the
functioning of the legal order, with the computer being just an instrument capable of accumulating data and
expanding the researchers' calculation power. It is undeniable that advances in computing have allowed a
growth in legal studies as a result of easier access to data and greater calculation power. However, these
facilities are not the essence of Jurimetry.

If our intention is to seek a faster and more effective legal order, we must first abandon intuitions,
idiosyncrasies and erudite hunches to investigate the real factors that affect the performance of the legal order
in its current configuration. The point is: you don't change what you ignore. complementary to other traditional
disciplines, which uses statistical methodology to understand the functioning of the legal order, make its
behavior predictable, evaluate its impact on life in society, inform parties, politicians, judges, prosecutors and
citizens about how the legal order works and, thus, contribute to bringing its performance closer to the
objectives desired by society.

with this, make clear what this discipline is not and does not intend to be.

Second, Jurimetrics is not an attempt to automate Law and reduce judicial decisions to an exact
mathematical calculation. On the contrary, it starts from the premise that the genesis of a concrete legal
decision is an act of will, the complexity of which prevents its

However, other important issues for the functioning of Law can be the subject of legal research, notably
those relating to external manifestations of the legal order. Such research, despite not being sufficient to
resolve all axiological issues, is, without a doubt, necessary to understand reality and the most effective means
to change it. It seems clear that the debate on reforms of the Judiciary and procedural law, to cite two notable
examples, involves understanding the flows and the current stock of jurisdictional processes.

46 47

Regardless of whether or not you are in favor of the script, the important thing is that the decision to adopt
it is made based on the consequences of each option. It is understandable the concern of legislators to reform
an order that treats differently people who acted in the same way. It hurts our sense of fairness and upsets our
expectation of safety. But it is also understandable that a society is willing to tolerate a certain degree of legal
uncertainty to ensure that judicial decisions result from human reflection, and not from a formula or
mathematical model. In abstract and absolute terms, both positions make sense. The question, therefore, can
only be resolved from a concrete relativization, which involves the answer to a jurimetric question. What
degree of legal uncertainty are we willing to tolerate to ensure that judgments are defined by human
consciences?

48

Jurimetry is a new legal discipline,

It is important to reinforce once again that Law is not entirely measurable.

To conclude, it is important to clear up three misconceptions associated with the expression Jurimetry and,

Measurable are only its concrete manifestations. Ideals, abstractions and values cannot be measured because
they lack extension and concreteness. Furthermore, all the issues surrounding political transformation
proposals and social justice ideals are counterfactual and, therefore, cannot be detected by empirical research,
be it qualitative or quantitative. And the persuasion of society regarding the ideals to be pursued is, without a
doubt, a problem of absolute relevance, which goes beyond the scope of Jurimetrics and enters the limits of
the philosophy of Law.

X. What Jurimetry is not

49

Machine Translated by Google
"Rather than critiquing the role of legal doctrine (contrary the 'rule skepticism' of the original Legal
Realists), ELS sees doctrine as a source of empirical propositions to be tested. Rather than
shifting the center from the legal academy to the disciplinary social sciences , ELS sees the
disciplines as repositories of technical skills that can be imported into established legal endeavors.
And rather than demanding that legal scholars engage more directly with the social world, ELS
emphasizes the ease with which statistically skilled law professors can pluck low-hanging empirical
fruit in the comfort of their campus offices." SCHUMAN, Mark C. Mertz, Elizabeth. Toward a new
legal empiricism: empirical legal studies and new legal realism. Annual Review of Law and Social Science, vol. 6.,2010.

Attributed to Jules Henri Poincaré. In: FELIX, Isabelle. Informatique, télématique e vie cotidiene.
Paris: La Documentation Française, 1980. p. 203.

As Theodore Eisenberg explains, there is no way to go in search of data without taking as a
starting point the theorems of an analytical theory to be tested: "Scholarly rigor can take the form
of requiring that a theory be clearly articulated before empirical tests are designed or results are
reported. One sometimes hears descriptive empirical work dismissed with the comment that, 'I
was taught that one had to have theoretical basis for a study before pursuing data'. The blog
pundit quoted above seems to be picking up on the same theme by stressing the need for an analytical framework."

In the original: "La science est bâtie de faits de la même fazn qu'une maison est bâtie de briques.
Mais une accumulation de faits n'est pas plus de la science qu'un tas de briques n'est une maison."

EISENBERG, Theodore. The origins, nature, and promise of empirical legal studies and a response
to concerns. University of Illinois Law Review, v. 5, 2011. p. 1732.

1

two

3

reduction to a deterministic model and that, therefore, the automation of the decision
process is not only undesirable, but unfeasible. For Jurimetrics to exist as an application
of statistical methods in legal research, it is essential that Law is a manifestation of
human freedom, with its uncertainties and variations.

Third, Jurimetrics does not intend to replace other areas of legal knowledge, such
as, for example, the philosophy of Law and dogmatics. Jurimetrics is a positive discipline
that aims to describe the characteristics of a legal order. The assessment of the political
and axiological convenience of a given legal order is not on the agenda of Jurimetrics
for the simple fact that such judgments cannot be confirmed through statistical tests.

Like any positive discipline, Jurimetrics can help statesmen, jurists, judges and
public policy makers to foresee the consequences of their decisions. However, the
exercise of decision-making is a matter of political preference that is ultimately beyond
the reach of empirical science.

FOOTNOTES

Machine Translated by Google
Same, p. 58.

168.

9

10

This classificatory concept of legal certainty corresponds to an objectivist conception of interpretation, focused on the result, in the

sense that it is up to the interpreter, through a static and deterministic activity, centered on exclusively semantic aspects, to only

reveal a pre-existing normative content to the process itself. of interpretation. The normative content, which can be assessed in

advance and completely, corresponds to a point with which reality conforms, or not. Law, from this perspective, is seen as a given

object, independent of its subject and its process of application and interpretation. Each rule corresponds to an interpretative

alternative or a normative meaning (R = A)." ÁVILA, Humberto. Legal security. São Paulo: Malheiros, 2011. p.

Kelsen, for example, recognizes the constitutive character of the judicial decision, which is seen as a stage in the process of

implementing the Law. KELSEN, Hans. Pure theory of Law. 6. ed. Coimbra: Armenio Amado, 1984. p. 328.

6

"The paradox lies in the fact that the more legal security through Law is intended to

4

AVILA, Humberto. Legal security. São Paulo: Malheiros, 2011. p. 40.
7
5

Same, p. 51.

Humberto Ávila, for example, states that legal security is a value linked to an ideal state of absolute certainty in which the citizen

would be able to accurately predict the legal consequences that may be attributed to their acts: "In effect, legal security is many

sometimes representative of an ideal state of (absolute) certainty as a possibility for citizens to be able to accurately predict the

content of the rules to which they are and will be subject and the exact consequences that will be attributed to their actions. In this

sense, security is equivalent to the content certainty of the norm and the exact predictability of the consequences to be attributed to

the acts performed, illustrated by the redundant expression 'absolute certainty'. Its concept is, therefore, classificatory, or 'all or

nothing', with no margin for indetermination.

FERRAZ, Tercio Sampaio. Introduction to the study of law. 4. ed. São Paulo: Atlas, 2003. p 39 ff.

8

Machine Translated by Google
13

16

12

11

14

15

same'." ÁVILA, Humberto. Legal security. São Paulo: Malheiros, 2011. p. 47.

reasonableness. Thus, the problem of legal certainty is an argumentative problem.

News

Full text of the research report at: [http://s.conjur.com.br/dl/pesquisa-ipea-cnj-custo-execucao fiscal.pdf].

Accessed on: 08.07. 2012.

PRIGOGINE, Ilya. The end of certainties. São Paulo: Editora da Universidade Estadual Paulista, 1996. p.

involves an attempt to react, through Law, against the insecurity created by the

normative (R = A, B or C), to be determined through argumentative structures provided

professionals:

Law, in this sense, is recognized as an activity dependent on the process of interpretation

P. 201-202.

in:

VILANOVA, Lourival. Logical structures and the positive law system. São Paulo: Max Limonad, 1997.

victim of himself. And the jurist, previously a mere interpreter, becomes a kind of detective,

margin of uncertainty. (...)

[www.cnj.jus.br/images/programas/justica-em

agreements of the

of

predefined." ÁVILA, Humberto. Legal security. São Paulo: Malheiros, 2011. p. 169-170.

guarantee, the less legal certainty of the Law can be achieved. The paradox, put

"In this sense, legal certainty [is] represented by the oxymoron 'relative certainty'. As regards

joint effort

by metanorms of interpretation such as the postulates of proportionality, coherence and

Right. Combating legal uncertainty therefore involves the legal system fighting against itself

159.

and of application. Each rule corresponds to some interpretative alternatives or meanings

like a separate science. This is precisely why the problem of legal certainty has always

advices

Such is the difficulty of identifying which standard is applicable. Just mastering the law constitutes

report

in

normative content, the concept of security has a non-classifying character, with inevitable

In short, it is this: the search for security leads to insecurity. In this respect, the law ends up being

Full

numbers/2010/rel_justica_numeros_2010.pdf]. Accessed on: 08.07.2012.

limited to the predetermination of minimum possible meanings through argumentative structures

of

Machine Translated by Google
19

21

20

17

18

PRIGOGINE, Ilya. The end of certainties. São Paulo: Editora da Universidade Estadual Paulista, 1996. p. 21.

Lourival Vilanova explains the distinction between the formal concept of variable and the material
values it can assume: "Variables, as we can see, are not symbols that physically vary in a temporal or
spatial field. They are fixed symbols, identifiable in the occurrences that occur in logical forms. Yes,
the attributable values vary, and always within an orbit". VILANOVA, Lourival.

" Calculability means the ideal state in which the citizen can know how and when changes can be
made, preventing him from being surprised. This calculability only exists if the citizen can control,
today, the effects that will be attributed to him by the Law tomorrow, which only occurs if the citizen
has, to a large extent, the ability to, approximately, anticipate and reduce the reduced and little varied
spectrum of criteria and argumentative structures that define consequences attributable,
heteronomously and coercively or autonomously and spontaneously, to acts, own and unrelated, or
to facts, occurred or liable to occur, controversial or uncontroversial, and the reasonable spectrum of
time within which the definitive consequence will be applied." ÁVILA, Humberto. Legal security. São
Paulo: Malheiros, 2011. p. 684.

Stewart Macaulay's explanation of why new legal realism is really new is based on this effort to
understand the consequences of Law: On defining New Legal Realism: I would stress the term new in
New Legal Realism. This effort is Realist because it is not primarily focused on judges, legal rules,
and elaborate system building after the fashion of Williston or Wigmore. NLR is interested in the
consequences of law both the intended and the unintended. Itis interested when people turn to
normative and sanction systems other than theones studied in law schools. It is new because only a
few of the oldies didit, although many of them talked about it.

Logical structures and the positive law system. São Paulo: Max Limonad, 1997. p. 48.

Certainly, just as with the older movement, there is room for disagreement and differing points of
view in one's own the word realism. If you ask whether what we are doing fits within the Langdellian
paradigm, we would clearly be out of bounds for him, law was a science of doctrine and its laboratory
was the law library. Somewhere there is a nice passage about all of that in Brainerd Curries two
articles in the 1950s Journal of Legal Education about all of this. In:

The expression calculability is used by the philosopher Karl Popper as one of the properties of

[http://newlegalrealism.wordpress.com/2012/07/]. Accessed on: 08.07. 2012.

[http://www.cnj.jus.br/noticias/cnj/16162:conselhos-profissionais-querem-solucionar-litigios-pormeio
da-conciliacao]. Accessed on: 08.07. 2012.

Machine Translated by Google
22

23

26

27

25

24

The quantitative approach approximates the proposal of Jurimetrics to the work that has been
developed in the United States under the name of Empirical Legal Studies - ELS. "ELS
methodological vision is more quantitative than qualitative, more confirmatory than exploratory,
and more contemporary than historical. Although several leading ELS scholars have endorsed
big-tent usages that would include qualitative as well as quantitative methods (eg, Diamond 2002,
Mitchell 2004) , other ELS proponents seem to take 'empirical' to apply only to statistical analysis
that hew closely to a formal hypothesis-testing version of the scientific method". SCHUMAN,
Mark C. Mertz, Elizabeth. Toward a new legal empiricism: empirical legal studies and new legal
realism. Annual Review of Law and Social Science, vol. 6, 2010, p. 558.

Popper selections. Princeton: Princeton University Press, 1985.

MALHOTRA, Naresh. Marketing research: an applied orientation. 6. ed. Porto Alegre: Bookman,
2012. p. 124-129.

MALHOTRA, Naresh. Marketing research: an applied orientation. 6. ed. Porto Alegre: Bookman,
2012. p. 112-120.

For a more detailed explanation of the role of measurement in empirical research: COOPER,
Donald R.; SCHINDLER, Pamela S. Research methods in management. 7. ed. Porto Alegre:
Bookman, 2003. p. 178-179; MALHOTRA, Naresh. Marketing research: an applied orientation. 6.
ed. Porto Alegre: Bookman, 2012. p. 200-201.

MALHOTRA, Naresh. Marketing research: an applied orientation. 6. ed. Porto Alegre: Bookman,
2012. p. 121-123.

determinism. For Popper, deterministic calculability is the ability to predetermine the occurrence
of an event with any degree of precision. Popper, Karl. In: MILLER, David (org.).

Ethnography (from the Greek ÿÿÿÿÿÿÿÿÿÿ, meaning writing about a people) is a qualitative
method of collecting data about the customs and values of a population, based on intersubjective
contact between the researcher and the social group under study. Participant observation is a
means of ethnographic production in which the researcher can not only observe, but must find
an active role to play in the social group, object of study.

Machine Translated by Google
In the original: "methodological eclecticism inevitably embracing qualitative and quantitative work [and

as a brand: How do we know what law really does? Can the best knowledge from social science be

Our group of New Legal Realists does not take an exclusionary or singular approach; weincorporate

links

31

Elizabeth. Toward a new legal empiricism: empirical legal studies and new legal realism. annual

[http://newlegalrealism.wordpress.com/].Accessed on: 08.07. 2012.

29

effort to develop translations of law and social science. This requires expertise in the language and
social science knowledge, New Legal Realists in the US have been working toward a new synthesis of

webpage since 2004. In 2011, we moved the Conversations section of the NLR web project to this blog

In the original: "a model-based approach coupled with a quantitative method [in which the researcher]

our scholarly writings, we have been holding conferences and posting on the New Legal Realism

reprint São Paulo: Atlas, 2011,. P. 123-138; COOPER, Donald R.; SCHINDLER, Pamela S. Methods of

Buildingfrom the law-and-society interdisciplinary tradition, and drawing on the fullrange of current

30

legal .studies and new legal realism. Annual Review of Law and Social Science. v. 6, 2010. p. 662.

32

I'm

really means in our liveswill require examining many kinds of questions, using multiple methods.

28

Interestingly, much of this excellent ongoing research has yet to be discovered and incorporated into

using] not only statistical analysis but also field-intensive methods such as participant observation

Review of Law and Social Science, vol. 6, 2010. p. 558.

categories of law, as well as expertise in interdisciplinary research on law.

The very definition contained in the main New Legal Realism blog reinforces multidisciplinary

whichnow

offers a positive theory of a legal institution and then tests that theory". SCHUMAN, Mark C.; MERTZ,

law, social science, and policy since 1997. Their work bridges disciplines and methods in a systematic

format, the the webpage. In

marketing: an applied orientation. 6. ed., Porto Alegre: Bookman, 2012.p. 109-130.
management research. Porto Alegre: Bookman, 2003. p. 131-132.; MALHOTRA, Naresh. Search

rest

and interviewing". SCHUMAN, Mark C.; MERTZ, Elizabeth. Toward a new legal empiricism: empirical

mainstream legal scholarship, despite the current interest in empiricism in law.

On qualitative research: COZBY, Paul C. Research methods in behavioral sciences. 5.

Getting people to cooperate and talk across disciplines is often the hardest job of all. In addition to

of

qualitative, experimental, and quantitative methods and study all aspects of law from the ground level of daily

life to the top-level of judges and politicians. We think that understanding what law

used to improve delivery of law on the ground? From the old legal realists to todays generation of law andsocietyresearchers,
scholars in law and social science have worked to answer these questions.

Machine Translated by Google
In:

project in that arena, and that mobility project is bound up with NLR's connections to empirical social

Lenza, Pietro de Jesús Lora Alarcón (coords.). Judiciary Reform. São Paulo: Method, 2005. p. 501.

concept of procedural acts per unit of time (day, month or year), based on the notion that the

Business 91.

37

The following excerpt nicely defines the distinctions between the two approaches. "Like ELS, NLR is firmly

34

sociolegal field". SCHUMAN, Mark C.; MERTZ, Elizabeth. Toward a new legal empiricism: empirical

SILVA, José Afonso da. Course of positive constitutional law. 11. ed. São Paulo: Malheiros Editores,

report

36

[http://www.Direitorp.usp.br/arquivos/noticias/sites_eventos/encontro_pesquisa/links_dos_trabalhos.htm].

question of how these two flavors of empiricism relate to one another and to other players in the

To see

practiced would indicate how quickly it advances in time.

BUENO, Cassio Scarpinella. Systematized course in civil procedural law. v. I. 4th ed. São Paulo:

See REED's purpose statement at: [http://reedpesquisa.org/institucional/]. See also themes

theoretical grounding, and sensitive translation than on quantitative technique, topical immediacy,

2013, P.

GRINOVER, Ada Pellegrini.The necessary infraconstitutional reform. In: André Ramos Tavares, Pedro

embedded in the language and problematics of the legal academy; NLR is actively pursuing mobility

As procedural speed cannot be measured in kilometers per hour, we proposed at ABJ the

Doing

legal studies and new legal realism. Annual Review of Law and Social Science. v. 6, 2010. p. 562-563.

1996. p. 186.

33

Accessed on: 25.07.2012.

38

[www.doingbusiness.org/~/media/GIAWB/Doing%20Business/Documents/Annual Reports/

English/DB13-Chapters/Enforcing-contracts.pdf] Accessed on: 21.01.2016.

and definitive hypothesis testing. NLR is also more concerned about integrating research from cross cultural and global

arenas. This conjunction of overlapping agendas and differing styles raises the

of the debate tables of the 1st Meeting of Empirical Research in Law in:

Saraiva, 2010. p. 176-178.

process follows a procedural trajectory in which the temporal frequency with which acts are

35

science. Compared with ELS, however, NLR relies more heavily on methodological diversity,

Machine Translated by Google
44

42

46

40

45

43

41

39

The name is inspired by mechanical rheology. Areology is the branch of fluid mechanics that
studies the physical properties that influence the transport of momentum in a fluid.

CARVALHO, Paulo de Barros. Tax Law Course. 18. ed. rev. and current. São Paulo: Saraiva, 2007.

p. 158.

Evidence From Booker. Coase-Sandor Institute for Law and Economics working paper n. 662 (2d
series).

United States vs. Booker,543 US 220(2005). Votes and oral arguments accessible at: [www.oyez.org/
cases/2000-2009/2004/2004_04_104/]. Accessed on: 21.01.2016.

CANOTILHO, JJ Gomes. Constitutional right. 6. ed. rev. Coimbra: Livraria Almedina, 1995. p. 371-
372.

MELLO, Celso Antônio Bandeira de. Administrative law course. 14. ed. rev., current. and amp. São
Paulo: Malheiros, 2004. p. 104-105.

Schuman and Mertz briefly explain the gains resulting from replacing personal intuition and
conventional wisdom with empirical research: "What unites these nonsense camps [ELS and NLR]
is an enthusiasm for applying rigorous empirical methods to questions of legal (as opposed to
primarily disciplinary) import. (...) The ELS mission, then, is the empirical study of all those
phenomena that have long commanded the attention of legal scholars and practitioners but have
heretofore been known only through doctrine, personal experience, conventional wisdom, and
surmise" . SCHUMAN, Mark C.; MERTZ, Elizabeth. Toward a new legal empiricism: empirical legal
studies and new legal realism. Annual Review of Law and Social Science. v. 6, 2010, p. 559.

YANG, Cristal S. Have Inter-Judge Sentencing Disparities Increased in an Advisory Guidelines Regime?

SILVA, DePlácido e. Legal Vocabulary, 15th ed. Rio de Janeiro: Forense, 1999. p. 739.

Machine Translated by Google
47

48

49

(...)

The origins, nature, and promise of empirical legal studies and a response to concerns. University of Illinois

Law Review. v. 5, 2011. p. 1734-1737.

[the empirical and statistical investigation] can provide a systematic knowledge of an important aspect of

society - the legal system - similar to knowledge about other central characteristics of society, such as the

economy, crime, and healthcare. These other features have highly developed data-gathering systems in

place that dwarf the available information about legal systems". EISENBERG, Theodore.

Theodore Eisenberg, commenting on the work of the National Center for State Courts, a type of CNJ of the

USA, explains the reasons for academics, courts, litigants and public policy agents to invest time and

resources in legal investigation of the Law: "the knowledge shortfall leaves everyone - litigants, policy

makers, the media, and the legal profession - without basic knowledge of how the legal system is actually

functioning. Only through massive efforts by organizations such as the National Center for State Courts

does the United States have elementary unbiased estimates of the outcomes of state court trials. Systematic

knowledge of settlement rate, the modal outcome in civil litigation, exists largely in relatively few studies

isolated by time or locale, and even less information is available about the terms of settlement .

It is pertinent to remember here the recommendation from The American Law Institute on the importance

of judicial statistics in the development of public policies: There has been a tendency on the one hand to

deify and on the other to decry the results of tabulations of court business. It is easy to go to either extreme.

More soundly, however, students of law administration are learning the proper function of such mass

statistics in providing trustworthy facts, so far as they go, of court activities which may be used to verify,

support, disprove, or suggest general hypotheses. The facts must be of such general nature as will lend

themselves to average or mass verification, such as the nature of the general run of business in the courts,

the character of the parties to the suit, the general methods of termination of cases, whether by court, jury

or some other form of trial or by agreement or withdrawal and so on; but withinthese limits the facts may

be definitely ascertained. Moreover, facts inthemselves do not prove what should be the policy of law

administration or thedirection of reform therein. Their function is but to cast light upon the factors which

should shape the rules of policy. The American Law Institute (Wickersham Commission). A Study of the

Business of the Federal Courts. Philadelphia: Executive office, The American Law Institute, 1934.

EISENBERG, Theodore. Why do empirical legal scholarship? San Diego Law Review. v. 41, 200., p. 1741.

© of this issue [2016]

Machine Translated by Google
I. Sociological pre-salt

7. Conclusion: nobody changes what he ignores

2018 - 07 - 17

Jurimetria 7.
CONCLUSION: NOBODY CHANGES WHAT THEY IGNORE

1

Knowing reality is the first step to transforming it. The lesson that underlies every
serious empirical research effort is that no one changes what they ignore. There is,
therefore, no incompatibility between Jurimetrics and the political aspirations of law.
Empirical research does not want to reduce the axiological dimension of law to a
handful of numbers, nor does it aim to replace human decisions with mathematical
models. On the contrary, there is complementarity between these efforts, since the
results of research into the world as it is provide relevant information about what we
should do to bring it closer to what we would like it to be.

The diffusion of computers as a daily work tool among professionals in general,
inside and outside the government, also contributed to the development of another
unexpected factor in the development of Jurimetrics: the accumulation

The speed of increase in its processing capacity inspired the so-called Moore's Law,
which states that computers double their performance each year for the same cost,
which allowed the dissemination of statistical analysis. Originally, due to budget
constraints resulting from the cost of computers and specialized programmers,
these analyzes were restricted to large government projects.

This is one of the reasons why empirical studies in law have been growing.
Lawyers are more aware that there is a lot going on in the courts and that the success
of future legislative reforms depends on good diagnoses of current problems. If laws
are the medicine for the ills of social coexistence, we have to be very attentive to the
courts, which are the hospitals where they manifest themselves. But, in addition to
responding to practical needs and helping jurists to make decisions, empirical
research is driven by an important technological factor: the emergence of broad
legal databases.

Keeping proportions, just as Word popularized word processors and turned every
user into an amateur editor, programs like "Stata" and "R" popularized database
processors by making it easier for many users to access statistical analysis.
Currently, a student of administration, marketing, mathematics or law who is
interested in exploring a database in his field has at his disposal free of charge (in
the case of R), or at a reasonable cost, sophisticated programs mounted on machines
with computational capacity 5000 times greater than that of a laboratory in the early
1970s.

The first computers that appeared in the 1970s were the size of a living room and
were not intended for personal use, in addition to being financially inaccessible.
They had a computing capacity equivalent to 1/5000th (or five thousand times less)
of a current laptop sold in a common retail store.

Machine Translated by Google
extensive databases.

These difficulties, however, are being overcome. The computerization of public
administration allowed the spontaneous accumulation of large databases containing
data on various governmental bodies, such as, for example, the bases of the Federal
Revenue Service, commercial boards, the Securities and Exchange Commission,
courts and various government records, such as the register of children in CNJ
shelters, the register of defaulters at notary offices, the national adoption register,
on the most diverse aspects of our social life. These databases are a kind of
"sociological pre-salt", a boulder of raw data waiting to be mined by researchers.

Of course, several studies still depend on data collection in the field, because,
however broad they are, these bases will not be able to answer all the questions.
However, there is still a surprising amount of information stored, accessible at a cost
and time comparatively lower than those involved in field research and data collection
efforts in a country of continental dimensions like Brazil.

The results are very interesting. At the state level, the study revealed a positive
association between development and litigation with a correlation coefficient of 0.83,
which means that each hundredth more in the HDI, at the state level, is associated
with an increase in the litigation rate, defined as the number of new cases in the state
per 100,000 inhabitants, from 625 cases per 100,000 inhabitants. The conclusion is
that the litigiousness rate is one of the potential indicators of the population's level
of well-being and that the Judiciary can expect a substantial increase in the demand
for judicial provision, if Brazil moves towards a superior state of socioeconomic
development.

Conducting statistical research has always been dependent on data collection.
The data collection procedure is undoubtedly the most expensive part of a survey
because it depends on hiring, training and supervising the work of several field
researchers, available to travel to different places and, through interviews or filling
out forms, obtain the data necessary to carry out the analyses. The costs related to
data collection were an impediment to a nationwide survey, given the number of
researchers needed to collect data and the difficulties in accessing corners of the
country.

An example of what the sociological pre-salt is is found in a recent survey
prepared by ABJ on litigation in Brazil. This research was planned to dialogue with
similar research carried out in India and Italy, which had as its motto the following
question: how does economic and social development affect the litigiousness rate?
To advance the research, the ABJ searched the bases of the UNDP Atlas for
information regarding the HDI by federation unit and by municipality in Brazil and
compared the results with the litigiousness rates measured according to the data
available in the Open Justice program of the CNJ, unfortunately discontinued in 2015.

two

Machine Translated by Google
Figure 2: Scatter plot between the 2005 state HDI (source) and the number of new cases
per 100,000 inhabitants in 2010 (source), with the least squares line represented by the
dashed red line.

Figure 3: Quadratic curve adjusted for the relationship between municipal HDI (IDHM,
UNDP 2012) and processes distributed per 100,000 inhabitants (source).

These two studies demonstrate the richness of the "sociological pre-salt" and the urgency of its

This study was then deepened and the results became even more interesting. At the
municipal level, the positive association between development and litigation was
maintained; however, the adjustment of the model revealed a quadratic function, indicating
a tendency for the litigiousness rate to stabilize at a natural level.

Machine Translated by Google
exploration. Both were planned and executed using existing bases and, despite this,
proved to be revealing, unveiling unsuspected aspects of an important issue for the
administration of the courts from different angles. Thus, before starting any empirical
research, it is important to check whether there are databases related to the topic
and whether they have already been exhausted. A premise from which Jurimetria
starts is that the majority of government and private initiative databases remain
unexplored. There is, therefore, immense potential to transform the raw data
accumulated in a disorderly manner by the Government and the private sector in
their computer systems into useful information.

But that's not the only reason. Part of the explanation lies in the fact that legal
courses in Brazil have remained restricted to an excessively legalistic stance,
studying the law as an end in itself and not as a means to achieve objectives desired
by society. While other social sciences, such as Economics, Political Science and

Throughout Brazil's history, jurists have played a fundamental role in the
development of public policies and in defining the country's institutional directions.
Due to their rhetorical skill, their knowledge of the intricacies of the legislative
process and departments, and their mastery over the interpretation of the law,
bachelors occupied the position of prominent advisers to heads of state, conceiving
the great government reforms championed by professional politicians.

This panorama changed, mainly from the second half of the century. XX, when
jurists were losing space and influence within the Government. Their decline, as
formulators of public policies, stems, on the one hand, from the emergence of new
higher education courses that did not exist in the first decades of our republic. At
that time, the national elites who chose not to follow a military or ecclesiastical
career had three faculties available: medicine, law and engineering. The emergence
of alternative degrees such as geography, history, economics, social sciences,
administration and accounting sciences has diluted the participation of traditional
professions not only in government, but in the market as a whole.

In addition, this study on litigation shows how the results of empirical research
vary according to access to so-called microdata. Jurimetry can be compared to
optical equipment capable of showing the intimate structure of the legal order with
different degrees of magnification. Observing the relationship between litigation and
development in the 27 states of the federation was like using a magnifying glass: at
the state level, this association was linear. When we increased the magnification and
observed the 5,570 municipalities at a microscopic level, the relationship was shown
to be quadratic. What's interesting here is how the same question has different
answers depending on the level of access to the granular data. And as the facts can
always be deepened, the results of this type of research must always be accepted
within the limits of the premises of each research. In Jurimetry there are no absolute
truths, only provisional answers, valid until an equally or deeper and broader study arises.

This when they were not themselves the rulers in charge of legitimizing these
efforts before the population. It is enough to remember that of the first thirteen
presidents of the so-called Old Republic, eleven were bachelors in law. Such influence
still persisted in the Getúlio Vargas period, not only because Getúlio Vargas himself
was a lawyer, but because of the role that ministers such as Francisco Campos,
Osvaldo Aranha, Vicente Rao and Gustavo Capanema played in various areas of
government, such as the Ministries of Foreign Affairs, Justice , Interior, Farm and Education.

II. The rebirth of jurisprudence

Machine Translated by Google
Psychology, went through a profound process of methodological review of an
interdisciplinary and empirical nature, Law remained isolated as an area of study of
legislative texts, contenting itself with passively interpreting the final product of the
processes of political transformation. Gradually, jurists were removed from command
positions and became a type of legal dispatchers with the function of ensuring that
projects proposed by the Government did not run into bureaucratic obstacles such
as quorums, deadlines or prior authorizations.

As a consequence, Brazil witnessed at the end of the century. XX to a process of
scrapping the legal professions. At the same time that its elite platoon was losing
space in the formulation of public policies, a torrent of low-skilled graduates was
dumped onto the market, without the technical conditions to perform tasks with any
level of complexity. As a result, while in other countries lawyers have parastatal
functions, enjoying public faith and acting as notaries, registrars and facilitators of
the Public Power, in Brazil, this class has few prerogatives, which are not always
respected, and lives mostly on legal assistance. and provides a mechanical service,
with low intellectual content and commoditized.

It is no surprise that the position of prominent "consiglieri" held by the jurists of
the Old Republic was gradually occupied by a new class of social scientists, who
chose to investigate the real behavior of people in society: economists.
Economists quickly realized that knowledge about reality was essential for designing
policies capable of promoting relevant advances, due to the self-evident finding that
no one can transform what they ignore. As they were among the first humanists to
recognize the importance of using statistics as a methodology for investigating
human behavior, economists advanced in the construction of an empirically based
social science and created more successful tools to explain the result of institutional
reforms. Such advances, added to the modernization of society, which seeks effective,
timely and cheap solutions, made Economics stop being a mere accidental discipline
in law courses to, in just over fifty years, gain curricular autonomy and become the
most powerful social science of our time.

The loss of influence of jurists in the development of public policies is a direct
consequence of the erosion of Law's ability to explain reality and offer solutions to
the challenges faced by modern governments. There are those who say that if Brasília
were built today, we would have the Praça dos Três Poderes in the pilot plan, with
the Palácio do Planalto, the National Congress and the Federal Supreme Court, and
next door, as the home of the main adviser of the three powers, no longer the Palace
of Justice, but the Palace of Efficiency, where the Ministry of Economy would operate.

Another problem was the scrapping of the profession. From the 1990s onwards, a
surprising number of Law courses began to proliferate in Brazil, dumping thousands
of graduates onto the market who were unprepared for the practice of the profession
and unable to pass the Bar Exam. The numbers are impressive. We have 1,156 law
schools in Brazil, an increase of 279% compared to the 305 schools that existed in
1995, with almost 740 thousand enrollments. In contrast, failure rates in the OAB
exam are very high and reached a record high of 90 in 2012. %. Having worked for a
year as an examiner for the OABSP legal education commission, I realized that these
new courses were not intended to train judges, prosecutors or lawyers, but to
complement the education of their students, making up for deficiencies arising from
primary and secondary education. , such as writing in Portuguese and logical
reasoning. These new Law faculties end up becoming a mix of two courses: one
supplementary and the other professional.

.
3

Machine Translated by Google
But should something be done or should the Law settle for a supporting role?

The jurist can no longer be content with describing the sanctions provided for in
existing norms and working as a taxonomist of legal types. The design of a legal
order and the construction of successful public policies go far beyond attributing
sanctions to reprehensible conduct. Rewarding desired behaviors, disseminating
information to help people make correct choices, creating supplementary norms for
cases of omission, building well thought out open clauses and establishing general
principles are some examples of alternative means to influence people's behavior,
which do not depend on the old concept that the only planning strategy is to monitor
and punish the population.

Particularly, I understand that it has a relevant contribution to be made to the
development of society, which is being delayed by the lack of a common
methodological language with post-revolution statistical science. The law is the
main tool of government action in democratic states of law, and the study of how it
impacts people's lives is indispensable. The head of jurists is a machine for
producing interesting working hypotheses about what is or is not working in the
interaction between government and society, formulated according to a rich daily
experience in the experience of human conflicts, typical of those who witness it in
their daily lives. Every day everything that went wrong: the broken marriages, the
unpaid debts, the bankrupt companies, the abandoned children. The problem is that
they have not been trained to test these hypotheses.

Some critics of Jurimetria say that every human decision has an insurmountable
component of free will and that, therefore, it is impossible to quantify and predict it
accurately. Causal models would be applicable to physics or chemistry, but not to
the humanities. The criticism is only partly true. Freedom of conscience and the

Furthermore, Law needs to descend from the high planes of legal theses and
enter the reality of human conflicts. And why does statistics play a key role in this
process? Because Law is concerned with social facts - crimes, indemnities and
payments, marriages, adoptions and separations, contracts, fines and defaults -
facts that flow in abundance in community life. Anything can be said of a human
being, unless he is original. Almost everything we do has been done by someone,
somewhere, at some time, sometimes hundreds or thousands of times. Understanding
these patterns is the essence of what a human science must do. In the right case,
we have to investigate the motivations behind the decision to file a lawsuit, or not to
settle, or to enter into a contract, or to commit a robbery. It is through the
investigation of the context in which these choices are made that the jurist will be
able to contribute to the creation of more effective norms, capable of deterring
people from the practice of socially unwanted acts and encouraging behaviors
considered healthy and productive.

But then what to do? The first point, the touchstone of this process, is to make
Law once again a social science, a science concerned with man, and not a branch
of literature that interprets abstract legal norms. The Law has to dig its hands into
the mud of jurisprudence, go into the field to interview the parties and judges and
deeply understand the disease before speculating about possible cures. Your
interest should not be in isolated norms, but in people's problems and the capacity
of norms to overcome them. Laws are "quasi-fiction" books, which tell stories about
what judges should do in certain situations, but which say nothing about what they
actually did. Restricting the work of jurists to the interpretation of these books
means condemning them to a state of alienation and reducing Law to a branch of
literary criticism.

Machine Translated by Google
A decision is never taken in a vacuum, and there is always a social, economic,
psychological, political, geographic, etc. context that helps to understand why a choice was made.

And finally, there is also a serious question of responsibility towards vulnerable
parts of society. If the courts are the hospitals of social life and the laws are the
medicines, jurists and legislators have a moral obligation to guarantee that the
medicines administered to the population will produce the expected effects. This,
however, does not happen. The Legislature understands that its role is to legislate
and that its job ends when a law is enacted. Once the law is made, the legislator
turns his back and concludes his task, assuming that if there are problems in the
application of the new law, civil society, through its associations, institutes, OCIPs
and NGOs, will provoke him to act again.

This passivity is wrong. Most people do not participate in civic organizations and
do not take part in political events other than periodic elections. Hence the existence
of countless ills, injustices and social problems that affect groups without the
capacity for political articulation. These are children abandoned in shelters awaiting
adoption, low-income elderly people who do not find support from their families and
suffer violence, small business owners plagued by bureaucracy, drug addicts who
have become homeless. Precisely because they do not have resources even for their
own subsistence, let alone to support an association, these groups are unable to
make their demands reach the National Congress. And the lack of official monitoring
mechanisms means that serious distortions, which affect vulnerable portions of the
population, end up ignored by legislators, contributing to the perception, partly true,
that legislative houses have become hostage to lobbyists and are no longer able to
represent their constituents.

Not surprisingly, people's freedom is usually exercised within certain known ranges
and, for no other reason, some choices are firmly associated with certain contexts.
For this reason, the effort to make predictions, even if approximate, pays its cost.
We will never predict exactly the time, place and authorship of all homicides.
However, we can know approximately how many homicides will occur in the city of
São Paulo next month and which are the most vulnerable areas, which is essential
information to guide the police's preventive action.

The solution to this situation involves broad political reform, I have no doubt; but
it also involves the use of Jurimetrics and regulatory impact analysis by legislators.
The powers of the Republic should exchange information on the situation of

Also, making predictions is unavoidable. Whether we like it or not, we are forced
to make decisions based on exercises in anticipating the future effects of our present
choices. Whenever a lawyer chooses a strategy to bring a case, or a legislator
chooses a wording for a new article of law, or a judge adopts a position in a judicial
decision, they imagine that the future results of these choices will probably be those
desired. Every choice presupposes a prediction.
Therefore, Jurimetrics is not proposing that legislators, judges and lawyers venture
into guesswork that they previously had no intention of doing. Quite the contrary, it
is simply offering an additional tool, structured on a reliable methodology and
available data, capable of helping them refine predictions that would be made in any
case, based on guesswork and the exercise of mere intuition.

creativity make human behavior ultimately unpredictable and immeasurable. But it
is also true that the exercise of a certain degree of freedom does not imply irrationality
or lack of motivation.

Machine Translated by Google
Eisenberg, Theodore, Robinson, Nick and Kalantry, Sital, Litigation as a Measure of Well-Being

(2012). Cornell Legal Studies Research Paper no. 12-28. Available at SSRN: [http://ssrn.com/

abstract="2036194" or http://dx.doi.org/10.2139/ssrn.2036194].

Data from the 1995 and 2012 higher education censuses from the National Institute of Educational
Studies and Research - INEP.

Available

typeDocument=Legisla%C3%A7%C3%A3o::Lei;expandGroup=date-2000s].

This number is based on the hertz of a processor. While those from the 70s had around 740KHz,

current ones have an average of 3.6GHz, which, in round numbers, is 5000 times more.

[www.lexml.gov.br/busca/search?f1- in:

FOOTNOTES

vulnerable portions of the population and prepare to periodically monitor these bases, organizing
a positive agenda of discussions with experts, with the aim of developing an active legislative
agenda and creating regulatory goals aimed at overcoming real problems.

Reducing, simplifying

and rationalizing the legal system should be one of the main goals of the Legislative Branch and
is, without a doubt, one of the areas in which Jurimetrics can make a relevant contribution.

For the same reasons, permanent regulatory simplification commissions should be created to
identify useless or dysfunctional standards. Once identified, the need for these standards would
be assessed for repeal purposes. Courts have a lot of information about which provisions are
applied in adjudicating disputes and which are not. Structuring judgments on bases that allow for
a recurring assessment of the applicability and uselessness of current norms is a measure that is
as elementary as it is necessary. Brazil is a country of high bureaucratic complexity, with 91
courts, 16,427 judges, 60,000 legislators and 324,000 laws in force.

END

© of this issue [2016]

3

4

4
two

1

Machine Translated by Google
Jurimetry

The first international statistics congress, held in 1853 in Belgium, which aimed to study the
standardization of international comparisons, apparently did not present strong probabilistic
concerns. When the works of R.
A. Fisher (1890-1962), an astronomer, presented the pioneering concepts of randomness and
likelihood, in publications from the second decade of the last century, a new field of research opened
up. Thus emerged the Science that aims to maximize information at the lowest cost, or the Science
of the Invisible as Master Prof. would say. Carlos A. de B. Pereira, famous and beloved Senior
Professor at IME-USP.

Chapter 2, Determinism and the Statistical Revolution, discusses the unfathomable

AFTERWORD

Well, now prefaced by the eminent Prof. Fabio Ulhoa Coelho, the author Marcelo Nunes Guedes,
a prominent lawyer, believes that the time has come for Law, its actors and its certainties often
arising from good old hermeneutics, to join our so-called different knowledge, Statistics. I believe
that the marriage between Law and Statistics has reached its moment, and I don't think it's too late.
But in fact we notice that these two sciences are old lovers. There is a lot to do to make the most of
this marriage. We would all like it not to be necessary to waste time on useless bureaucracy,
although it is believed that some useful ones are necessary. Having unlimited time for an
epistemological debate is what this book invites, challenges and proposes for everyone to dedicate
themselves to both areas on their honeymoon.

AFTERWORD

I understand that postfacing is not concluding, much less explaining what has been read. It is
about criticizing, recommending, interpreting and collaborating. The honorable invitation was
accepted, and now there is little care. I step on the sands of the "inferiority complex" of the social
sciences, and, being a Statistician, I invite you to make the marriage of the two knowledges a happy one.

Probability Theory, which heavily depends on deep mathematical knowledge, has a defined
logical structure, using axioms and theorems that allow us to clarify the different judgments that
involve uncertainties and a vocation to rationally deal with a huge range of applications. Statistics,
in turn, relies heavily on probability to build competent models for prediction, estimation and
hypothesis testing. The logical foundations of statistics are, in fact, built within the formalisms of
probability, the theory of chance considered by many as the science of randomness.

2018 - 07 - 17

The book begins with the issue of bibliographic weightlifting, a concept introduced and returned
to later in the last chapter. In short, it's about knowing reality to solve problems. I wonder if this
question of arguing in a loop, the first chapter being tied to the last, is part of the rhetoric taught in
good schools. It must be, as it is well argued. The issue of the insufficiency of theoretical efforts to
understand Law, the legal complexity that embraces us, the myriad of information that can suffocate
us without us realizing it, the relevance of statistical techniques and methods in a changing world,
makes us feel sorry for the inability of empirical research in the social sciences in competently facing
naturally arising challenges.

Machine Translated by Google
Chapter 2, Determinism and statistical revolution, discusses everything from the
unfathomable divine designs, through the aversion to Statistics, to the question of
rationality in predicting events, where Karl Popper is cited under his famous The Open
Universe. Girenzer's following sentence seems to predict heated debates from now on.
The Empire of Chance is attributed the following sentence: "In Determinism, probability
is a mental state, a measure of our ignorance, and not an objective quality of reality." I
feel comfortable with this promising, grandstand future when I read about the statistical
revolution. The issue of the Central Limit Theorem, which helped to consolidate the
scientific revolution, deserves attention. From now on, humanity can reproduce
experiences to try to validate other knowledge. Independent reproductions of many
measurements have approximate probability distributions. In the second decade of the
last century, in France, this Golden Theorem was rigorously proven.

Chapter 3, Statistical Methods, makes me anxious in the stands because I know the
rules of the game. The modern meaning of Statistics is not that of a collection of data
about the population and finances of a State. I think that Marcelo, when stating that this
definition may be subject to controversy, was polite. Statistics is not just that; and much
more. When discussing the definition of Probability, Marcelo introduces the issue of
Expected Risk, which I believe will be relevant in future Jurimetrics work. He writes, in
an inspired way, that the probability of God existing allows us to predict an infinite
benefit. I am more modest in exemplifying the same situation. Let's imagine a game in
which someone bets R$1.50 (one real and fifty cents) and can win R$200 million with a
chance that is equivalent to flipping 22 coins and all having the same face (a different
one loses). This is an illusion tax (or Megasena). But, for sure (probability one) those
who don't play won't win. Or, getting ahead of myself a little in the text, everyone except
the Australian platypus loses.

Therefore, quoting Ian Hacking, we arrive at the Probabilistic Revolution. Here Marcelo
Nunes could have emphasized and discussed a necessary restriction to the Theorem: independence!

The lottery paradox, cited and owed to Keynes, A Treatise on Probability, shows that
much work to correct probabilistic misconceptions remains to be done. It is not a
paradox and not even constructive criticism. Let's go back to Law, and remember that
established authority may be able to control - or as Ross says, influence - people's
behavior. Marcelo transitions the path from this legal statement to Statistics concepts
with propriety. It clarifies that samples do not represent populations, or explains in a
footnote the Law of Large Numbers, for example. When quoting Oliver Holmes, he
relates a rare event to injustice. In an elegant way, it shows that the lottery paradox is
not pertinent. Furthermore, it discusses the transition from the Era of Formalism to the
Era of Legal Realism. Let us remember that Megasena distributes the

Probabilistic independence makes the power of Statistics applications unsurpassable.
Without it, the methods could result in the undesirable "each case is different". Let's say
I would like to predict, with some measurement of uncertainty, whether or not I will win
a dispute. Knowing the judge's district, the thesis adopted by my lawyer, the history of
similar cases, etc., helps to alter my uncertainty. On the other hand, the judge's
horoscope sign (month in which the judge was born) is information independent of the
outcome to be predicted! Only astrologers would say yes, that there is an association
between these facts! Putting the statistical methodology to construct and serve
Jurimetry, with its conditions for the theorems to be valid, and this Probabilists have
been demanding in Statistics for more than a century, is one of the challenges exposed
by Ilya Prigogine, in the widely cited book, The end of certainties, and remember when
it is described that we live in a privileged moment in the history of sciences. Let us
escape the blind and illiterate people that the ironic Stigler talks about.

Another necessary restriction is on variances, but the reader will encounter this concept
later.

Machine Translated by Google
exactly the same!

In Chapter 6, Characteristics of Jurimetrics, the text's bricks are arranged to define the
cornerstone of Jurimetrics: controlling uncertainty in Law. The probabilistic cause, the
prognosis, the possibility of even measuring speed or procedural viscosity, rheology - which
we understand as necessary to measure productivity and set goals - everything is
conceptualized. The escape from circular definitions, well exemplified in the issue of legal
certainty, judicial guidelines, the United States against Booker case , even in the absence of
consecrated doctrinaires, show what this book came for: Jurimetrics is a new discipline, both
legal and statistical. . I am sure that this book makes a great contribution to Law moving
forward in its purpose of being a manifestation of human freedom. In the Conclusion, via the
perception that we now have broad legal databases, the microdata are there, Marcelo places
himself as one more person to take Law away from an inglorious destiny, where this would
be just a branch of literary criticism reduced to a state of alienation. This optimism he calls
the pre-salt state.

In closing, in this afterword I refer to the preface by Eminent Prof. Fábio Coelho, who
kindly invited me to an epistemological debate, presiding and guiding, and which he
appropriately called a fertile discussion. Linking the pre and post-easy courses (I'm learning)
I say yes, Professor Fábio, we all now have a text to invite students and law professionals to
study and thus make the fundamentals of Jurimetrics accessible. Marcelo Guedes Nunes has
all the merits. We, Statisticians, with due respect, will gladly collaborate.

Chapter 4, Origins of Jurimetrics, defines standards; They are like chess rules! However,
realizing that inventing certainty where it does not exist, a swampy terrain that judges live in,
the use of statistical methods must be recommended, as Jurimetrics brings scientific methods
of use in the legal field, a field that consists of legislators, witnesses, parties and judges. For
all that has been said and defended, it is understood that the methodology of Jurimetry is
Statistics. A fair recognition that the study of uncertainty is useful to Law - habent sua sidera
lites; a just cause is lost because the stars are unfavorable, said Calamandrei.

Adilson Simonis

Arriving at Chapter 5, Concept of Jurimetrics, the three operational pillars of Jurimetrics
are presented: Legal, Statistical and Computational. The judge (mouth of the law) is
characterized and the definition of a miscarriage of justice is defined, which I translate as the
occurrence of a rare event. A black swan or an Australian platypus: or I hope for the very
unlikely 22 identical sides when tossing 22 coins. Marcelo Nunes reminds us of the famous
study that provides evidence that suicide can be a social phenomenon and that the use of
statistical information can mean that few platypuses appear on random walks around northern
Australia. When relating Jurimetry to the functioning of a legal order, or the meaning of a law,
legal uncertainty and people's unpredictable behavior are accepted, defining the field of study
as stochastic.

Head of the Department of Statistics at the Institute of Mathematics and Statistics of

It couldn't be any different! It convinces us that "if A is, then B is", contrasting with the legal
law (probabilistic when measuring uncertainty) "A is, then B must be". Without a doubt, an
extensive territory, especially since we are actually talking about "if A is, then B becomes
with probability p". The uncertainty, inherent to Law, is then defined and we realize that
judicial processes are in fact stochastic.

that was sold to a lucky person who matched the six numbers and the car insurance company
distributes what was sold to the unfortunate person who suffered an accident. The probabilistic paradigm is

Machine Translated by Google
University of São Paulo - IME USP.

© of this issue [2016]

Machine Translated by Google
BASS, Richard F. Stochastic processes. Cambridge: Cambridge University Press, 2011.

BOLFARINE, Heleno & BUSSAB, Wilton O. Sampling elements. São Paulo: Blucher, 2005.

ABUGU, Joseph EO A comparative analysis of the extent of judicial discretion in minority
protection litigation: the United Kingdom and United States. International Company and
Commercial Law Review, n.18, 2007.

BERNOULLI, Jacob. The art of conjecturing. Together with letters to a friend on sets in
court tennis. Baltimore: The John Hopkins University Press, 2006.

BAADE, Hans. Jurimetrics. New York and London: Basic Books, 1963.

BOISTE, Pierre Claude Victor. Dictionnaire universel de la langue française, avec le latin
et les étymologies, extract comparatif, concordance et critique de tous les dictionnaires. 8th
ed., Paris: Librarie de Firmin Didot, 1836.

ABRAN, Nelson. Limited companies. 10. ed. São Paulo: Saraiva, 2012.

ADAM, James. The republic of Plato edited with critical notes and appendices.

BLOCK, Randolph. Book review: Supreme Court Policu Making: Explanation and

BUI, Xuan Hai. Transitional adjustment problems in contemporary Vietnam company law.
Journal of International Banking Law and Regulation, (20), 2005.

ACHENWALL, Gottfried. Staatssissenchaft der vornehmen Europaischen Reiche und
Republiken, 1749.

BERNSTEIN, Peter L. Against the gods: The remarkable story of the risk. New York: John
Wiley & Sons, 1998.

BUSSAB, Wilson de O. & MORETTIN, Pedro A. Basic statistics. 6. ed. São Paulo:

ALMEIDA, Marcos Elidius Michelli de. Legal Aspects of Limited Companies. They are

BOBBIO, Norberto. Theory of the legal order. 10. ed. Brasília: University of Brasília, 1999.

Cambridge: Cambridge University Press, 1902.

Prediction. (by Harold J. Spaeth). American Bar Foundation Research Journal, (3), 1980.

ÁVILA, Humberto. Legal security. São Paulo: Malheiros, 2011.

Setback, 1996.

Paulo: Quartier Latin, 2004.

BOHR, Niehls. Atomic physics and human knowledge: Essays 1932-1957. São Paulo:

BIBLIOGRAPHY

BIBLIOGRAPHY

2018 - 07 - 17
Jurimetry

Machine Translated by Google
COHEN, I Bernard. Revolution in science. Cambridge: The Belknap Press, 1985.

CRAMTON, PR GIBBONS & KLEMPERER, P. Dissolving a Partnership Efficiently,
Econometrica, (55), 1987.

CHEUNG, Rita. The statutory minority remedies of unfair prejudice and just and equitable
winding up: the English Law Commission's recommendations as models for reform in Hong
Kong. International Company and Commercial Law Review, no. 20, 2008.

Right and power. São Paulo: Saraiva, 1992.

COHEN, Laurence Jonathan. An introduction to the philosophy of induction and

COZBY, Paul C. Research methods in behavioral sciences. 5. reimp. São Paulo: Atlas,
2011.

Company Lawyer, no. 13, 1992.

São Paulo: Cengage Learning, 2011.

_________________.

CRAWFORD, VP A game of fair division. Review of Economic Studies, (44), 1977.

DEGROOT, Morris H. Probability and statistics. Massachusetts: Addison-Wesley
Publishing Co, 1989.

CHURCHILL, Gilbert A; BROWN, Tom J & SUTER, Tracy A. Basic marketing research.

probability. London: Clarendon Press, 1989.

DANTAS, Lourenço (coord). The lived history [interviews]. São Paulo: The State of São
Paulo, 1981.

Saraiva, 2010.

2003.

COOPER, Donald R & SCHINDLER, Pamela S. Research methods in administration. 7th
Ed., Porto Alegre: Bookman, 2003.

COELHO, Fábio Ulhoa. The limited liability company in the new Civil Code. Sao Paulo: Saraiva,

_________________.

DOGSON, Charles L. Euclid and his modern rivals. Cambridge: Cambridge University
Press, 1879.

CHEFFINS, Brian R. Using theory to study law. Cambridge Law Journal, n.58, 1999.

Commercial law course(2). 16. ed. São Paulo: Saraiva, 2012.

CORNFORD, Francis MacDonald. Plato's cosmology, the Timeaus of Plato translated with
a running commentary. London: Compton Printing Ltd, 1937.

CHEFFINS, Brian R & DINE, Janet M. Shareholder remedies: lessons from Canada.

CARDOZO, Benjamin N. The nature of judicial process. New Orleans: Quid Pro Law
Books, 2010.

_________________. Civil Law Course. 5. ed. São Paulo: Saraiva,(1), 2012.

COOTER, Robert. Maturing into normal science: the effect of empirical legal studies on
Law and economics. University of Illinois Law Review, (5), 2011.

Machine Translated by Google
_______________.Introduction to the study of law: technique, decision, domination. 4.
ed. São Paulo: Atlas, 2003.

FRIEDMAN, Milton. Essays in positive economics. Chicago: University of Chicago Press,

The origins, nature, and promise of empirical legal studies and a
response to concerns. University of Illinois Law Review,(5),2011.

FERRAZ Jr., Tercio Sampaio. Social function of legal dogmatics. São Paulo: Max
Limonad, 1998.

FIGUEIRA JÚNIOR, Joel Dias & LOPES, Maurício Antonio Ribeiro. Comments on the law of

FRIEDMAN, Lawrence M. American law in the 20th century. New Haven: Yale University
Press, 2001.

2004.

FELIX, Isabelle. Informatique, télématique e vie cotidiene. Paris: La Documentation
Française, 1980.

FRANCE, Erasmo Valladão Novaes and France. Topics in corporate law, bankruptcy and
company theory. São Paulo: Malheiros, 2009.

1953.

GILLISPIE, Charles C. Pierre-Simon Laplace: a life in exact science. Princeton: Princeton

EMPIRICUS, Sextus. Outlines of Pyrrhonism. Cambridge: Harvard University Press, 1933.

special civil and criminal courts. 2nd ed. São Paulo: Ed. RT, 1997.

GARCIA, Dinio de Santis. Introduction to legal informatics. São Paulo: José Bushatsky,
1976.

DU PLEISS, JJ Some international developments in company law: a South African

FÉRES, Marcelo Andrade. Society in common. Legal discipline and related institutes. They are

FRANK, Jerome. The law and the modern mind. With a new introduction by Brian H.

GILMORE, Grant. The ages of American law. New Haven: Yale University Press, 1977.

_______________.

FRANK, EA Sander & LUKASZ Rozdeiczer, Matching Cases and Dispute Resolution
Procedures: Detailed Analysis Leading to a Mediation-Centered Approach. Harvard
Negotiation Law Review no. 11, 2006.

University press, 1997.

Eco, Umberto. On mirrors and other essays. Rio de Janeiro: New Frontier, 1989.

FERNÁNDEZ-ARMESTO, Felipe. Truth: a story. Rio de Janeiro: Record, 2000.

FRIEDMAN, Lawrence M. A history of American law. 3rd ed., New York: Simon &
Schuster, 2005.

EISENBERG, Theodore. Why do empirical legal scholarship? San Diego Law Review, (41),

perspective. Company Lawyer, no. 14, 1993.

Paul: Saraiva, 2011.

Bix. New Jersey: Transaction Publishers, 2008.

Machine Translated by Google
KITTSTEINER, Thomas & DE FRUTOS, María-Angeles. Efficient partnership dissolution

__________.

HEIDE, C & SENETA, E. Statisticians of the Centuries. New York: Springer-Verlag, 2001.

Long). São Paulo: Ideas and letters, 2008.

KILGOUR, Marc & EDEN, Colin. Handbook of group decision and negotiation. New York: Springer, (4),
2010.

A treatise on probability. London: MacMillan, 1921.

________________.

The taming of chance. New York: Cambridge University Press, 1990.

Euclid in Greek. Cambridge: Cambridge University Press, 1920.

HAUSWALD, Robert B. H & HEGE, Ulrich, Ownership and control in Joint Ventures: Theory and Evidence.
AFA San Diego Meetings, 2004.

HOLMES, Oliver W. The path of the law. Harvard Law Review, (10), 1897.

KENNEDY, Peter. A guide to econometrics. 6th ed., Cambridge: Wiley-Blackwell, 2008.

Ontario: Broadview, 2011.

University press, 2011.

New Jersey: John Wiley and Sons, 2003.

KELSEN, Hans. Pure theory of law. 6. ed. Coimbra: Armênio Amado, 1984.

Publishing Co, 1977.

decision making and the new empiricism. University of Illinois Law Review. (4), 2002.

GIRENZER, Gerd. The Empire of Chance: How Probability Changed Science and Every Day Life.
Cambridge: CambridgeUniversityPress, 1989.

HAHN, Roger. Pierre-Simon Laplace: a determined scientist. Cambridge: Harvard Press, 2005.

____________________.

___________________.

___________.

HUFFMAN, Carl A. The Pythagorean Tradition. In: Beginnings of Greek philosophy (org. AA

KEYNES, John Maynard. A treatise on probability. London: Dover, 2004.

HEATH, Thomas Little. A history of Greek mathematics. New York: Dover, 1981.

Law in science and science in law. Harvard Law Review,(12),7, 1899.

The emergency of probability. New York: Cambridge University Press, 1975.
The emergence of probability. New York: Cambridge University Press, 2006.

KADANE, Joseph B. Statistics in the Law. New York: Oxford University Press, 2008.

HALD, Anders. A history of statistics and probability and their application before 1750.

HACKING, Ian. An introduction to probability and inductive logic. New York: Cambridge

HOBBES, Thomas. Leviathan: Parts I and II. (Rev. AP Matirnich and Brian Battiste).

HUME, David. An inquiry concerning human understanding. Indianapolis: Hackett

HEISE, Michael. The past, present, and future of empirical legal scholarship: judicial

_________.

Machine Translated by Google
LORENZ KRUGER, Lorraine J. Daston, Michael Heidelberger, Gerd Gigerenzer and Mary
Morgan. The probabilistic revolution. Ideas in History (1), Ideas in Science (2).

Cheerful: Bookman, 2012.

LlEWELLYN, Karl. The bramble bush lectures: the classic lectures on the Law and Law
Schools. Oxford: Oxford University Press, 2008.

_____________. Jurimetrics: the next step forward. Minnesota Law Review, (33), 1949.

Massachusetts: MIT Press, 1990.

MALHOTRA, Naresh. Marketing research: an applied orientation. 6th edition, Porto

LEITER, Brian. American cool realism. Public law and legal theory research paper n. 2,
2002.

______________.

______________. Informatics for social science. Turin: Einaudi, 1985.

McGUINESS, Kevin. Recent perspectives on company law. Company Lawyer, no. 19, 1998.

MERRIL, Ray M. & Timmrech, Thomas C. Introduction to epidemiology. 4th., Ontario:
Johns and Bartlet, 2002.

A realistic jurisprudence: the next step. Columbia Law Review, (30), 1930.

LOSANO, Mario G. Legal informatics. São Paulo: Saraiva, 1976.

MEARS, T Lambert. The institutes of Gaius and Justinian, the twelve tables, and the
CXVIIIth and CXXVIIth novels, with introduction and translations. London: Stevens and Sons,
1882.

under buy or sell clauses. The RAND Journal of Economics, (39),1, 2008.

_____________. Jurimetrics: science and prediction in the field of law. Minnesota Law
Review,(46), 1961.

LOWE, Simon. A guide to French business entities. International Company and Commercial
Law Review, (6), 1995.

LOEVINGER, Lee. An introduction to Legal Logic. Indiana Law Journal, (27), 1952.

System and structure in law: of the century. XX to postmodernity. São
Paulo: WMF Martins Fontes (3), 2011.

MISES, Richard Von. Probability, statistics and truth. London: Dover, 1981.

LEITE JR., Antonio Goulart. Affectio societatis in civil society and simple society.

problems, (28), 1963.

______________.

Rio de Janeiro: Forense, 2006.

LEIBNIZ, Gottfried Wilhelm. From Conditionibus. Paris: Librarie Philosophique J. VRIN,
2002.

_____________. Jurimetrics: the methodology of legal inquiry. Law and contemporary

LOWRY, John. Reconstructing shareholder actions: a response to the Law Commission's
Consultation Paper. Company Lawyer, no. 18, 1997.

Machine Translated by Google
OLIPHANT, Herman. A study of day calendars. New York: Johns Hopkins University Press, 1932.

POPPER, Karl. Objective knowledge, corrected edition. Oxford: Oxford University Press.

__________.

POSNER, Richard. How judges think. Cambridge: Harvard University Press, 2008.

NICHOLAS NASSIN TALEB. The black swan: the impact of the highly improbable. New York: Randon
House, 2007.

Rio de Janeiro: Renew, 2000.

PETTY, Sir William. The economic writings of Sir William Petty. London: Routledge, 1997.

The open universe: an argument for indeterminism. London: Cambridge

_______________.

The logic of scientific discovery. New York: Routledge, 1959.

MONFORT, Roland P. Recent developments in Belgian company law, International Company and
Commercial Law Review,(6), 1995.

Paul: Edgard Blucher, 2006.

New Jersey: Transaction Publishers, 1998.

_________.

___________. Popper selections (org.David Miller). Princeton: Princeton University Press, 1985.

MITCHELL, Gregory. Empirical legal scholarship as scientific dialogue. north carolina

Chez Lefrèvre Librarie, 1826.

Plateau. Timaeus. Fairfield: World Library Books, 2008.

NUNES, Marcelo Guedes. Judicial intervention in the administration of companies. In: Power of Control
and Other Topics of Corporate Law and Capital Markets (org. Luís André N. de Moura Azevedo and Rodrigo
R. Monteiro de Castro). São Paulo: Quartier Latin, 2010.

POUND, N. Roscoe. Mechanical jurisprudence. Columbia Law Review, (8), 1908.

PRIGOGINE, Ilya. The end of certainties. São Paulo: State University Publishing House

PIMENTEL, Alendre Freire. Cybernetic law, a theoretical and logical application approach.

University press, 1982.

MOREIRA, José Carlos Barbosa. Civil procedure matters. São Paulo: Saraiva, 2004.

MOORE, Underhill. & HOPE, Theodore. An institutional approach to the Law of commercial banking. Yale
Law Journal,(38), 1929.

Criminal justice in America, with a new introduction by Ron Christenson.

PAULINO, Carlos Daniel & SINGER, Julio da Motta. Categorized data analysis. They are

Law Review, (83), 2004.

The logic of scientific discovery. New York: Basic Books, 1959.

_________.

1972.

PASCAL, Blaise. Les Pensées de Bl. Pascal suivies d'une nouvelle table analytique. Paris:

Machine Translated by Google
RIBEIRO, Renato Ventura. Exclusion of partners in corporations. São Paulo:

SADDI, Jairo & PINHEIRO, Armando Castelar. Law, Economics and Markets. River of

STIGLER, Stephen. The history of statistics. The measurement of uncertainty before 1900.

STEINBRUNER, John D. The cybernetic theory of decision: new dimensions of politics

RESCHER, Nicholas. Fairness: theory and practice of distributive justice, New Jersey:
Transaction, 2002.

judicial decision making. Michigan Law Review, (109), 2011.

ROSS, Alf. On law and justice, New Jersey: The Lawbook Exchange, 2004.

studies and new legal realism. Annual Review of Law and Social Science,(6), 2010.
SCHUMAN, Mark C. MERTZ, Elizabeth. Toward a new legal empiricism: empirical legal

RADIN, Stephen C. The business judgment rule, fiduciary duties of corporate directors.

University press, 1997.

SUGARMAN, David. Reconceptualizing company law: reflections on the Law Commission's
consultation paper on shareholders' remedies. Company Lawyer. n. 18, 1997.

SABINE, George H. Descriptive and normative sciences. The philosophical review, (21)4,
1912.

Paulista, 1996.

RICHARD RW Brooks and KATHRYN E. Spier, Trigger Happy or Gun Shy? Dissolving
Common-Value Partnerships with Texas Shootouts. Law and Economics Papers. Working
Paper 19, 2004.

RUSSEL, Bertrand. History of Western Philosophy. 3rd ed. São Paulo: Companhia Editora
Nacional,(1),1969.

RIBEIRO, Ivan Cesar. Good governance, provisions and contingencies. Valor Econômico
newspaper. Sao Paulo, p. And 3, 26 Mar., 2008.

analysis. New Jersey: Princeton University Press, 1974.

RUBIN, Edward. The real formalists, the real realists and what they tell us about

SHAPIRA, Giora. Valuation of shares in buyout orders. Company Lawyer. n. 16, 1995.

RAPHAEL, David D. Hobbes: morals and politics. London: Routledge, 2004.
6th ed., New York: Aspen, 2009.

TAMANAHA, Brian Z. Beyond the formalist-realist divide. New Jersey: Princeton

ROOLENBERG, Richard. The World of Benjamin Cardozo. Cambridge: Harvard

PULLE, Austin I. The new company law of Indonesia. Company Lawyer,(17), 1996.

SAMBURSKY, Samuel. The physical world of the Greeks. London: Routledge & Keagan
Paul, 1987.

Cambridge: Harvard University Press, 1986.

January: Elsevier, 2005.

Latin Quarter, 2005.

Machine Translated by Google
© of this issue [2016]

WESSELS, Bob. Recent changes in corporate law in the Netherlands. International
Company and Commercial Law Review. (6), 1995.

the Federal Courts. Philadelphia: Executive office, The American law institute, 1934.

the disregard of legal personality. Belo Horizonte: Del Rey, 2007.

WHITE, G. Edward. Patterns of American legal thought. New Orleans: Quid Pro Law
Books, 2010.

ZICKMUND, William G. Principles of marketing research. São Paulo: Pioneer
Thompson Learning, 2006.

THE AMERICAN LAW INSTITUTE (Wickersham Commission). A Study of the Business of

VILANOVA, Lourival. Logical structures and the system of positive law. São Paulo:
Max Limonad, 1997.

machine. Massachusetts: MIT Press, 1965.

TIMM, Luciano Benetti (org.). Law and Economics. 2nd ed. Porto Alegre: Lawyer's
Bookshop, 2008.

WIENER, Norbert. Cybernetics: or control and communication in the animal and the

University Press, 2010.

WAISBERG, Ivo. Competition Law and Policy for Developing Countries.

YAVASI, Mahmut. Limited liability companies in Turkish company law. International
Company and Commercial Law Review, (8), 1997.

VON MISES, Richard. Probability, statistics and truth. New York: Dover Publications,
1981.

WOOLDRIDGE, Frank. Germany: partnerships under German law, Company Lawyer,
(16), 1995.

TAYLOR, Thomas. The commentaries of Proclus on the Timaeus of Plato, in five books.

WARDE JR., Walfrido Jorge. Liability of partners: the limitation crisis and theory

litigation. New York: Springer-Verlag, 1997.

London: AJ Valpy, 1820.

TAYLOR, CCW The atomists, Leucippus and Democritus: fragments, a text, a
translation and commentaries. Toronto: University of Toronto Press, 1999.

São Paulo: Aduaneiras, 2006.

ZEIZEL, Hans; KAYE, David. Prove it with figures: empirical methods in law and

Machine Translated by Google